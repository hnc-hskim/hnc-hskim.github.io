[
{
	"uri": "https://hnc-hskim.github.io/hugo/comments/",
	"title": "Comments",
	"tags": [],
	"description": "",
	"content": "[참고] https://velog.io/@mellonggo/Github-%ED%8E%98%EC%9D%B4%EC%A7%80-%EB%B8%94%EB%A1%9C%EA%B7%B8-%EB%A7%8C%EB%93%A4%EA%B8%B0-with-Hugo\n댓글 기능 추가  리파지터리 생성  blog-comments로 리파지터리 생성  layouts/partials/custom-footer.html 파일 생성 및 스크립트 추가  테마별로 지정해야할 위치가 다를수 있다. learn 테마의 경우 post 레이아웃을 찾을수 없어 custom-footer.html에 추가한다.\n\r 아래 코드를 복하하여 custom-footer.html에 붙여넣는다.  \u0026lt;script src=\u0026#34;https://utteranc.es/client.js\u0026#34; repo=\u0026#34;user id/blog-comments\u0026#34; issue-term=\u0026#34;title\u0026#34; theme=\u0026#34;github-light\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34; async\u0026gt; \u0026lt;/script\u0026gt; \r첫 로딩시 댓글 기능은 비활성화되어 있고 github 로그인 인증을 통해 해당 리파지터리에서 utterances app 사용을 승인하면 이후 댓글 기능을 사용할 수 있다. 댓글의 경우 생성한 리파지터리의 Issues 생성 기능을 통해 동작한다.\n\r댓글 삭제 해당 기능은 따로 제공하지 않는것 같다. 직접 리파지터리에서 이슈로 이동후 본인이 등록한 이슈를 삭제하자\n"
},
{
	"uri": "https://hnc-hskim.github.io/hugo/",
	"title": "Hugoes",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://hnc-hskim.github.io/cloud/",
	"title": "Clouds",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://hnc-hskim.github.io/cloud/operation/scheduling/",
	"title": "Scheduling",
	"tags": [],
	"description": "",
	"content": "노드에 파드 할당하기 특정한 노드(들) 집합에서만 동작하도록 파드를 제한할 수 있다. 이를 수행하는 방법에는 여러 가지가 있으며 권장되는 접근 방식은 모두 레이블 셀렉터를 사용하여 선택을 용이하게 한다. 보통은 스케줄러가 자동으로 합리적인 배치(예: 자원이 부족한 노드에 파드를 배치하지 않도록 노드 간에 파드를 분배)를 수행하기에 이러한 제약 조건은 필요하지 않다. 그러나, 예를 들어 SSD가 장착된 머신에 파드가 배포되도록 하거나 또는 많은 통신을 하는 두 개의 서로 다른 서비스의 파드를 동일한 가용성 영역(availability zone)에 배치하는 경우와 같이, 파드가 어느 노드에 배포될지를 제어해야 하는 경우도 있다.\n방법  노드 레이블에 매칭되는 nodeSelector 필드 어피니티 / 안티 어피니티 nodeName 필드  1. 노드 레이블 다른 쿠버네티스 오브젝트와 마찬가지로, 노드도 레이블을 가진다. 레이블을 수동으로 추가할 수 있다. 또한 쿠버네티스도 클러스터의 모든 노드에 표준화된 레이블 집합을 적용한다. 잘 알려진 레이블, 어노테이션, 테인트에서 널리 사용되는 노드 레이블의 목록을 확인한다.\n 노드 조회  $ kubectl get nodes NAME STATUS ROLES AGE VERSION ip-10-83-80-162.ap-northeast-2.compute.internal Ready \u0026lt;none\u0026gt; 6d15h v1.21.12-eks-5308cf7 ip-10-83-82-103.ap-northeast-2.compute.internal Ready \u0026lt;none\u0026gt; 6d15h v1.21.12-eks-5308cf7 ip-10-83-84-128.ap-northeast-2.compute.internal Ready \u0026lt;none\u0026gt; 6d15h v1.21.12-eks-5308cf7  노드 레이블 조회  $ kubectl get nodes --show-labels NAME STATUS ROLES AGE VERSION LABELS ip-10-83-80-162.ap-northeast-2.compute.internal Ready \u0026lt;none\u0026gt; 6d15h v1.21.12-eks-5308cf7 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=m5.2xlarge,beta.kubernetes.io/os=linux,eks.amazonaws.com/capacityType=ON_DEMAND,eks.amazonaws.com/nodegroup-image=ami-0918f823d29c638d9,eks.amazonaws.com/nodegroup=black-nodegroup-20220708083127543500000015,eks.amazonaws.com/sourceLaunchTemplateId=lt-01141f4c6a453c7f0,eks.amazonaws.com/sourceLaunchTemplateVersion=1,failure-domain.beta.kubernetes.io/region=ap-northeast-2,failure-domain.beta.kubernetes.io/zone=ap-northeast-2a,kubernetes.io/arch=amd64,kubernetes.io/hostname=ip-10-83-80-162.ap-northeast-2.compute.internal,kubernetes.io/os=linux,node.kubernetes.io/instance-type=m5.2xlarge,topology.ebs.csi.aws.com/zone=ap-northeast-2a,topology.kubernetes.io/region=ap-northeast-2,topology.kubernetes.io/zone=ap-northeast-2a ip-10-83-82-103.ap-northeast-2.compute.internal Ready \u0026lt;none\u0026gt; 6d15h v1.21.12-eks-5308cf7 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=m5.2xlarge,beta.kubernetes.io/os=linux,eks.amazonaws.com/capacityType=ON_DEMAND,eks.amazonaws.com/nodegroup-image=ami-0918f823d29c638d9,eks.amazonaws.com/nodegroup=black-nodegroup-20220708083127543500000015,eks.amazonaws.com/sourceLaunchTemplateId=lt-01141f4c6a453c7f0,eks.amazonaws.com/sourceLaunchTemplateVersion=1,failure-domain.beta.kubernetes.io/region=ap-northeast-2,failure-domain.beta.kubernetes.io/zone=ap-northeast-2b,kubernetes.io/arch=amd64,kubernetes.io/hostname=ip-10-83-82-103.ap-northeast-2.compute.internal,kubernetes.io/os=linux,node.kubernetes.io/instance-type=m5.2xlarge,topology.ebs.csi.aws.com/zone=ap-northeast-2b,topology.kubernetes.io/region=ap-northeast-2,topology.kubernetes.io/zone=ap-northeast-2b ip-10-83-84-128.ap-northeast-2.compute.internal Ready \u0026lt;none\u0026gt; 6d15h v1.21.12-eks-5308cf7 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=m5.2xlarge,beta.kubernetes.io/os=linux,eks.amazonaws.com/capacityType=ON_DEMAND,eks.amazonaws.com/nodegroup-image=ami-0918f823d29c638d9,eks.amazonaws.com/nodegroup=black-nodegroup-20220708083127543500000015,eks.amazonaws.com/sourceLaunchTemplateId=lt-01141f4c6a453c7f0,eks.amazonaws.com/sourceLaunchTemplateVersion=1,failure-domain.beta.kubernetes.io/region=ap-northeast-2,failure-domain.beta.kubernetes.io/zone=ap-northeast-2c,kubernetes.io/arch=amd64,kubernetes.io/hostname=ip-10-83-84-128.ap-northeast-2.compute.internal,kubernetes.io/os=linux,node.kubernetes.io/instance-type=m5.2xlarge,topology.ebs.csi.aws.com/zone=ap-northeast-2c,topology.kubernetes.io/region=ap-northeast-2,topology.kubernetes.io/zone=ap-northeast-2c  호스트 이름으로 조회  $ kubectl describe nodes ip-10-83-80-162.ap-northeast-2.compute.internal Name: ip-10-83-80-162.ap-northeast-2.compute.internal Roles: \u0026lt;none\u0026gt; Labels: beta.kubernetes.io/arch=amd64 beta.kubernetes.io/instance-type=m5.2xlarge beta.kubernetes.io/os=linux eks.amazonaws.com/capacityType=ON_DEMAND eks.amazonaws.com/nodegroup=black-nodegroup-20220708083127543500000015 eks.amazonaws.com/nodegroup-image=ami-0918f823d29c638d9 eks.amazonaws.com/sourceLaunchTemplateId=lt-01141f4c6a453c7f0 eks.amazonaws.com/sourceLaunchTemplateVersion=1 failure-domain.beta.kubernetes.io/region=ap-northeast-2 failure-domain.beta.kubernetes.io/zone=ap-northeast-2a kubernetes.io/arch=amd64 kubernetes.io/hostname=ip-10-83-80-162.ap-northeast-2.compute.internal kubernetes.io/os=linux node.kubernetes.io/instance-type=m5.2xlarge topology.ebs.csi.aws.com/zone=ap-northeast-2a topology.kubernetes.io/region=ap-northeast-2 topology.kubernetes.io/zone=ap-northeast-2a Annotations: csi.volume.kubernetes.io/nodeid: {\u0026#34;ebs.csi.aws.com\u0026#34;:\u0026#34;i-0f255f242c1b4616e\u0026#34;,\u0026#34;efs.csi.aws.com\u0026#34;:\u0026#34;i-0f255f242c1b4616e\u0026#34;} node.alpha.kubernetes.io/ttl: 0 volumes.kubernetes.io/controller-managed-attach-detach: true CreationTimestamp: Fri, 08 Jul 2022 17:32:26 +0900 Taints: \u0026lt;none\u0026gt; Unschedulable: false Lease: HolderIdentity: ip-10-83-80-162.ap-northeast-2.compute.internal AcquireTime: \u0026lt;unset\u0026gt; RenewTime: Fri, 15 Jul 2022 08:58:03 +0900 Conditions: Type Status LastHeartbeatTime LastTransitionTime Reason Message ---- ------ ----------------- ------------------ ------ ------- MemoryPressure False Fri, 15 Jul 2022 08:55:51 +0900 Fri, 08 Jul 2022 17:32:26 +0900 KubeletHasSufficientMemory kubelet has sufficient memory available DiskPressure False Fri, 15 Jul 2022 08:55:51 +0900 Fri, 08 Jul 2022 17:32:26 +0900 KubeletHasNoDiskPressure kubelet has no disk pressure PIDPressure False Fri, 15 Jul 2022 08:55:51 +0900 Fri, 08 Jul 2022 17:32:26 +0900 KubeletHasSufficientPID kubelet has sufficient PID available Ready True Fri, 15 Jul 2022 08:55:51 +0900 Fri, 08 Jul 2022 17:32:47 +0900 KubeletReady kubelet is posting ready status Addresses: InternalIP: 10.83.80.162 Hostname: ip-10-83-80-162.ap-northeast-2.compute.internal InternalDNS: ip-10-83-80-162.ap-northeast-2.compute.internal Capacity: attachable-volumes-aws-ebs: 25 cpu: 8 ephemeral-storage: 20959212Ki hugepages-1Gi: 0 hugepages-2Mi: 0 memory: 32408676Ki pods: 58 Allocatable: attachable-volumes-aws-ebs: 25 cpu: 7910m ephemeral-storage: 18242267924 hugepages-1Gi: 0 hugepages-2Mi: 0 memory: 31391844Ki pods: 58 System Info: Machine ID: ec2cf9fd6e8ff9955c3f7269a4a9d3da System UUID: ec2cf9fd-6e8f-f995-5c3f-7269a4a9d3da Boot ID: c48572a6-8e6d-427b-b5b4-f9f21e8e0838 Kernel Version: 5.4.196-108.356.amzn2.x86_64 OS Image: Amazon Linux 2 Operating System: linux Architecture: amd64 Container Runtime Version: docker://20.10.13 Kubelet Version: v1.21.12-eks-5308cf7 Kube-Proxy Version: v1.21.12-eks-5308cf7 ProviderID: aws:///ap-northeast-2a/i-0f255f242c1b4616e Non-terminated Pods: (19 in total) Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits Age --------- ---- ------------ ---------- --------------- ------------- --- code-server code-server-84f85bfbb7-q9rs5 0 (0%) 0 (0%) 0 (0%) 0 (0%) 42h gatekeeper-system gatekeeper-controller-manager-77768dcc76-fmggf 100m (1%) 1 (12%) 256Mi (0%) 512Mi (1%) 19h kube-system alb-controller-aws-load-balancer-controller-579798fdbf-5w8zb 0 (0%) 0 (0%) 0 (0%) 0 (0%) 6d14h kube-system aws-cluster-autoscaler-84bd9c55fb-k28ps 0 (0%) 0 (0%) 0 (0%) 0 (0%) 6d14h kube-system aws-node-lsfh6 25m (0%) 0 (0%) 0 (0%) 0 (0%) 6d15h kube-system coredns-6dbb778559-5vn52 100m (1%) 0 (0%) 70Mi (0%) 170Mi (0%) 6d15h kube-system ebs-csi-node-q6df6 0 (0%) 0 (0%) 0 (0%) 0 (0%) 6d14h kube-system efs-csi-controller-664994d876-9wqqx 0 (0%) 0 (0%) 0 (0%) 0 (0%) 17h kube-system efs-csi-node-4x5tj 0 (0%) 0 (0%) 0 (0%) 0 (0%) 17h kube-system kube-proxy-26nsg 100m (1%) 0 (0%) 0 (0%) 0 (0%) 6d15h linkerd-viz tap-766dd477f8-x2m94 200m (2%) 100m (1%) 70Mi (0%) 500Mi (1%) 16h linkerd linkerd-destination-7c8564ff97-g8gf6 300m (3%) 100m (1%) 120Mi (0%) 750Mi (2%) 16h linkerd linkerd-identity-67bcfd69d4-l94zv 200m (2%) 100m (1%) 30Mi (0%) 500Mi (1%) 16h linkerd linkerd-proxy-injector-6f464ddc76-98wj4 200m (2%) 100m (1%) 70Mi (0%) 500Mi (1%) 16h litmus subscriber-cd959f546-4g8kx 125m (1%) 225m (2%) 300Mi (0%) 500Mi (1%) 21h litmus workflow-controller-856d568f68-wt6dj 125m (1%) 225m (2%) 300Mi (0%) 500Mi (1%) 21h log-stack fluentd-ccwzk 300m (3%) 300m (3%) 1Gi (3%) 1Gi (3%) 46h log-stack opensearch-cluster-coordinate-0 1 (12%) 1 (12%) 2Gi (6%) 2Gi (6%) 46h log-stack opensearch-cluster-master-0 1 (12%) 1 (12%) 2Gi (6%) 2Gi (6%) 46h Allocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) Resource Requests Limits -------- -------- ------ cpu 3775m (47%) 4150m (52%) memory 6336Mi (20%) 9052Mi (29%) ephemeral-storage 1000Mi (5%) 2Gi (11%) hugepages-1Gi 0 (0%) 0 (0%) hugepages-2Mi 0 (0%) 0 (0%) attachable-volumes-aws-ebs 0 0 Events: \u0026lt;none\u0026gt;  적용된 레이블 확인  Labels: beta.kubernetes.io/arch=amd64 beta.kubernetes.io/instance-type=m5.2xlarge beta.kubernetes.io/os=linux eks.amazonaws.com/capacityType=ON_DEMAND eks.amazonaws.com/nodegroup=black-nodegroup-20220708083127543500000015 eks.amazonaws.com/nodegroup-image=ami-0918f823d29c638d9 eks.amazonaws.com/sourceLaunchTemplateId=lt-01141f4c6a453c7f0 eks.amazonaws.com/sourceLaunchTemplateVersion=1 failure-domain.beta.kubernetes.io/region=ap-northeast-2 failure-domain.beta.kubernetes.io/zone=ap-northeast-2a kubernetes.io/arch=amd64 kubernetes.io/hostname=ip-10-83-80-162.ap-northeast-2.compute.internal kubernetes.io/os=linux node.kubernetes.io/instance-type=m5.2xlarge topology.ebs.csi.aws.com/zone=ap-northeast-2a topology.kubernetes.io/region=ap-northeast-2 topology.kubernetes.io/zone=ap-northeast-2a  현재 스케줄링중인 pod 상태  Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits Age --------- ---- ------------ ---------- --------------- ------------- --- code-server code-server-84f85bfbb7-q9rs5 0 (0%) 0 (0%) 0 (0%) 0 (0%) 42h gatekeeper-system gatekeeper-controller-manager-77768dcc76-fmggf 100m (1%) 1 (12%) 256Mi (0%) 512Mi (1%) 19h kube-system alb-controller-aws-load-balancer-controller-579798fdbf-5w8zb 0 (0%) 0 (0%) 0 (0%) 0 (0%) 6d14h kube-system aws-cluster-autoscaler-84bd9c55fb-k28ps 0 (0%) 0 (0%) 0 (0%) 0 (0%) 6d14h kube-system aws-node-lsfh6 25m (0%) 0 (0%) 0 (0%) 0 (0%) 6d15h kube-system coredns-6dbb778559-5vn52 100m (1%) 0 (0%) 70Mi (0%) 170Mi (0%) 6d15h kube-system ebs-csi-node-q6df6 0 (0%) 0 (0%) 0 (0%) 0 (0%) 6d14h kube-system efs-csi-controller-664994d876-9wqqx 0 (0%) 0 (0%) 0 (0%) 0 (0%) 17h kube-system efs-csi-node-4x5tj 0 (0%) 0 (0%) 0 (0%) 0 (0%) 17h kube-system kube-proxy-26nsg 100m (1%) 0 (0%) 0 (0%) 0 (0%) 6d15h linkerd-viz tap-766dd477f8-x2m94 200m (2%) 100m (1%) 70Mi (0%) 500Mi (1%) 16h linkerd linkerd-destination-7c8564ff97-g8gf6 300m (3%) 100m (1%) 120Mi (0%) 750Mi (2%) 16h linkerd linkerd-identity-67bcfd69d4-l94zv 200m (2%) 100m (1%) 30Mi (0%) 500Mi (1%) 16h linkerd linkerd-proxy-injector-6f464ddc76-98wj4 200m (2%) 100m (1%) 70Mi (0%) 500Mi (1%) 16h litmus subscriber-cd959f546-4g8kx 125m (1%) 225m (2%) 300Mi (0%) 500Mi (1%) 21h litmus workflow-controller-856d568f68-wt6dj 125m (1%) 225m (2%) 300Mi (0%) 500Mi (1%) 21h log-stack fluentd-ccwzk 300m (3%) 300m (3%) 1Gi (3%) 1Gi (3%) 46h log-stack opensearch-cluster-coordinate-0 1 (12%) 1 (12%) 2Gi (6%) 2Gi (6%) 46h log-stack opensearch-cluster-master-0 1 (12%) 1 (12%) 2Gi (6%) 2Gi (6%) 46h  리소스 사용 현황을 보자 cpu 점유율이 47%이다.  Allocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) Resource Requests Limits -------- -------- ------ cpu 3775m (47%) 4150m (52%) memory 6336Mi (20%) 9052Mi (29%) ephemeral-storage 1000Mi (5%) 2Gi (11%) hugepages-1Gi 0 (0%) 0 (0%) hugepages-2Mi 0 (0%) 0 (0%) attachable-volumes-aws-ebs 0 0 label을 이용한 특정 Node에 Pod 배포 테스트를 위해 nginx app을 1개 배포해 보자.\n 네임스페이스를 생성한다.(scheduling-test)  $ kubectl create namespace scheduling-test namespace/scheduling-test created  label 추가  kubectl label nodes [node_name] [key]=[value] $ kubectl label nodes ip-10-83-80-162.ap-northeast-2.compute.internal key=mytest-node node/ip-10-83-80-162.ap-northeast-2.compute.internal labeled # 레이블 삭제 kubectl label nodes mytest-node key-  레이블 확인(key : mytest-node)  $ kubectl describe nodes ip-10-83-80-162.ap-northeast-2.compute.internal Name: ip-10-83-80-162.ap-northeast-2.compute.internal Roles: \u0026lt;none\u0026gt; Labels: beta.kubernetes.io/arch=amd64 beta.kubernetes.io/instance-type=m5.2xlarge beta.kubernetes.io/os=linux eks.amazonaws.com/capacityType=ON_DEMAND eks.amazonaws.com/nodegroup=black-nodegroup-20220708083127543500000015 eks.amazonaws.com/nodegroup-image=ami-0918f823d29c638d9 eks.amazonaws.com/sourceLaunchTemplateId=lt-01141f4c6a453c7f0 eks.amazonaws.com/sourceLaunchTemplateVersion=1 failure-domain.beta.kubernetes.io/region=ap-northeast-2 failure-domain.beta.kubernetes.io/zone=ap-northeast-2a key=mytest-node kubernetes.io/arch=amd64 kubernetes.io/hostname=ip-10-83-80-162.ap-northeast-2.compute.internal kubernetes.io/os=linux node.kubernetes.io/instance-type=m5.2xlarge topology.ebs.csi.aws.com/zone=ap-northeast-2a topology.kubernetes.io/region=ap-northeast-2 topology.kubernetes.io/zone=ap-northeast-2a  scheduling.yaml  apiVersion: apps/v1 kind: Deployment metadata: name: my-nginx namespace: scheduling-test labels: app: my-nginx spec: replicas: 3 selector: matchLabels: app: my-nginx template: metadata: labels: app: my-nginx spec: containers: - name: my-nginx image: nginx:1.14.2 ports: - containerPort: 80 resources: requests: cpu: \u0026#34;500m\u0026#34; limits: cpu: \u0026#34;1000m\u0026#34; nodeSelector: key: mytest-node --- apiVersion: v1 kind: Service metadata: name: my-nginx namespace: scheduling-test labels: run: my-nginx spec: ports: - port: 80 targetPort: 80 protocol: TCP selector: app: my-nginx --- apiVersion: extensions/v1beta1 kind: Ingress metadata: name: scheduling-ingress namespace: scheduling-test annotations: kubernetes.io/ingress.class: nginx spec: rules: - host: nginxtest.black.cloud.hancom.com http: paths: - backend: serviceName: my-nginx servicePort: 80  배포  $ kubectl apply -f .\\nodeselectortest.yaml deployment.apps/my-nginx created service/my-nginx created ingress.extensions/scheduling-ingress created  pod 상태 조회  $ kubectl get pods -n scheduling-test NAME READY STATUS RESTARTS AGE my-nginx-5b5f4bdd49-qwk7x 1/1 Running 0 9s  node 상태 조회  ...... Non-terminated Pods: (20 in total) Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits Age --------- ---- ------------ ---------- --------------- ------------- --- code-server code-server-84f85bfbb7-q9rs5 0 (0%) 0 (0%) 0 (0%) 0 (0%) 43h gatekeeper-system gatekeeper-controller-manager-77768dcc76-fmggf 100m (1%) 1 (12%) 256Mi (0%) 512Mi (1%) 20h kube-system alb-controller-aws-load-balancer-controller-579798fdbf-5w8zb 0 (0%) 0 (0%) 0 (0%) 0 (0%) 6d15h kube-system aws-cluster-autoscaler-84bd9c55fb-k28ps 0 (0%) 0 (0%) 0 (0%) 0 (0%) 6d15h kube-system aws-node-lsfh6 25m (0%) 0 (0%) 0 (0%) 0 (0%) 6d15h kube-system coredns-6dbb778559-5vn52 100m (1%) 0 (0%) 70Mi (0%) 170Mi (0%) 6d16h kube-system ebs-csi-node-q6df6 0 (0%) 0 (0%) 0 (0%) 0 (0%) 6d15h kube-system efs-csi-controller-664994d876-9wqqx 0 (0%) 0 (0%) 0 (0%) 0 (0%) 17h kube-system efs-csi-node-4x5tj 0 (0%) 0 (0%) 0 (0%) 0 (0%) 17h kube-system kube-proxy-26nsg 100m (1%) 0 (0%) 0 (0%) 0 (0%) 6d15h linkerd-viz tap-766dd477f8-x2m94 200m (2%) 100m (1%) 70Mi (0%) 500Mi (1%) 16h linkerd linkerd-destination-7c8564ff97-g8gf6 300m (3%) 100m (1%) 120Mi (0%) 750Mi (2%) 16h linkerd linkerd-identity-67bcfd69d4-l94zv 200m (2%) 100m (1%) 30Mi (0%) 500Mi (1%) 16h linkerd linkerd-proxy-injector-6f464ddc76-98wj4 200m (2%) 100m (1%) 70Mi (0%) 500Mi (1%) 16h litmus subscriber-cd959f546-4g8kx 125m (1%) 225m (2%) 300Mi (0%) 500Mi (1%) 22h litmus workflow-controller-856d568f68-wt6dj 125m (1%) 225m (2%) 300Mi (0%) 500Mi (1%) 22h log-stack fluentd-ccwzk 300m (3%) 300m (3%) 1Gi (3%) 1Gi (3%) 47h log-stack opensearch-cluster-coordinate-0 1 (12%) 1 (12%) 2Gi (6%) 2Gi (6%) 47h log-stack opensearch-cluster-master-0 1 (12%) 1 (12%) 2Gi (6%) 2Gi (6%) 47h scheduling-test my-nginx-5b5f4bdd49-qwk7x 500m (6%) 1 (12%) 0 (0%) 0 (0%) 90s Allocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) Resource Requests Limits -------- -------- ------ cpu 4275m (54%) 5150m (65%) memory 6336Mi (20%) 9052Mi (29%) ephemeral-storage 1000Mi (5%) 2Gi (11%) hugepages-1Gi 0 (0%) 0 (0%) hugepages-2Mi 0 (0%) 0 (0%) attachable-volumes-aws-ebs 0 0 Deployment를 수정해 리소스 제한 요청을 늘려보자  기존 리소스 삭제  $ kubectl delete -f nodeselectortest.yaml deployment.apps \u0026#34;my-nginx\u0026#34; deleted service \u0026#34;my-nginx\u0026#34; deleted ingress.extensions \u0026#34;scheduling-ingress\u0026#34; deleted resources: requests: cpu: \u0026#34;5000m\u0026#34; limits: cpu: \u0026#34;6000m\u0026#34;  현재 상태 확인  $ kubectl describe nodes ip-10-83-80-162.ap-northeast-2.compute.internal ..... Non-terminated Pods: (19 in total) Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits Age --------- ---- ------------ ---------- --------------- ------------- --- code-server code-server-84f85bfbb7-q9rs5 0 (0%) 0 (0%) 0 (0%) 0 (0%) 43h gatekeeper-system gatekeeper-controller-manager-77768dcc76-fmggf 100m (1%) 1 (12%) 256Mi (0%) 512Mi (1%) 20h kube-system alb-controller-aws-load-balancer-controller-579798fdbf-5w8zb 0 (0%) 0 (0%) 0 (0%) 0 (0%) 6d15h kube-system aws-cluster-autoscaler-84bd9c55fb-k28ps 0 (0%) 0 (0%) 0 (0%) 0 (0%) 6d15h kube-system aws-node-lsfh6 25m (0%) 0 (0%) 0 (0%) 0 (0%) 6d16h kube-system coredns-6dbb778559-5vn52 100m (1%) 0 (0%) 70Mi (0%) 170Mi (0%) 6d16h kube-system ebs-csi-node-q6df6 0 (0%) 0 (0%) 0 (0%) 0 (0%) 6d15h kube-system efs-csi-controller-664994d876-9wqqx 0 (0%) 0 (0%) 0 (0%) 0 (0%) 18h kube-system efs-csi-node-4x5tj 0 (0%) 0 (0%) 0 (0%) 0 (0%) 18h kube-system kube-proxy-26nsg 100m (1%) 0 (0%) 0 (0%) 0 (0%) 6d16h linkerd-viz tap-766dd477f8-x2m94 200m (2%) 100m (1%) 70Mi (0%) 500Mi (1%) 16h linkerd linkerd-destination-7c8564ff97-g8gf6 300m (3%) 100m (1%) 120Mi (0%) 750Mi (2%) 16h linkerd linkerd-identity-67bcfd69d4-l94zv 200m (2%) 100m (1%) 30Mi (0%) 500Mi (1%) 16h linkerd linkerd-proxy-injector-6f464ddc76-98wj4 200m (2%) 100m (1%) 70Mi (0%) 500Mi (1%) 16h litmus subscriber-cd959f546-4g8kx 125m (1%) 225m (2%) 300Mi (0%) 500Mi (1%) 22h litmus workflow-controller-856d568f68-wt6dj 125m (1%) 225m (2%) 300Mi (0%) 500Mi (1%) 22h log-stack fluentd-ccwzk 300m (3%) 300m (3%) 1Gi (3%) 1Gi (3%) 47h log-stack opensearch-cluster-coordinate-0 1 (12%) 1 (12%) 2Gi (6%) 2Gi (6%) 47h log-stack opensearch-cluster-master-0 1 (12%) 1 (12%) 2Gi (6%) 2Gi (6%) 47h Allocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) Resource Requests Limits -------- -------- ------ cpu 3775m (47%) 4150m (52%) memory 6336Mi (20%) 9052Mi (29%) ephemeral-storage 1000Mi (5%) 2Gi (11%) hugepages-1Gi 0 (0%) 0 (0%) hugepages-2Mi 0 (0%) 0 (0%) attachable-volumes-aws-ebs 0 0 Events: \u0026lt;none\u0026gt;  배포후 pod 상태 확인  $ kubectl apply -f nodeselectortest.yaml deployment.apps/my-nginx created service/my-nginx created ingress.extensions/scheduling-ingress created $ kubectl get pods -n scheduling-test NAME READY STATUS RESTARTS AGE my-nginx-7cbcf9fb8d-lqcpg 0/1 Pending 0 13s  로그 조회  $ kubectl describe pods/my-nginx-7cbcf9fb8d-lqcpg -n scheduling-test Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 5m28s (x2 over 5m29s) default-scheduler 0/3 nodes are available: 1 Insufficient cpu, 2 node(s) didn\u0026#39;t match Pod\u0026#39;s node affinity/selector. Normal TriggeredScaleUp 5m26s cluster-autoscaler pod triggered scale-up: [{eks-black-nodegroup-20220708083127543500000015-58c0ee77-b6ac-cd2a-5d95-c6d8c8c059b3 3-\u0026gt;4 (max: 6)}] Warning FailedScheduling 4m15s (x3 over 4m43s) default-scheduler 0/4 nodes are available: 1 Insufficient cpu, 1 node(s) had taint {node.kubernetes.io/not-ready: }, that the pod didn\u0026#39;t tolerate, 2 node(s) didn\u0026#39;t match Pod\u0026#39;s node affinity/selector. Warning FailedScheduling 3m55s (x2 over 4m5s) default-scheduler 0/4 nodes are available: 1 Insufficient cpu, 3 node(s) didn\u0026#39;t match Pod\u0026#39;s node affinity/selector. Warning FailedScheduling 2m51s (x3 over 3m19s) default-scheduler 0/5 nodes are available: 1 Insufficient cpu, 1 node(s) had taint {node.kubernetes.io/not-ready: }, that the pod didn\u0026#39;t tolerate, 3 node(s) didn\u0026#39;t match Pod\u0026#39;s node affinity/selector. Warning FailedScheduling 2m31s (x2 over 2m41s) default-scheduler 0/5 nodes are available: 1 Insufficient cpu, 4 node(s) didn\u0026#39;t match Pod\u0026#39;s node affinity/selector. Warning FailedScheduling 93s (x3 over 2m1s) default-scheduler 0/6 nodes are available: 1 Insufficient cpu, 1 node(s) had taint {node.kubernetes.io/not-ready: }, that the pod didn\u0026#39;t tolerate, 4 node(s) didn\u0026#39;t match Pod\u0026#39;s node affinity/selector. Warning FailedScheduling 72s (x2 over 82s) default-scheduler 0/6 nodes are available: 1 Insufficient cpu, 5 node(s) didn\u0026#39;t match Pod\u0026#39;s node affinity/selector. Normal NotTriggerScaleUp 24s (x10 over 2m35s) cluster-autoscaler pod didn\u0026#39;t trigger scale-up: 1 max node group size reached 확인 결과 Running중인 pod의 드레인이나 리스케줄링 같은 특이상황은 관찰되지 않았으며, 리소스 부족으로 오토스케일링이 발생하여 max 값까지 노드들이 증가하지만 레이블이 존재하지 않으므로 pod 배포에 실패한다.\npriorityClassName 적용후 동작 확인  priorityClass 적용  nodeSelector: key: mytest-node priorityClassName: system-cluster-critical  배포 성공 및 기존 pod 드레인 확인  Non-terminated Pods: (16 in total) Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits Age --------- ---- ------------ ---------- --------------- ------------- --- code-server code-server-84f85bfbb7-q9rs5 0 (0%) 0 (0%) 0 (0%) 0 (0%) 43h gatekeeper-system gatekeeper-controller-manager-77768dcc76-fmggf 100m (1%) 1 (12%) 256Mi (0%) 512Mi (1%) 20h kube-system alb-controller-aws-load-balancer-controller-579798fdbf-5w8zb 0 (0%) 0 (0%) 0 (0%) 0 (0%) 6d15h kube-system aws-cluster-autoscaler-84bd9c55fb-k28ps 0 (0%) 0 (0%) 0 (0%) 0 (0%) 6d15h kube-system aws-node-lsfh6 25m (0%) 0 (0%) 0 (0%) 0 (0%) 6d16h kube-system coredns-6dbb778559-5vn52 100m (1%) 0 (0%) 70Mi (0%) 170Mi (0%) 6d16h kube-system ebs-csi-node-q6df6 0 (0%) 0 (0%) 0 (0%) 0 (0%) 6d15h kube-system efs-csi-controller-664994d876-9wqqx 0 (0%) 0 (0%) 0 (0%) 0 (0%) 18h kube-system efs-csi-node-4x5tj 0 (0%) 0 (0%) 0 (0%) 0 (0%) 18h kube-system kube-proxy-26nsg 100m (1%) 0 (0%) 0 (0%) 0 (0%) 6d16h litmus subscriber-cd959f546-4g8kx 125m (1%) 225m (2%) 300Mi (0%) 500Mi (1%) 22h litmus workflow-controller-856d568f68-wt6dj 125m (1%) 225m (2%) 300Mi (0%) 500Mi (1%) 22h log-stack fluentd-ccwzk 300m (3%) 300m (3%) 1Gi (3%) 1Gi (3%) 47h log-stack opensearch-cluster-coordinate-0 1 (12%) 1 (12%) 2Gi (6%) 2Gi (6%) 47h log-stack opensearch-cluster-master-0 1 (12%) 1 (12%) 2Gi (6%) 2Gi (6%) 47h scheduling-test my-nginx-7c9849cffb-nr6gn 5 (63%) 6 (75%) 0 (0%) 0 (0%) 99s Allocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) Resource Requests Limits -------- -------- ------ cpu 7875m (99%) 9750m (123%) memory 6046Mi (19%) 6802Mi (22%) ephemeral-storage 1000Mi (5%) 2Gi (11%) hugepages-1Gi 0 (0%) 0 (0%) hugepages-2Mi 0 (0%) 0 (0%) attachable-volumes-aws-ebs 0 0 Events: \u0026lt;none\u0026gt; 결론 우선순위가 낮은 pod들이 이동되는것을 확인함\n"
},
{
	"uri": "https://hnc-hskim.github.io/workshop/ide/vscode/",
	"title": "vscode",
	"tags": [],
	"description": "",
	"content": "\rcode server를 활용한 실습 환경 구성이 가능한지 검증해보자.\n\rVS code extentions  cloud code kubernetes  [참고] https://www.sobyte.net/post/2021-12/deploy-vscode-on-k8s/\n[참고2] https://github.com/coder/coder\n아래 명세서를 파일로 저장후 배포한다. (패스워드및 네임스페이스 확인)  code-server.yaml  apiVersion: v1 kind: Namespace metadata: name: code-server --- apiVersion: v1 kind: Service metadata: name: code-server namespace: code-server spec: ports: - port: 80 targetPort: 8080 selector: app: code-server --- apiVersion: apps/v1 kind: Deployment metadata: labels: app: code-server name: code-server namespace: code-server spec: selector: matchLabels: app: code-server template: metadata: labels: app: code-server spec: containers: - image: codercom/code-server imagePullPolicy: IfNotPresent name: code-server ports: - containerPort: 8080 env: - name: PASSWORD value: \u0026#34;your password\u0026#34; --- apiVersion: extensions/v1beta1 kind: Ingress metadata: name: code-server-ingress namespace: code-server annotations: kubernetes.io/ingress.class: nginx spec: rules: - host: \u0026#34;your_host_name.example.com\u0026#34; http: paths: - backend: serviceName: code-server servicePort: 80 ingress 확인(Host에 접속) kubectl get ingress -n code-server NAME CLASS HOSTS ADDRESS PORTS AGE code-server-ingress \u0026lt;none\u0026gt; code.**********.com a**************1-716630360.ap-northeast-2.elb.amazonaws.com 80 16m 접속 화면 web vs code 화면 sample HTML  Extension을 선택하고 Live Preview 검색후 설치한다.  node.js 설치  터미널을 열고 다음 항목을 붙여 넣는다.  curl -sL https://deb.nodesource.com/setup_14.x | sudo -E bash - sudo apt-get install -y nodejs "
},
{
	"uri": "https://hnc-hskim.github.io/workshop/",
	"title": "Workshops",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://hnc-hskim.github.io/hugo/style/",
	"title": "Style",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://hnc-hskim.github.io/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Contents  kubernetes  CLI   workshop  MSA    A notice disclaimer\n\rAn information disclaimer\n\rA tip disclaimer\n\rA warning disclaimer\n\r"
},
{
	"uri": "https://hnc-hskim.github.io/kubernetes/",
	"title": "Kubernetes",
	"tags": [],
	"description": "",
	"content": "CLI 사용법 "
},
{
	"uri": "https://hnc-hskim.github.io/kubernetes/aws-cli/",
	"title": "Aws Cli",
	"tags": [],
	"description": "",
	"content": "aws 계정 정보 조회 aws sts get-caller-identity --profile \u0026#34;name\u0026#34; kubeconfig 등록 aws eks --profile \u0026#34;profile name\u0026#34; update-kubeconfig --name \u0026#34;cluster name\u0026#34; --region ap-northeast-2 "
},
{
	"uri": "https://hnc-hskim.github.io/kubernetes/cli/",
	"title": "Cli",
	"tags": [],
	"description": "",
	"content": "컨텍스트 조회 # 조회 kubectl config get-contexts # 사용 kubectl config use-context \u0026#34;context name\u0026#34; 클러스터명 조회 kubectl config view --minify -o jsonpath=\u0026#39;{.clusters[].name}\u0026#39; 노드에 레이블 추가 # 레이블 추가 kubectl label nodes \u0026lt;your-node-name\u0026gt; disktype=ssd # 레이블 확인 kubectl get nodes --show-labels "
},
{
	"uri": "https://hnc-hskim.github.io/workshop/msa/software-architecture/",
	"title": "Software Architecture",
	"tags": [],
	"description": "",
	"content": "[참고] https://dev.to/zachgoll/introduction-to-software-architecture-monolithic-vs-layered-vs-microservices-452\n[참고] https://github.com/mermaid-js/mermaid\nDiagram Editor\n\u0026ldquo;Any intelligent fool can make things bigger, more complex, and more violent. It takes a touch of genius—and a lot of courage to move in the opposite direction\u0026rdquo;\n[From E.F. Schumacher\u0026rsquo;s book Small is Beautiful]\n\r1. 소프트웨어 아키텍처 소프트웨어 구조 또는 소프트웨어 아키텍처(software architecture)는 소프트웨어의 구성요소들 사이에서 유기적 관계를 표현하고 소프트웨어의 설계와 업그레이드를 통제하는 지침과 원칙이다.\n1.1 소프트웨어 아키텍처 설계시 고려사항  성능: 회전하는 \u0026ldquo;로드 중\u0026rdquo; 아이콘이 사라지기 전에 얼마나 기다려야 합니까? 가용성: 시스템이 실행되는 시간의 백분율은 무엇입니까? 사용성: 사용자가 시스템의 인터페이스를 쉽게 파악할 수 있습니까? 수정 가능성: 개발자가 시스템에 기능을 추가하려는 경우 수행하기 쉽습니까? 상호 운용성: 시스템이 다른 시스템과 원활하게 작동합니까? 보안: 시스템 주변에 보안 포트리스가 있습니까? 이식성: 시스템이 다양한 플랫폼(예: Windows, Mac, Linux)에서 실행될 수 있습니까? 확장성: 사용자 기반을 빠르게 성장시키면 시스템이 새로운 트래픽을 충족하도록 쉽게 확장할 수 있습니까? 배포 가능성: 프로덕션 환경에 새로운 기능을 추가하는 것이 쉽습니까? 안전: 소프트웨어가 물리적 사물을 제어하는 ​​경우 실제 사람에게 위험합니까?  2. 소프트웨어 아키텍처가 프로젝트의 성공에 중요한 13가지 이유 원문\r번역\r\r1. An architecture will inhibit or enable a system’s driving quality attributes. 2. The decisions made in an architecture allow you to reason about and manage change as the system evolves. 3. The analysis of an architecture enables early prediction of a system’s qualities. 4. A documented architecture enhances communication among stakeholders. 5. The architecture is a carrier of the earliest and hence most fundamental, hardest-to-change design decisions. 6. An architecture defines a set of constraints on subsequent implementation. 7. The architecture dictates the structure of an organization, or vice versa. 8. An architecture can provide the basis for evolutionary prototyping. 9. An architecture is the key artifact that allows the architect and project manager to reason about cost and schedule. 10. An architecture can be created as a transferable, reusable model that forms the heart of a product line. 11. Architecture-based development focuses attention on the assembly of components, rather than simply on their creation. 12. By restricting design alternatives, architecture channels the creativity of developers, reducing design and system complexity. 13. An architecture can be the foundation for training a new team member \r\r1. 아키텍처는 시스템의 구동 품질 속성을 억제하거나 활성화합니다. 2. 아키텍처에서 내린 결정을 통해 시스템이 발전함에 따라 변경 사항을 추론하고 관리할 수 있습니다. 3. 아키텍처 분석을 통해 시스템 품질을 조기에 예측할 수 있습니다. 4. 문서화된 아키텍처는 이해 관계자 간의 의사 소통을 향상시킵니다. 5. 아키텍처는 가장 초기에 가장 기본적이고 가장 변경하기 어려운 설계 결정의 전달자입니다. 6. 아키텍처는 후속 구현에 대한 일련의 제약 조건을 정의합니다. 7. 아키텍처는 조직의 구조를 결정하거나 그 반대의 경우도 마찬가지입니다. 8. 아키텍처는 진화적 프로토타이핑의 기초를 제공할 수 있습니다. 9. 아키텍처는 건축가와 프로젝트 관리자가 비용과 일정에 대해 추론할 수 있도록 하는 핵심 아티팩트입니다. 10. 아키텍처는 제품 라인의 핵심을 형성하는 양도 가능하고 재사용 가능한 모델로 생성될 수 있습니다. 11. 아키텍처 기반 개발은 단순히 구성 요소를 만드는 것보다 구성 요소의 조립에 주의를 집중합니다. 12. 설계 대안을 제한함으로써 아키텍처는 개발자의 창의성을 전달하여 설계 및 시스템 복잡성을 줄입니다. 13. 아키텍처는 새로운 팀원을 교육하기 위한 기반이 될 수 있습니다. \r\r\r\r\r"
},
{
	"uri": "https://hnc-hskim.github.io/workshop/msa/",
	"title": "Msa",
	"tags": [],
	"description": "",
	"content": "[netflex microservice] https://netflixtechblog.com/tagged/microservices\n마이크로서비스는 소프트웨어가 잘 정의된 API를 통해 통신하는 소규모의 독립적인 서비스로 구성되어 있는 경우의 소프트웨어 개발을 위한 아키텍처 및 조직적 접근 방식입니다.\n이러한 서비스는 독립적인 소규모 팀에서 보유합니다. 마이크로서비스 아키텍처는 애플리케이션의 확장을 용이하게 하고 개발 속도를 앞당겨 혁신을 실현하고 새로운 기능의 출시 시간을 단축할 수 있게 해 줍니다.\n\rGoals  소프트웨어 아키텍처가 필요한 이유 모놀리식 아키텍처 계층화된 아키텍처 마이크로 서비스 아키텍처  마이크로서비스 아키텍처의 경우, 애플리케이션이 독립적인 구성 요소로 구축되어 각 애플리케이션 프로세스가 서비스로 실행됩니다. 이러한 서비스는 경량 API를 사용하여 잘 정의된 인터페이스를 통해 통신합니다. 서비스는 비즈니스 기능을 위해 구축되며 서비스마다 한 가지 기능을 수행합니다. 서비스가 독립적으로 실행되기 때문에 애플리케이션의 특정 기능에 대한 수요를 충족하도록 각각의 서비스를 업데이트, 배포 및 확장할 수 있습니다.\n[출처] https://aws.amazon.com/ko/microservices/\nbluewhale-users\n"
},
{
	"uri": "https://hnc-hskim.github.io/workshop/msa/concept/",
	"title": "Concept",
	"tags": [],
	"description": "",
	"content": "\r아래에 사용된 예제는 NodeJS, ExpressJS, MongoDB를 이용하여 구성되었습니다.\n\rDiagram Editor\nmermaid flowchart syntax\n1. 모놀리식 아키텍처 VS 마이크로서비스 아키텍처 Monolithic Architecture 전통적인 모놀로식 아키텍처가 비효율적으로 보일 수 있지만 실제로 개발중인 애플리케이션이 충분히 복잡하지 않다면 마이크로서비스 아키텍처의 이점을 확인하기는 쉽지 않다. 비교적 간단한 아키텍처의 경우 충분히 활용 가능한 솔루션이라고 할 수 있다.\n따라서 모놀로딕 아키텍처로 시작하여 마이크로 서비스 아키텍처로 리팩토링하는것이 효율적인 개발 방법일수도 있다.\n장점  손쉬운 배포 : 단일 실행파일 또는 디렉토리로 작성되어 배포가 쉽다. 개발 : 단일 코드베이스로 구성되어 애플리케이션 개발이 쉽다. 성능 : 단일 API를 사용하여 마이크로서비스의 여러 API가 수행하는 결과와 동일한 기능을 수행한다. 테스트 간소화 : 중앙집중식 구성으로 분산된 환경보다 End-TO-End 테스트를 더 빠르게 수행할 수 있다. 디버깅 : 모든 코드가 한곳에 있으므로 요청을 트랙킹해 문제를 찾기 더 쉽다.  애플리케이션 구조 아래의 코드를 통해 monolithic 구조를 살펴보자.(계층으로 분리되지 않은 단일 구성을 설명하기 위한 예제이다. 즉, 모든 구성요소가 하나의 코드베이스로 묶인 아키텍처이다. )\n[monolithic architecture example github] https://github.com/zachgoll/monolithic-architecture-example-app\n이 코드에서 확인 가능한것은 애플리케이션간의 구분이 없다는것이다. app.js에서 데이터베이스, 서버 및 API 엔드포인트에 대한 연결을 확인할 수 있다.\nconst express = require(\u0026#34;express\u0026#34;); const app = express(); const mongoose = require(\u0026#34;mongoose\u0026#34;); const cors = require(\u0026#34;cors\u0026#34;); const bodyParser = require(\u0026#34;body-parser\u0026#34;);  // This will allow our presentation layer to retrieve data from this API without // running into cross-origin issues (CORS) app.use(cors()); app.use(bodyParser.json());  // ============================================ // ========== DATABASE CONNECTION =========== // ============================================ // Connect to running database mongoose.connect(  `mongodb://${process.env.DB_USER}:${process.env.DB_PW}@127.0.0.1:27017/monolithic_app_db`,  { useNewUrlParser: true } );  // User schema for mongodb const UserSchema = mongoose.Schema(  {  name: { type: String },  email: { type: String },  },  { collection: \u0026#34;users\u0026#34; } );  // Define the mongoose model for use below in method const User = mongoose.model(\u0026#34;User\u0026#34;, UserSchema);  function getUserByEmail(email, callback) {  try {  User.findOne({ email: email }, callback);  } catch (err) {  callback(err);  } }  // set the view engine to ejs app.set(\u0026#34;view engine\u0026#34;, \u0026#34;ejs\u0026#34;);  // index page app.get(\u0026#34;/\u0026#34;, function (req, res) {  res.render(\u0026#34;home\u0026#34;); });  // ============================================ // ============ API ENDPOINT ================ // ============================================ app.post(\u0026#34;/register\u0026#34;, function (req, res) {  const newUser = new User({  name: req.body.name,  email: req.body.email,  });   newUser.save((err, user) =\u0026gt; {  res.status(200).json(user);  }); });  // ============================================ // ============== SERVER ===================== // ============================================ app.listen(8080); console.log(\u0026#34;Visit app at http://localhost:8080\u0026#34;); 이 모놀리딕 애플리케이션이 확장하기 시작할 경우 빠르게 코드는 엉망이 될 것이다. 이 단계에서 대부분 마이크로서비스 아키텍처로의 전환을 선택하지만, 리팩토링을 통하여 계층화된 아키텍처로 바꾸는것을 다른 하나의 옵션으로 고민해 볼 수 있다.\nMonolithic Architecture (with better \u0026ldquo;layered\u0026rdquo; or \u0026ldquo;n-tier\u0026rdquo; design) 계층화된 아키텍처는 애플리케이션을 일반적으로 다음과 같은 레이어들로 분할할 수 있다.\n 프리젠테이션 계층(Presentation Layer) 비지니스 계층(Business Layer) 데이터 액세스 계층(Data Access Layer)  다른 형태로 다음과 같은 레이어로 분류할 수도 있다.\n Presentation Layer Application Layer Domain Layer Persistence Layer  Layered Architecture Diagram flowchart TD subgraph Layered-Architecture subgraph Presentation-Layer direction LR Angular --- A{{Closed}} end subgraph Business-Layer direction LR Express --- B{{Closed}} end subgraph Shared-Utilities-Layer direction LR String-Utilities --- Object-Transformation-Utilities --- C{{Closed}} end subgraph Data-Layer direction LR Mongo --- D{{Closed}} end end Presentation-Layer -- Business-Layer Business-Layer -- Shared-Utilities-Layer Shared-Utilities-Layer -- Data-Layer \r중요한 점은 각 레이어 구조에서 바로 아래 레이어만 사용할 수 있게 하도록 구조를 분리하는것입니다. 하지만 Utility 레이어의 경우처럼 때로는 공유하여 쓸수 있는 레이어가 필요할 수도 있습니다. 다이어그램에서 모든 레이어에서 사용할 수 있도록 열린 레이어로 생성한것을 확인할 수 있습니다.\nApplication Structure 위에서 언급한대로 계층화된 아키텍처에서는 각 계층이 바로 아래 계층만 사용할수 있다는 규칙이 있습니다. 그럼 이 중요한 규칙을 기반으로 monolothic archicture를 변경해 보겠습니다.\n 프레젠테이션 계층은 HTML 사용자 양식에서 호출합니다. 프레젠테이션 계층 자바스크립트는 양식을 처리하고 비즈니스 계층에 대한 호출을 실행합니다. 비즈니스 계층은 양식 정보를 처리하고 데이터 액세스 계층을 호출합니다. 데이터 액세스 계층은 정보를 처리하고 사용자를 위해 데이터베이스에 쿼리합니다. 데이터 액세스 계층은 비즈니스 계층에 정보를 반환합니다. 비즈니스 계층은 HTTP를 통해 프레젠테이션 계층에 정보를 반환합니다. 프레젠테이션 레이어는 새로운 정보로 뷰를 렌더링합니다.  1. 프레젠테이션 계층은 HTML 사용자 양식에서 호출합니다. \u0026lt;!-- File: home.ejs --\u0026gt;  \u0026lt;!-- On form submit, home.ejs executes the getDataFromBusinessLayer() function --\u0026gt;  \u0026lt;form id=\u0026#34;emailform\u0026#34; onsubmit=\u0026#34;getDataFromBusinessLayer()\u0026#34;\u0026gt;  \u0026lt;input name=\u0026#34;email\u0026#34; id=\u0026#34;email\u0026#34; placeholder=\u0026#34;Enter email...\u0026#34; /\u0026gt;  \u0026lt;button type=\u0026#34;submit\u0026#34;\u0026gt;Load Profile\u0026lt;/button\u0026gt; \u0026lt;/form\u0026gt; 2. 프리젠테이션 계층 자바스크립트는 양식을 처리하고 비즈니스 계층에 대한 호출을 실행합니다. // File: presentation-layer-user.js  function getDataFromBusinessLayer() {  event.preventDefault();  const email = $(\u0026#34;#email\u0026#34;).val();   // Perform the GET request to the business layer  // ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  $.ajax({  url: `http://localhost:8081/get-user/${email}`,  type: \u0026#34;GET\u0026#34;,  success: function (user) {  // Render the user object on the page  // Ommitted for brevity  },  error: function (jqXHR, textStatus, ex) {  console.log(textStatus + \u0026#34;,\u0026#34; + ex + \u0026#34;,\u0026#34; + jqXHR.responseText);  },  }); } 비즈니스 계층은 양식 정보를 처리하고 데이터 액세스 계층을 호출합니다.  // File: business-layer-user.js  app.get(\u0026#34;/get-user/:useremail\u0026#34;, function (req, res) {  // Makes a call to the data access layer  // ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  const user = User.getUserByEmail(req.params.useremail, (error, user) =\u0026gt; {  res.status(200).json({  name: user.name,  email: user.email,  profileUrl: user.profileUrl,  });  }); }); 데이터 접근 계층은 정보를 처리하고 사용자를 위해 데이터베이스에 쿼리합니다.  // File: data-layer-user.js  module.exports.getUserByEmail = (email, callback) =\u0026gt; {  try {  // Makes a call to the database  // ^^^^^^^^^^^^^^^^^^^^^^^^^^^^  User.findOne({ email: email }, callback);  } catch (err) {  callback(err);  } };  데이터 액세스 계층은 비즈니스 계층에 정보를 반환합니다.\n  비즈니스 계층은 HTTP를 통해 프레젠테이션 계층에 정보를 반환합니다.\n  프레젠테이션 레이어는 새로운 정보로 뷰를 렌더링합니다.\n  각 단계를 통해 계층이 담당하는 구체적인 의무를\n[monolithic layered architecture example github] https://github.com/zachgoll/layered-architecture-example-app\n"
},
{
	"uri": "https://hnc-hskim.github.io/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://hnc-hskim.github.io/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]