[
{
	"uri": "https://hnc-hskim.github.io/workshop/msa/concept/",
	"title": "Concept",
	"tags": [],
	"description": "",
	"content": "\r아래에 사용된 예제는 NodeJS, ExpressJS, MongoDB를 이용하여 구성되었습니다.\n\rReferenes  Diagram Editor mermaid flowchart syntax  \r모놀리식 아키텍처는 소프트웨어 프로그램의 전통적인 모델로, 자체 포함 방식이며 다른 애플리케이션과 독립적인 통합된 유닛으로 만들어집니다. “모놀리스\u0026quot;라는 단어는 거대하고 빙하 같은 것을 의미하는 경우가 많은데, 소프트웨어 설계의 모놀리식 아키텍처도 크게 다르지 않습니다. 모놀리식 아키텍처는 모든 비즈니스 관련 사항을 함께 결합하는 하나의 코드 베이스를 갖춘 대규모의 단일 컴퓨팅 네트워크입니다.\n\r[출처] https://www.atlassian.com/ko/microservices/microservices-architecture/microservices-vs-monolith\n1. 모놀리식 아키텍처 VS 마이크로서비스 아키텍처 Monolithic Architecture 전통적인 모놀로식 아키텍처가 비효율적으로 보일 수 있지만 간단한 아키텍처의 경우 모놀리식 아키텍처도 충분히 활용 가능한 솔루션이라고 할 수 있습니다. 시작부터 마이크로 서비스를 고려하는것이 나쁜것은 아니지만 개발중인 애플리케이션이 충분히 복잡하지 않다면 마이크로서비스 아키텍처의 이점을 확인하기는 쉽지 않습니다. 모놀로딕 아키텍처로 시작하여 서비스 확장에 따라 마이크로 서비스 아키텍처로 리팩토링하는것이 효율적인 개발 방법일수도 있습니다.\n장점  손쉬운 배포 : 단일 실행파일 또는 디렉토리로 작성되어 배포가 쉽다. 개발 : 단일 코드베이스로 구성되어 애플리케이션 개발이 쉽다. 성능 : 단일 API를 사용하여 마이크로서비스의 여러 API가 수행하는 결과와 동일한 기능을 수행한다. 테스트 간소화 : 중앙집중식 구성으로 분산된 환경보다 End-TO-End 테스트를 더 빠르게 수행할 수 있다. 디버깅 : 모든 코드가 한곳에 있으므로 요청을 트랙킹해 문제를 찾기 더 쉽다.  애플리케이션 구조 아래의 코드를 통해 monolithic 구조를 살펴보자. (이 예는 Monolithic 아키텍처를 설명하기 위한것으로 구조화되지 않은것이 Monolithic 아키텍처의 특징으로 오해하면 안된다. )\n[monolithic architecture example github] https://github.com/zachgoll/monolithic-architecture-example-app\n이 코드에서 확인 가능한것은 애플리케이션간의 구분이 없다는것이다. app.js에서 데이터베이스, 서버 및 API 엔드포인트에 대한 연결을 확인할 수 있다.\nconst express = require(\u0026#34;express\u0026#34;); const app = express(); const mongoose = require(\u0026#34;mongoose\u0026#34;); const cors = require(\u0026#34;cors\u0026#34;); const bodyParser = require(\u0026#34;body-parser\u0026#34;);  // This will allow our presentation layer to retrieve data from this API without // running into cross-origin issues (CORS) app.use(cors()); app.use(bodyParser.json());  // ============================================ // ========== DATABASE CONNECTION =========== // ============================================ // Connect to running database mongoose.connect(  `mongodb://${process.env.DB_USER}:${process.env.DB_PW}@127.0.0.1:27017/monolithic_app_db`,  { useNewUrlParser: true } );  // User schema for mongodb const UserSchema = mongoose.Schema(  {  name: { type: String },  email: { type: String },  },  { collection: \u0026#34;users\u0026#34; } );  // Define the mongoose model for use below in method const User = mongoose.model(\u0026#34;User\u0026#34;, UserSchema);  function getUserByEmail(email, callback) {  try {  User.findOne({ email: email }, callback);  } catch (err) {  callback(err);  } }  // set the view engine to ejs app.set(\u0026#34;view engine\u0026#34;, \u0026#34;ejs\u0026#34;);  // index page app.get(\u0026#34;/\u0026#34;, function (req, res) {  res.render(\u0026#34;home\u0026#34;); });  // ============================================ // ============ API ENDPOINT ================ // ============================================ app.post(\u0026#34;/register\u0026#34;, function (req, res) {  const newUser = new User({  name: req.body.name,  email: req.body.email,  });   newUser.save((err, user) =\u0026gt; {  res.status(200).json(user);  }); });  // ============================================ // ============== SERVER ===================== // ============================================ app.listen(8080); console.log(\u0026#34;Visit app at http://localhost:8080\u0026#34;); 이 모놀리딕 애플리케이션이 확장하기 시작할 경우 빠르게 코드는 엉망이 될 것이다. 이 단계에서 대부분 마이크로서비스 아키텍처로의 전환을 선택하지만, 리팩토링을 통하여 계층화된 아키텍처로 바꾸는것을 다른 하나의 옵션으로 고민해 볼 수 있다.\nMonolithic Architecture (with better \u0026ldquo;layered\u0026rdquo; or \u0026ldquo;n-tier\u0026rdquo; design) 계층화된 아키텍처는 애플리케이션을 일반적으로 다음과 같은 레이어들로 분할할 수 있다.\n 프리젠테이션 계층(Presentation Layer) 비지니스 계층(Business Layer) 데이터 액세스 계층(Data Access Layer)  다른 형태로 다음과 같은 레이어로 분류할 수도 있다.\n Presentation Layer Application Layer Domain Layer Persistence Layer  Layered Architecture Diagram flowchart TD subgraph Layered-Architecture subgraph Presentation-Layer direction LR Angular --- A{{Closed}} end subgraph Business-Layer direction LR Express --- B{{Closed}} end subgraph Shared-Utilities-Layer direction LR String-Utilities --- Object-Transformation-Utilities --- C{{Closed}} end subgraph Data-Layer direction LR Mongo --- D{{Closed}} end end Presentation-Layer -- Business-Layer Business-Layer -- Shared-Utilities-Layer Shared-Utilities-Layer -- Data-Layer \r중요한 점은 각 레이어 구조에서 바로 아래 레이어만 사용할 수 있게 하도록 구조를 분리하는것입니다. 하지만 Utility 레이어의 경우처럼 때로는 공유하여 쓸수 있는 레이어가 필요할 수도 있습니다. 다이어그램에서 모든 레이어에서 사용할 수 있도록 열린 레이어로 생성한것을 확인할 수 있습니다.\nApplication Structure 위에서 언급한대로 계층화된 아키텍처에서는 각 계층이 바로 아래 계층만 사용할수 있다는 규칙이 있습니다. 그럼 이 중요한 규칙을 기반으로 monolothic archicture를 변경해 보겠습니다.\n 프레젠테이션 계층은 HTML 사용자 양식에서 호출합니다. 프레젠테이션 계층 자바스크립트는 양식을 처리하고 비즈니스 계층에 대한 호출을 실행합니다. 비즈니스 계층은 양식 정보를 처리하고 데이터 액세스 계층을 호출합니다. 데이터 액세스 계층은 정보를 처리하고 사용자를 위해 데이터베이스에 쿼리합니다. 데이터 액세스 계층은 비즈니스 계층에 정보를 반환합니다. 비즈니스 계층은 HTTP를 통해 프레젠테이션 계층에 정보를 반환합니다. 프레젠테이션 레이어는 새로운 정보로 뷰를 렌더링합니다.  1. 프레젠테이션 계층은 HTML 사용자 양식에서 호출합니다. \u0026lt;!-- File: home.ejs --\u0026gt;  \u0026lt;!-- On form submit, home.ejs executes the getDataFromBusinessLayer() function --\u0026gt;  \u0026lt;form id=\u0026#34;emailform\u0026#34; onsubmit=\u0026#34;getDataFromBusinessLayer()\u0026#34;\u0026gt;  \u0026lt;input name=\u0026#34;email\u0026#34; id=\u0026#34;email\u0026#34; placeholder=\u0026#34;Enter email...\u0026#34; /\u0026gt;  \u0026lt;button type=\u0026#34;submit\u0026#34;\u0026gt;Load Profile\u0026lt;/button\u0026gt; \u0026lt;/form\u0026gt; 2. 프리젠테이션 계층 자바스크립트는 양식을 처리하고 비즈니스 계층에 대한 호출을 실행합니다. // File: presentation-layer-user.js  function getDataFromBusinessLayer() {  event.preventDefault();  const email = $(\u0026#34;#email\u0026#34;).val();   // Perform the GET request to the business layer  // ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  $.ajax({  url: `http://localhost:8081/get-user/${email}`,  type: \u0026#34;GET\u0026#34;,  success: function (user) {  // Render the user object on the page  // Ommitted for brevity  },  error: function (jqXHR, textStatus, ex) {  console.log(textStatus + \u0026#34;,\u0026#34; + ex + \u0026#34;,\u0026#34; + jqXHR.responseText);  },  }); } 비즈니스 계층은 양식 정보를 처리하고 데이터 액세스 계층을 호출합니다.  // File: business-layer-user.js  app.get(\u0026#34;/get-user/:useremail\u0026#34;, function (req, res) {  // Makes a call to the data access layer  // ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  const user = User.getUserByEmail(req.params.useremail, (error, user) =\u0026gt; {  res.status(200).json({  name: user.name,  email: user.email,  profileUrl: user.profileUrl,  });  }); }); 데이터 접근 계층은 정보를 처리하고 사용자를 위해 데이터베이스에 쿼리합니다.  // File: data-layer-user.js  module.exports.getUserByEmail = (email, callback) =\u0026gt; {  try {  // Makes a call to the database  // ^^^^^^^^^^^^^^^^^^^^^^^^^^^^  User.findOne({ email: email }, callback);  } catch (err) {  callback(err);  } };  데이터 액세스 계층은 비즈니스 계층에 정보를 반환합니다.\n  비즈니스 계층은 HTTP를 통해 프레젠테이션 계층에 정보를 반환합니다.\n  프레젠테이션 레이어는 새로운 정보로 뷰를 렌더링합니다.\n  각 단계를 통해 계층이 담당하는 구체적인 의무를\n[monolithic layered architecture example github] https://github.com/zachgoll/layered-architecture-example-app\n"
},
{
	"uri": "https://hnc-hskim.github.io/workshop/msa/software-architecture-pattern/",
	"title": "Software Architecture Pattern",
	"tags": [],
	"description": "",
	"content": "Referenes  10 Common Software Architectural Patterns in a nutshell Architectural patterns  10가지 소프트웨어 아키텍처 패턴  계층화 패턴 (Layered pattern) 클라이언트-서버 패턴 (Client-server pattern) 마스터-슬레이브 패턴 (Master-slave pattern) 파이프-필터 패턴 (Pipe-filter pattern) 브로커 패턴 (Broker pattern) 피어 투 피어 패턴 (Peer-to-peer pattern) 이벤트-버스 패턴 (Event-bus pattern) 모델-뷰-컨트롤러 패턴 (Model-view-controller pattern) 블랙보드 패턴 (Blackboard pattern) 인터프리터 패턴 (Interpreter pattern)  1. 계층화 패턴 (Layered pattern) 계층화 패턴에서 흔히 볼 수 있는 4개의 계층은 다음과 같다.\n 프레젠테이션 계층 ( UI 계층 이라고도 함 ) 애플리케이션 계층 ( 서비스 계층 이라고도 함 ) 비즈니스 논리 계층 ( 도메인 계층 이라고도 함 ) 데이터 액세스 계층 ( 지속성 계층 이라고도 함 )  (1) 일반 데스크탑 애플리케이션\n(2) 전자 상거래 웹 애플리케이션\n\r2. 클라이언트-서버 패턴 (Client-server pattern) 흔히 사용되는 패턴입니다. 서버와 여러 클라이언트가 존재할 수 있습니다. 서버는 여러 클라이언트에 서비스를 제공합니다.\n 클라이언트는 서버에 서비스 요청 서버는 클라이언트에 서비스 제공 서버는 클라이언트 요청을 계속 수신  3. 마스터-슬레이브 패턴 (Master-slave pattern) 서비스 주체가 Master와 Slave로 구성됩니다. 마스터 컴포넌트는 슬래이브 컴포넌트간에 작업을 분배하고 슬래이가 반환한 결과로부터 최종 결과를 계산합니다.\n(1) 데이터베이스 복제\n\r4. 파이프-필터 패턴 (Pipe-filter pattern) 데이터스트림을 생성하고 가공하는 시스템을 구성하는데 사용할 수 있습니다. 각 단계는 필터 구성 요소로 구분되고 처리 데이터는 파이프를 통해 전달됩니다.\n(1) 컴파일러\n\r5. 브로커 패턴 (Broker pattern) 분리된 구성 요소가 있는 분산 시스템을 구성하는데 사용됩니다. 원격 서비스 호출을 통해 서로 상호작용할 수 있습니다. 브로커 구성 요소는 구성 요소간의 통신을 담당합니다.\n(1) Apache ActiveMQ\n(2) Apache Kafka\n(3) RabbitMQ\n\r6. 피어 투 피어 패턴 (Peer-to-peer pattern) 개별 구성요소는 피어로 표현하며, 각 피어는 다른 피어에게 서비스를 요청하는 클라이언트와 다른 피어에게 서비스를 제공하는 서버역할을 모두 할 수 있습니다.\n(1) 파일 공유 네트워크(Gnutella)\n(2) 블록체인 기반 상품\n\r7. 이벤트-버스 패턴 (Event-bus pattern) 이벤트 소스, 이벤트 리스터, 채널 및 이벤트 버스 4가지 구성요소로 구성됩니다. 소스는 이벤트 버스의 특정 채널에 메시지를 게시합니다. 청취자는 특정 채널을 구독합니다. 리스너트 이전에 구독한 채널에 게시된 메시지에 대한 알림을 받습니다.\n(1) 안드로이드 개발\n(2) 알림서비스\n\r8. 모델-뷰-컨트롤러 패턴 (Model-view-controller pattern) MVC 패턴으로 불리며 대화형 애플리케이션을 구성하는데 사용됩니다.\n Model : 핵심 기능 및 데이터 View : 사용자에게 정보를 표시 Controller : 사용자의 입력을 처리  (1) 웹 애플리케이션을 위한 아키텍처로 사용\n\r9. 블랙보드 패턴 (Blackboard pattern) 이 패턴은 솔루션이 결정되지 않은 문제에 유용합니다.\n blackboard : 솔루션 공간의 객체를 포함하는 구조화된 전역 메모리 지식 소스 : 고유한 표현이 있는 특수 모듈 제어 구성 요소 : 모듈을 선택하고 구성 및 실행합니다.  (1) 음성 인식\n(2) 차량 식별 및 추적\n\r10. 인터프리터 패턴 (Interpreter pattern) 이 패턴은 전용 언어로 작성된 프로그램을 해석하는 구성 요소를 설계하는데 사용됩니다. 기본 아이디어는 언어의 각 기호에 대한 클래스를 구성하는것입니다.\n(1) SQL과 같은 데이터베이스 쿼리 언어\n(2) 통신 프로토콜에 사용\n\r"
},
{
	"uri": "https://hnc-hskim.github.io/workshop/msa/software-architecture/",
	"title": "Software Architecture",
	"tags": [],
	"description": "",
	"content": "[참고] https://dev.to/zachgoll/introduction-to-software-architecture-monolithic-vs-layered-vs-microservices-452\n[참고] https://github.com/mermaid-js/mermaid\nDiagram Editor\n\u0026ldquo;Any intelligent fool can make things bigger, more complex, and more violent. It takes a touch of genius—and a lot of courage to move in the opposite direction\u0026rdquo;\n[From E.F. Schumacher\u0026rsquo;s book Small is Beautiful]\n\r1. 소프트웨어 아키텍처 소프트웨어 구조 또는 소프트웨어 아키텍처(software architecture)는 소프트웨어의 구성요소들 사이에서 유기적 관계를 표현하고 소프트웨어의 설계와 업그레이드를 통제하는 지침과 원칙이다.\n1.1 소프트웨어 아키텍처 설계시 고려사항  성능: 회전하는 \u0026ldquo;로드 중\u0026rdquo; 아이콘이 사라지기 전에 얼마나 기다려야 합니까? 가용성: 시스템이 실행되는 시간의 백분율은 무엇입니까? 사용성: 사용자가 시스템의 인터페이스를 쉽게 파악할 수 있습니까? 수정 가능성: 개발자가 시스템에 기능을 추가하려는 경우 수행하기 쉽습니까? 상호 운용성: 시스템이 다른 시스템과 원활하게 작동합니까? 보안: 시스템 주변에 보안 포트리스가 있습니까? 이식성: 시스템이 다양한 플랫폼(예: Windows, Mac, Linux)에서 실행될 수 있습니까? 확장성: 사용자 기반을 빠르게 성장시키면 시스템이 새로운 트래픽을 충족하도록 쉽게 확장할 수 있습니까? 배포 가능성: 프로덕션 환경에 새로운 기능을 추가하는 것이 쉽습니까? 안전: 소프트웨어가 물리적 사물을 제어하는 ​​경우 실제 사람에게 위험합니까?  2. 소프트웨어 아키텍처가 프로젝트의 성공에 중요한 13가지 이유 원문\r번역\r\r1. An architecture will inhibit or enable a system’s driving quality attributes. 2. The decisions made in an architecture allow you to reason about and manage change as the system evolves. 3. The analysis of an architecture enables early prediction of a system’s qualities. 4. A documented architecture enhances communication among stakeholders. 5. The architecture is a carrier of the earliest and hence most fundamental, hardest-to-change design decisions. 6. An architecture defines a set of constraints on subsequent implementation. 7. The architecture dictates the structure of an organization, or vice versa. 8. An architecture can provide the basis for evolutionary prototyping. 9. An architecture is the key artifact that allows the architect and project manager to reason about cost and schedule. 10. An architecture can be created as a transferable, reusable model that forms the heart of a product line. 11. Architecture-based development focuses attention on the assembly of components, rather than simply on their creation. 12. By restricting design alternatives, architecture channels the creativity of developers, reducing design and system complexity. 13. An architecture can be the foundation for training a new team member \r\r1. 아키텍처는 시스템의 구동 품질 속성을 억제하거나 활성화합니다. 2. 아키텍처에서 내린 결정을 통해 시스템이 발전함에 따라 변경 사항을 추론하고 관리할 수 있습니다. 3. 아키텍처 분석을 통해 시스템 품질을 조기에 예측할 수 있습니다. 4. 문서화된 아키텍처는 이해 관계자 간의 의사 소통을 향상시킵니다. 5. 아키텍처는 가장 초기에 가장 기본적이고 가장 변경하기 어려운 설계 결정의 전달자입니다. 6. 아키텍처는 후속 구현에 대한 일련의 제약 조건을 정의합니다. 7. 아키텍처는 조직의 구조를 결정하거나 그 반대의 경우도 마찬가지입니다. 8. 아키텍처는 진화적 프로토타이핑의 기초를 제공할 수 있습니다. 9. 아키텍처는 건축가와 프로젝트 관리자가 비용과 일정에 대해 추론할 수 있도록 하는 핵심 아티팩트입니다. 10. 아키텍처는 제품 라인의 핵심을 형성하는 양도 가능하고 재사용 가능한 모델로 생성될 수 있습니다. 11. 아키텍처 기반 개발은 단순히 구성 요소를 만드는 것보다 구성 요소의 조립에 주의를 집중합니다. 12. 설계 대안을 제한함으로써 아키텍처는 개발자의 창의성을 전달하여 설계 및 시스템 복잡성을 줄입니다. 13. 아키텍처는 새로운 팀원을 교육하기 위한 기반이 될 수 있습니다. \r\r\r\r\r"
},
{
	"uri": "https://hnc-hskim.github.io/cloud/",
	"title": "Clouds",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://hnc-hskim.github.io/cloud/rancher/monitoring/",
	"title": "Monitoring",
	"tags": [],
	"description": "",
	"content": "Rancher에 모니터링 도구 설치 및 GPU 모니터링 References  [참고1] https://nvidia.github.io/gpu-monitoring-tools/ [참고2] https://passwd.tistory.com/entry/NVIDIAgpu-monitoring-tools-dcgm-exporter-CrashLoopBackOff  rancher에서 monitoring 도구 설치   Apps -\u0026gt; Charts 이동후 monitoring 검색   Monitoring 설치 설치를 진행하면 모니터링 앱은 Rancher 의 cattle-monitoring-system namespace 에 배포됨\n(설치후 랜처 로그아웃후 다시 로그인)\n  네비게이션 영역을 보면 Monitoring 메뉴가 추가되어 있음   대쉬보드 확인   grafana 확인   그라파나에 로그인합니다. Grafana 인스턴스의 기본 관리자 사용자 이름과 비밀번호는 입니다 admin/prom-operator. (비밀번호가 있는 사람에 관계없이 Rancher의 클러스터 관리자 권한은 여전히 ​​Grafana 인스턴스에 액세스해야 합니다.) 차트를 배포하거나 업그레이드할 때 대체 자격 증명을 제공할 수도 있습니다.\n\rGPU 노드 모니터링을 위한 dcgm-exporter 설치  Helm v3 설치  curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 \u0026amp;\u0026amp; \\ chmod 700 get_helm.sh \u0026amp;\u0026amp; \\ ./get_helm.sh  Helm 저장소 설정  helm repo add gpu-helm-charts \\ https://nvidia.github.io/gpu-monitoring-tools/helm-charts  레파지터리 업데이트  helm repo update  DCGM-Exporter 공식 차트 설치  #helm install --generate-name gpu-helm-charts/dcgm-exporter # orca mlops node를 모니터링하기 위해 tolerations, nodeSelector 수정한 values파일로 설치한다. helm install -f dcgm-values.yaml --generate-name gpu-helm-charts/dcgm-exporter  차트 확인  $ helm search repo gpu-helm-charts NAME CHART VERSION APP VERSION DESCRIPTION gpu-helm-charts/dcgm-exporter 2.4.0 2.4.0 A Helm chart for DCGM exporter gpu-helm-charts/kube-prometheus 0.0.43 Manifests, dashboards, and alerting rules for e... gpu-helm-charts/prometheus-operator 0.0.15 Provides easy monitoring definitions for Kubern... $ helm inspect chart gpu-helm-charts/dcgm-exporter apiVersion: v2 appVersion: 2.4.0 description: A Helm chart for DCGM exporter home: https://github.com/nvidia/gpu-monitoring-tools/ icon: https://assets.nvidiagrid.net/ngc/logos/DCGM.png keywords: - gpu - cuda - compute - monitoring - telemetry - tesla kubeVersion: \u0026#39;\u0026gt;= 1.13.0-0\u0026#39; name: dcgm-exporter sources: - https://gitlab.com/nvidia/container-toolkit/gpu-monitoring-tools version: 2.4.0  dcgm-values.yaml로 저장한다.  $ helm inspect values gpu-helm-charts/dcgm-exporter # Copyright (c) 2020, NVIDIA CORPORATION. All rights reserved. # # Licensed under the Apache License, Version 2.0 (the \u0026#34;License\u0026#34;); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \u0026#34;AS IS\u0026#34; BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. image: repository: nvcr.io/nvidia/k8s/dcgm-exporter pullPolicy: IfNotPresent # Image tag defaults to AppVersion, but you can use the tag key # for the image tag, e.g: tag: 2.2.9-2.4.0-ubuntu18.04 # Comment the following line to stop profiling metrics from DCGM arguments: [\u0026#34;-f\u0026#34;, \u0026#34;/etc/dcgm-exporter/dcp-metrics-included.csv\u0026#34;] # NOTE: in general, add any command line arguments to arguments above # and they will be passed through. # Use \u0026#34;-r\u0026#34;, \u0026#34;\u0026lt;HOST\u0026gt;:\u0026lt;PORT\u0026gt;\u0026#34; to connect to an already running hostengine # Example arguments: [\u0026#34;-r\u0026#34;, \u0026#34;host123:5555\u0026#34;] # Use \u0026#34;-n\u0026#34; to remove the hostname tag from the output. # Example arguments: [\u0026#34;-n\u0026#34;] # Use \u0026#34;-d\u0026#34; to specify the devices to monitor. -d must be followed by a string # in the following format: [f] or [g[:numeric_range][+]][i[:numeric_range]] # Where a numeric range is something like 0-4 or 0,2,4, etc. # Example arguments: [\u0026#34;-d\u0026#34;, \u0026#34;g+i\u0026#34;] to monitor all GPUs and GPU instances or # [\u0026#34;-d\u0026#34;, \u0026#34;g:0-3\u0026#34;] to monitor GPUs 0-3. imagePullSecrets: [] nameOverride: \u0026#34;\u0026#34; fullnameOverride: \u0026#34;\u0026#34; serviceAccount: # Specifies whether a service account should be created create: true # Annotations to add to the service account annotations: {} # The name of the service account to use. # If not set and create is true, a name is generated using the fullname template name: podSecurityContext: {} # fsGroup: 2000 securityContext: runAsNonRoot: false runAsUser: 0 capabilities: add: [\u0026#34;SYS_ADMIN\u0026#34;] # readOnlyRootFilesystem: true service: type: ClusterIP port: 9400 address: \u0026#34;:9400\u0026#34; # Annotations to add to the service annotations: {} resources: {} # limits: # cpu: 100m # memory: 128Mi # requests: # cpu: 100m # memory: 128Mi serviceMonitor: enabled: true interval: 15s additionalLabels: {} #monitoring: prometheus mapPodsMetrics: false nodeSelector: {} #node: gpu tolerations: [] #- operator: Exists affinity: {} #nodeAffinity: # requiredDuringSchedulingIgnoredDuringExecution: # nodeSelectorTerms: # - matchExpressions: # - key: nvidia-gpu # operator: Exists extraHostVolumes: [] #- name: host-binaries # hostPath: /opt/bin extraVolumeMounts: [] #- name: host-binaries # mountPath: /opt/bin # readOnly: true extraEnv: [] #- name: EXTRA_VAR # value: \u0026#34;TheStringValue\u0026#34;  helm 삭제  $ helm ls NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION dcgm-exporter-1658202339 default 1 2022-07-19 12:45:40.1694572 +0900 KST deployed dcgm-exporter-2.4.0 2.4.0 $ helm delete dcgm-exporter-1658202339 exporter 추가후 동작 확인 $ kubectl get pods -A | grep exporter cattle-monitoring-system rancher-monitoring-prometheus-node-exporter-6q7pp 1/1 Running 0 56m cattle-monitoring-system rancher-monitoring-prometheus-node-exporter-8bmpz 1/1 Running 0 56m cattle-monitoring-system rancher-monitoring-prometheus-node-exporter-8xrk6 1/1 Running 0 56m cattle-monitoring-system rancher-monitoring-prometheus-node-exporter-blvhr 1/1 Running 0 56m cattle-monitoring-system rancher-monitoring-prometheus-node-exporter-kc4ql 1/1 Running 0 56m cattle-monitoring-system rancher-monitoring-prometheus-node-exporter-l56nm 1/1 Running 0 56m cattle-monitoring-system rancher-monitoring-prometheus-node-exporter-qkk82 1/1 Running 0 56m cattle-monitoring-system rancher-monitoring-prometheus-node-exporter-s654d 1/1 Running 0 56m cattle-monitoring-system rancher-monitoring-prometheus-node-exporter-x6hjz 1/1 Running 0 56m default dcgm-exporter-1658202339-6wwlg 0/1 CrashLoopBackOff 6 5m22s default dcgm-exporter-1658202339-ffch6 0/1 CrashLoopBackOff 6 5m22s default dcgm-exporter-1658202339-kgldt 0/1 CrashLoopBackOff 7 5m22s default dcgm-exporter-1658202339-nprlc 0/1 CrashLoopBackOff 6 5m22s CrashLoopBackoff 상태 확인  로그는 정상  $ kubectl logs dcgm-exporter-1658205662-qwhs9 time=\u0026#34;2022-07-19T03:50:16Z\u0026#34; level=info msg=\u0026#34;Starting dcgm-exporter\u0026#34; time=\u0026#34;2022-07-19T03:50:16Z\u0026#34; level=info msg=\u0026#34;DCGM successfully initialized!\u0026#34; time=\u0026#34;2022-07-19T03:50:16Z\u0026#34; level=info msg=\u0026#34;Collecting DCP Metrics\u0026#34; time=\u0026#34;2022-07-19T03:50:16Z\u0026#34; level=info msg=\u0026#34;Kubernetes metrics collection enabled!\u0026#34; time=\u0026#34;2022-07-19T03:50:16Z\u0026#34; level=info msg=\u0026#34;Pipeline starting\u0026#34; time=\u0026#34;2022-07-19T03:50:16Z\u0026#34; level=info msg=\u0026#34;Starting webserver\u0026#34;  상태 확인  $ kubectl describe pod dcgm-exporter-1658205662-qwhs9 ...... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 29m default-scheduler Successfully assigned default/dcgm-exporter-1658205662-qwhs9 to hcidc-sv-paz-orca-worker-09 Normal Pulled 29m kubelet Container image \u0026#34;nvcr.io/nvidia/k8s/dcgm-exporter:2.2.9-2.4.0-ubuntu18.04\u0026#34; already present on machine Normal Created 29m kubelet Created container exporter Normal Started 29m kubelet Started container exporter Warning Unhealthy 28m (x3 over 29m) kubelet Readiness probe failed: HTTP probe failed with statuscode: 503 CrashLoopBackOff 해결 $ kubectl edit daemonset.apps/dcgm-exporter-1658205662 # initialDelaySeconds를 60으로 변경 ....... livenessProbe: failureThreshold: 3 httpGet: path: /health port: 9400 scheme: HTTP initialDelaySeconds: 60 periodSeconds: 5 successThreshold: 1 timeoutSeconds: 1 grafana로 이동후 gpu dashboard 설치 DCGM Exporter Dashboard 설치 https://grafana.com/grafana/dashboards/12239   Import 선택   dashboard id(12239) 입력   Prometheus 선택   Dashboard 확인   "
},
{
	"uri": "https://hnc-hskim.github.io/workshop/",
	"title": "Workshops",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://hnc-hskim.github.io/terraform/introduction/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "테라폼(Terraform)은 하시코프(Hashicorp)에서 오픈소스로 개발중인 클라우드 인프라스트럭처 자동화를 지향하는 코드로서의 인프라스트럭처(Infrastructure as Code), IaC1 도구입니다.\nAWS 클라우드 포메이션AWS CloudFormation의 경우 AWS만 지원하는 것과 달리 테라폼의 경우 아마존 웹 서비스, 구글 클라우드 플랫폼(Google Cloud Platform), 마이크로소프트 애저(Microsoft Azure)와 같은 주요 클라우드 서비스를 비롯한 다양한 클라우드 서비스들을 프로바이더 방식으로 제공하고 있습니다. 이를 통해 테라폼만으로 멀티 클라우드의 리소스들을 선언하고 코드로 관리하는 것도 가능합니다.\n테라폼은 고(Go) 프로그래밍 언어로 개발하고 있습니다.\n테라폼 공식 홈\n  IaC는 코드로 인프라스트럭처를 관리한다는 개념으로 테라폼에서는 하시코프 설정 언어(HCL, Hashicorp Configuration Language)을 사용해 클라우드 리소스를 선언합니다.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   "
},
{
	"uri": "https://hnc-hskim.github.io/terraform/",
	"title": "Terraforms",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://hnc-hskim.github.io/references/kubernetes/",
	"title": "Kubernetes",
	"tags": [],
	"description": "",
	"content": "참고  Advanced Scheduling  "
},
{
	"uri": "https://hnc-hskim.github.io/references/",
	"title": "References",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://hnc-hskim.github.io/references/terraform/",
	"title": "Terraform",
	"tags": [],
	"description": "",
	"content": "참고  helm values파일내 환경변수 전달 nginx controller acm 적용  "
},
{
	"uri": "https://hnc-hskim.github.io/hugo/comments/",
	"title": "Comments",
	"tags": [],
	"description": "",
	"content": "[참고] https://velog.io/@mellonggo/Github-%ED%8E%98%EC%9D%B4%EC%A7%80-%EB%B8%94%EB%A1%9C%EA%B7%B8-%EB%A7%8C%EB%93%A4%EA%B8%B0-with-Hugo\n댓글 기능 추가  리파지터리 생성  blog-comments로 리파지터리 생성  layouts/partials/custom-footer.html 파일 생성 및 스크립트 추가  테마별로 지정해야할 위치가 다를수 있다. learn 테마의 경우 post 레이아웃을 찾을수 없어 custom-footer.html에 추가한다.\n\r 아래 코드를 복하하여 custom-footer.html에 붙여넣는다.  \u0026lt;script src=\u0026#34;https://utteranc.es/client.js\u0026#34; repo=\u0026#34;user id/blog-comments\u0026#34; issue-term=\u0026#34;title\u0026#34; theme=\u0026#34;github-light\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34; async\u0026gt; \u0026lt;/script\u0026gt; \r첫 로딩시 댓글 기능은 비활성화되어 있고 github 로그인 인증을 통해 해당 리파지터리에서 utterances app 사용을 승인하면 이후 댓글 기능을 사용할 수 있다. 댓글의 경우 생성한 리파지터리의 Issues 생성 기능을 통해 동작한다.\n\r댓글 삭제 해당 기능은 따로 제공하지 않는것 같다. 직접 리파지터리에서 이슈로 이동후 본인이 등록한 이슈를 삭제하자\n댓글 기능 상단의 Comment를 누르면 댓글 리파지터리의 이슈탭으로 이동한다. 이곳에서 삭제하자\n"
},
{
	"uri": "https://hnc-hskim.github.io/hugo/",
	"title": "Hugoes",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://hnc-hskim.github.io/cloud/operation/scheduling/",
	"title": "Scheduling",
	"tags": [],
	"description": "",
	"content": "노드에 파드 할당하기 특정한 노드(들) 집합에서만 동작하도록 파드를 제한할 수 있다. 이를 수행하는 방법에는 여러 가지가 있으며 권장되는 접근 방식은 모두 레이블 셀렉터를 사용하여 선택을 용이하게 한다. 보통은 스케줄러가 자동으로 합리적인 배치(예: 자원이 부족한 노드에 파드를 배치하지 않도록 노드 간에 파드를 분배)를 수행하기에 이러한 제약 조건은 필요하지 않다. 그러나, 예를 들어 SSD가 장착된 머신에 파드가 배포되도록 하거나 또는 많은 통신을 하는 두 개의 서로 다른 서비스의 파드를 동일한 가용성 영역(availability zone)에 배치하는 경우와 같이, 파드가 어느 노드에 배포될지를 제어해야 하는 경우도 있다.\n방법  노드 레이블에 매칭되는 nodeSelector 필드 어피니티 / 안티 어피니티 nodeName 필드  1. 노드 레이블 다른 쿠버네티스 오브젝트와 마찬가지로, 노드도 레이블을 가진다. 레이블을 수동으로 추가할 수 있다. 또한 쿠버네티스도 클러스터의 모든 노드에 표준화된 레이블 집합을 적용한다. 잘 알려진 레이블, 어노테이션, 테인트에서 널리 사용되는 노드 레이블의 목록을 확인한다.\n 노드 조회  $ kubectl get nodes NAME STATUS ROLES AGE VERSION ip-10-83-80-162.ap-northeast-2.compute.internal Ready \u0026lt;none\u0026gt; 6d15h v1.21.12-eks-5308cf7 ip-10-83-82-103.ap-northeast-2.compute.internal Ready \u0026lt;none\u0026gt; 6d15h v1.21.12-eks-5308cf7 ip-10-83-84-128.ap-northeast-2.compute.internal Ready \u0026lt;none\u0026gt; 6d15h v1.21.12-eks-5308cf7  노드 레이블 조회  $ kubectl get nodes --show-labels NAME STATUS ROLES AGE VERSION LABELS ip-10-83-80-162.ap-northeast-2.compute.internal Ready \u0026lt;none\u0026gt; 6d15h v1.21.12-eks-5308cf7 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=m5.2xlarge,beta.kubernetes.io/os=linux,eks.amazonaws.com/capacityType=ON_DEMAND,eks.amazonaws.com/nodegroup-image=ami-0918f823d29c638d9,eks.amazonaws.com/nodegroup=black-nodegroup-20220708083127543500000015,eks.amazonaws.com/sourceLaunchTemplateId=lt-01141f4c6a453c7f0,eks.amazonaws.com/sourceLaunchTemplateVersion=1,failure-domain.beta.kubernetes.io/region=ap-northeast-2,failure-domain.beta.kubernetes.io/zone=ap-northeast-2a,kubernetes.io/arch=amd64,kubernetes.io/hostname=ip-10-83-80-162.ap-northeast-2.compute.internal,kubernetes.io/os=linux,node.kubernetes.io/instance-type=m5.2xlarge,topology.ebs.csi.aws.com/zone=ap-northeast-2a,topology.kubernetes.io/region=ap-northeast-2,topology.kubernetes.io/zone=ap-northeast-2a ip-10-83-82-103.ap-northeast-2.compute.internal Ready \u0026lt;none\u0026gt; 6d15h v1.21.12-eks-5308cf7 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=m5.2xlarge,beta.kubernetes.io/os=linux,eks.amazonaws.com/capacityType=ON_DEMAND,eks.amazonaws.com/nodegroup-image=ami-0918f823d29c638d9,eks.amazonaws.com/nodegroup=black-nodegroup-20220708083127543500000015,eks.amazonaws.com/sourceLaunchTemplateId=lt-01141f4c6a453c7f0,eks.amazonaws.com/sourceLaunchTemplateVersion=1,failure-domain.beta.kubernetes.io/region=ap-northeast-2,failure-domain.beta.kubernetes.io/zone=ap-northeast-2b,kubernetes.io/arch=amd64,kubernetes.io/hostname=ip-10-83-82-103.ap-northeast-2.compute.internal,kubernetes.io/os=linux,node.kubernetes.io/instance-type=m5.2xlarge,topology.ebs.csi.aws.com/zone=ap-northeast-2b,topology.kubernetes.io/region=ap-northeast-2,topology.kubernetes.io/zone=ap-northeast-2b ip-10-83-84-128.ap-northeast-2.compute.internal Ready \u0026lt;none\u0026gt; 6d15h v1.21.12-eks-5308cf7 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=m5.2xlarge,beta.kubernetes.io/os=linux,eks.amazonaws.com/capacityType=ON_DEMAND,eks.amazonaws.com/nodegroup-image=ami-0918f823d29c638d9,eks.amazonaws.com/nodegroup=black-nodegroup-20220708083127543500000015,eks.amazonaws.com/sourceLaunchTemplateId=lt-01141f4c6a453c7f0,eks.amazonaws.com/sourceLaunchTemplateVersion=1,failure-domain.beta.kubernetes.io/region=ap-northeast-2,failure-domain.beta.kubernetes.io/zone=ap-northeast-2c,kubernetes.io/arch=amd64,kubernetes.io/hostname=ip-10-83-84-128.ap-northeast-2.compute.internal,kubernetes.io/os=linux,node.kubernetes.io/instance-type=m5.2xlarge,topology.ebs.csi.aws.com/zone=ap-northeast-2c,topology.kubernetes.io/region=ap-northeast-2,topology.kubernetes.io/zone=ap-northeast-2c  호스트 이름으로 조회  $ kubectl describe nodes ip-10-83-80-162.ap-northeast-2.compute.internal Name: ip-10-83-80-162.ap-northeast-2.compute.internal Roles: \u0026lt;none\u0026gt; Labels: beta.kubernetes.io/arch=amd64 beta.kubernetes.io/instance-type=m5.2xlarge beta.kubernetes.io/os=linux eks.amazonaws.com/capacityType=ON_DEMAND eks.amazonaws.com/nodegroup=black-nodegroup-20220708083127543500000015 eks.amazonaws.com/nodegroup-image=ami-0918f823d29c638d9 eks.amazonaws.com/sourceLaunchTemplateId=lt-01141f4c6a453c7f0 eks.amazonaws.com/sourceLaunchTemplateVersion=1 failure-domain.beta.kubernetes.io/region=ap-northeast-2 failure-domain.beta.kubernetes.io/zone=ap-northeast-2a kubernetes.io/arch=amd64 kubernetes.io/hostname=ip-10-83-80-162.ap-northeast-2.compute.internal kubernetes.io/os=linux node.kubernetes.io/instance-type=m5.2xlarge topology.ebs.csi.aws.com/zone=ap-northeast-2a topology.kubernetes.io/region=ap-northeast-2 topology.kubernetes.io/zone=ap-northeast-2a Annotations: csi.volume.kubernetes.io/nodeid: {\u0026#34;ebs.csi.aws.com\u0026#34;:\u0026#34;i-0f255f242c1b4616e\u0026#34;,\u0026#34;efs.csi.aws.com\u0026#34;:\u0026#34;i-0f255f242c1b4616e\u0026#34;} node.alpha.kubernetes.io/ttl: 0 volumes.kubernetes.io/controller-managed-attach-detach: true CreationTimestamp: Fri, 08 Jul 2022 17:32:26 +0900 Taints: \u0026lt;none\u0026gt; Unschedulable: false Lease: HolderIdentity: ip-10-83-80-162.ap-northeast-2.compute.internal AcquireTime: \u0026lt;unset\u0026gt; RenewTime: Fri, 15 Jul 2022 08:58:03 +0900 Conditions: Type Status LastHeartbeatTime LastTransitionTime Reason Message ---- ------ ----------------- ------------------ ------ ------- MemoryPressure False Fri, 15 Jul 2022 08:55:51 +0900 Fri, 08 Jul 2022 17:32:26 +0900 KubeletHasSufficientMemory kubelet has sufficient memory available DiskPressure False Fri, 15 Jul 2022 08:55:51 +0900 Fri, 08 Jul 2022 17:32:26 +0900 KubeletHasNoDiskPressure kubelet has no disk pressure PIDPressure False Fri, 15 Jul 2022 08:55:51 +0900 Fri, 08 Jul 2022 17:32:26 +0900 KubeletHasSufficientPID kubelet has sufficient PID available Ready True Fri, 15 Jul 2022 08:55:51 +0900 Fri, 08 Jul 2022 17:32:47 +0900 KubeletReady kubelet is posting ready status Addresses: InternalIP: 10.83.80.162 Hostname: ip-10-83-80-162.ap-northeast-2.compute.internal InternalDNS: ip-10-83-80-162.ap-northeast-2.compute.internal Capacity: attachable-volumes-aws-ebs: 25 cpu: 8 ephemeral-storage: 20959212Ki hugepages-1Gi: 0 hugepages-2Mi: 0 memory: 32408676Ki pods: 58 Allocatable: attachable-volumes-aws-ebs: 25 cpu: 7910m ephemeral-storage: 18242267924 hugepages-1Gi: 0 hugepages-2Mi: 0 memory: 31391844Ki pods: 58 System Info: Machine ID: ec2cf9fd6e8ff9955c3f7269a4a9d3da System UUID: ec2cf9fd-6e8f-f995-5c3f-7269a4a9d3da Boot ID: c48572a6-8e6d-427b-b5b4-f9f21e8e0838 Kernel Version: 5.4.196-108.356.amzn2.x86_64 OS Image: Amazon Linux 2 Operating System: linux Architecture: amd64 Container Runtime Version: docker://20.10.13 Kubelet Version: v1.21.12-eks-5308cf7 Kube-Proxy Version: v1.21.12-eks-5308cf7 ProviderID: aws:///ap-northeast-2a/i-0f255f242c1b4616e Non-terminated Pods: (19 in total) Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits Age --------- ---- ------------ ---------- --------------- ------------- --- code-server code-server-84f85bfbb7-q9rs5 0 (0%) 0 (0%) 0 (0%) 0 (0%) 42h gatekeeper-system gatekeeper-controller-manager-77768dcc76-fmggf 100m (1%) 1 (12%) 256Mi (0%) 512Mi (1%) 19h kube-system alb-controller-aws-load-balancer-controller-579798fdbf-5w8zb 0 (0%) 0 (0%) 0 (0%) 0 (0%) 6d14h kube-system aws-cluster-autoscaler-84bd9c55fb-k28ps 0 (0%) 0 (0%) 0 (0%) 0 (0%) 6d14h kube-system aws-node-lsfh6 25m (0%) 0 (0%) 0 (0%) 0 (0%) 6d15h kube-system coredns-6dbb778559-5vn52 100m (1%) 0 (0%) 70Mi (0%) 170Mi (0%) 6d15h kube-system ebs-csi-node-q6df6 0 (0%) 0 (0%) 0 (0%) 0 (0%) 6d14h kube-system efs-csi-controller-664994d876-9wqqx 0 (0%) 0 (0%) 0 (0%) 0 (0%) 17h kube-system efs-csi-node-4x5tj 0 (0%) 0 (0%) 0 (0%) 0 (0%) 17h kube-system kube-proxy-26nsg 100m (1%) 0 (0%) 0 (0%) 0 (0%) 6d15h linkerd-viz tap-766dd477f8-x2m94 200m (2%) 100m (1%) 70Mi (0%) 500Mi (1%) 16h linkerd linkerd-destination-7c8564ff97-g8gf6 300m (3%) 100m (1%) 120Mi (0%) 750Mi (2%) 16h linkerd linkerd-identity-67bcfd69d4-l94zv 200m (2%) 100m (1%) 30Mi (0%) 500Mi (1%) 16h linkerd linkerd-proxy-injector-6f464ddc76-98wj4 200m (2%) 100m (1%) 70Mi (0%) 500Mi (1%) 16h litmus subscriber-cd959f546-4g8kx 125m (1%) 225m (2%) 300Mi (0%) 500Mi (1%) 21h litmus workflow-controller-856d568f68-wt6dj 125m (1%) 225m (2%) 300Mi (0%) 500Mi (1%) 21h log-stack fluentd-ccwzk 300m (3%) 300m (3%) 1Gi (3%) 1Gi (3%) 46h log-stack opensearch-cluster-coordinate-0 1 (12%) 1 (12%) 2Gi (6%) 2Gi (6%) 46h log-stack opensearch-cluster-master-0 1 (12%) 1 (12%) 2Gi (6%) 2Gi (6%) 46h Allocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) Resource Requests Limits -------- -------- ------ cpu 3775m (47%) 4150m (52%) memory 6336Mi (20%) 9052Mi (29%) ephemeral-storage 1000Mi (5%) 2Gi (11%) hugepages-1Gi 0 (0%) 0 (0%) hugepages-2Mi 0 (0%) 0 (0%) attachable-volumes-aws-ebs 0 0 Events: \u0026lt;none\u0026gt;  적용된 레이블 확인  Labels: beta.kubernetes.io/arch=amd64 beta.kubernetes.io/instance-type=m5.2xlarge beta.kubernetes.io/os=linux eks.amazonaws.com/capacityType=ON_DEMAND eks.amazonaws.com/nodegroup=black-nodegroup-20220708083127543500000015 eks.amazonaws.com/nodegroup-image=ami-0918f823d29c638d9 eks.amazonaws.com/sourceLaunchTemplateId=lt-01141f4c6a453c7f0 eks.amazonaws.com/sourceLaunchTemplateVersion=1 failure-domain.beta.kubernetes.io/region=ap-northeast-2 failure-domain.beta.kubernetes.io/zone=ap-northeast-2a kubernetes.io/arch=amd64 kubernetes.io/hostname=ip-10-83-80-162.ap-northeast-2.compute.internal kubernetes.io/os=linux node.kubernetes.io/instance-type=m5.2xlarge topology.ebs.csi.aws.com/zone=ap-northeast-2a topology.kubernetes.io/region=ap-northeast-2 topology.kubernetes.io/zone=ap-northeast-2a  현재 스케줄링중인 pod 상태  Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits Age --------- ---- ------------ ---------- --------------- ------------- --- code-server code-server-84f85bfbb7-q9rs5 0 (0%) 0 (0%) 0 (0%) 0 (0%) 42h gatekeeper-system gatekeeper-controller-manager-77768dcc76-fmggf 100m (1%) 1 (12%) 256Mi (0%) 512Mi (1%) 19h kube-system alb-controller-aws-load-balancer-controller-579798fdbf-5w8zb 0 (0%) 0 (0%) 0 (0%) 0 (0%) 6d14h kube-system aws-cluster-autoscaler-84bd9c55fb-k28ps 0 (0%) 0 (0%) 0 (0%) 0 (0%) 6d14h kube-system aws-node-lsfh6 25m (0%) 0 (0%) 0 (0%) 0 (0%) 6d15h kube-system coredns-6dbb778559-5vn52 100m (1%) 0 (0%) 70Mi (0%) 170Mi (0%) 6d15h kube-system ebs-csi-node-q6df6 0 (0%) 0 (0%) 0 (0%) 0 (0%) 6d14h kube-system efs-csi-controller-664994d876-9wqqx 0 (0%) 0 (0%) 0 (0%) 0 (0%) 17h kube-system efs-csi-node-4x5tj 0 (0%) 0 (0%) 0 (0%) 0 (0%) 17h kube-system kube-proxy-26nsg 100m (1%) 0 (0%) 0 (0%) 0 (0%) 6d15h linkerd-viz tap-766dd477f8-x2m94 200m (2%) 100m (1%) 70Mi (0%) 500Mi (1%) 16h linkerd linkerd-destination-7c8564ff97-g8gf6 300m (3%) 100m (1%) 120Mi (0%) 750Mi (2%) 16h linkerd linkerd-identity-67bcfd69d4-l94zv 200m (2%) 100m (1%) 30Mi (0%) 500Mi (1%) 16h linkerd linkerd-proxy-injector-6f464ddc76-98wj4 200m (2%) 100m (1%) 70Mi (0%) 500Mi (1%) 16h litmus subscriber-cd959f546-4g8kx 125m (1%) 225m (2%) 300Mi (0%) 500Mi (1%) 21h litmus workflow-controller-856d568f68-wt6dj 125m (1%) 225m (2%) 300Mi (0%) 500Mi (1%) 21h log-stack fluentd-ccwzk 300m (3%) 300m (3%) 1Gi (3%) 1Gi (3%) 46h log-stack opensearch-cluster-coordinate-0 1 (12%) 1 (12%) 2Gi (6%) 2Gi (6%) 46h log-stack opensearch-cluster-master-0 1 (12%) 1 (12%) 2Gi (6%) 2Gi (6%) 46h  리소스 사용 현황을 보자 cpu 점유율이 47%이다.  Allocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) Resource Requests Limits -------- -------- ------ cpu 3775m (47%) 4150m (52%) memory 6336Mi (20%) 9052Mi (29%) ephemeral-storage 1000Mi (5%) 2Gi (11%) hugepages-1Gi 0 (0%) 0 (0%) hugepages-2Mi 0 (0%) 0 (0%) attachable-volumes-aws-ebs 0 0 label을 이용한 특정 Node에 Pod 배포 테스트를 위해 nginx app을 1개 배포해 보자.\n 네임스페이스를 생성한다.(scheduling-test)  $ kubectl create namespace scheduling-test namespace/scheduling-test created  label 추가  kubectl label nodes [node_name] [key]=[value] $ kubectl label nodes ip-10-83-80-162.ap-northeast-2.compute.internal key=mytest-node node/ip-10-83-80-162.ap-northeast-2.compute.internal labeled # 레이블 삭제 kubectl label nodes mytest-node key-  레이블 확인(key : mytest-node)  $ kubectl describe nodes ip-10-83-80-162.ap-northeast-2.compute.internal Name: ip-10-83-80-162.ap-northeast-2.compute.internal Roles: \u0026lt;none\u0026gt; Labels: beta.kubernetes.io/arch=amd64 beta.kubernetes.io/instance-type=m5.2xlarge beta.kubernetes.io/os=linux eks.amazonaws.com/capacityType=ON_DEMAND eks.amazonaws.com/nodegroup=black-nodegroup-20220708083127543500000015 eks.amazonaws.com/nodegroup-image=ami-0918f823d29c638d9 eks.amazonaws.com/sourceLaunchTemplateId=lt-01141f4c6a453c7f0 eks.amazonaws.com/sourceLaunchTemplateVersion=1 failure-domain.beta.kubernetes.io/region=ap-northeast-2 failure-domain.beta.kubernetes.io/zone=ap-northeast-2a key=mytest-node kubernetes.io/arch=amd64 kubernetes.io/hostname=ip-10-83-80-162.ap-northeast-2.compute.internal kubernetes.io/os=linux node.kubernetes.io/instance-type=m5.2xlarge topology.ebs.csi.aws.com/zone=ap-northeast-2a topology.kubernetes.io/region=ap-northeast-2 topology.kubernetes.io/zone=ap-northeast-2a  scheduling.yaml  apiVersion: apps/v1 kind: Deployment metadata: name: my-nginx namespace: scheduling-test labels: app: my-nginx spec: replicas: 3 selector: matchLabels: app: my-nginx template: metadata: labels: app: my-nginx spec: containers: - name: my-nginx image: nginx:1.14.2 ports: - containerPort: 80 resources: requests: cpu: \u0026#34;500m\u0026#34; limits: cpu: \u0026#34;1000m\u0026#34; nodeSelector: key: mytest-node --- apiVersion: v1 kind: Service metadata: name: my-nginx namespace: scheduling-test labels: run: my-nginx spec: ports: - port: 80 targetPort: 80 protocol: TCP selector: app: my-nginx --- apiVersion: extensions/v1beta1 kind: Ingress metadata: name: scheduling-ingress namespace: scheduling-test annotations: kubernetes.io/ingress.class: nginx spec: rules: - host: nginxtest.black.cloud.hancom.com http: paths: - backend: serviceName: my-nginx servicePort: 80  배포  $ kubectl apply -f .\\nodeselectortest.yaml deployment.apps/my-nginx created service/my-nginx created ingress.extensions/scheduling-ingress created  pod 상태 조회  $ kubectl get pods -n scheduling-test NAME READY STATUS RESTARTS AGE my-nginx-5b5f4bdd49-qwk7x 1/1 Running 0 9s  node 상태 조회  ...... Non-terminated Pods: (20 in total) Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits Age --------- ---- ------------ ---------- --------------- ------------- --- code-server code-server-84f85bfbb7-q9rs5 0 (0%) 0 (0%) 0 (0%) 0 (0%) 43h gatekeeper-system gatekeeper-controller-manager-77768dcc76-fmggf 100m (1%) 1 (12%) 256Mi (0%) 512Mi (1%) 20h kube-system alb-controller-aws-load-balancer-controller-579798fdbf-5w8zb 0 (0%) 0 (0%) 0 (0%) 0 (0%) 6d15h kube-system aws-cluster-autoscaler-84bd9c55fb-k28ps 0 (0%) 0 (0%) 0 (0%) 0 (0%) 6d15h kube-system aws-node-lsfh6 25m (0%) 0 (0%) 0 (0%) 0 (0%) 6d15h kube-system coredns-6dbb778559-5vn52 100m (1%) 0 (0%) 70Mi (0%) 170Mi (0%) 6d16h kube-system ebs-csi-node-q6df6 0 (0%) 0 (0%) 0 (0%) 0 (0%) 6d15h kube-system efs-csi-controller-664994d876-9wqqx 0 (0%) 0 (0%) 0 (0%) 0 (0%) 17h kube-system efs-csi-node-4x5tj 0 (0%) 0 (0%) 0 (0%) 0 (0%) 17h kube-system kube-proxy-26nsg 100m (1%) 0 (0%) 0 (0%) 0 (0%) 6d15h linkerd-viz tap-766dd477f8-x2m94 200m (2%) 100m (1%) 70Mi (0%) 500Mi (1%) 16h linkerd linkerd-destination-7c8564ff97-g8gf6 300m (3%) 100m (1%) 120Mi (0%) 750Mi (2%) 16h linkerd linkerd-identity-67bcfd69d4-l94zv 200m (2%) 100m (1%) 30Mi (0%) 500Mi (1%) 16h linkerd linkerd-proxy-injector-6f464ddc76-98wj4 200m (2%) 100m (1%) 70Mi (0%) 500Mi (1%) 16h litmus subscriber-cd959f546-4g8kx 125m (1%) 225m (2%) 300Mi (0%) 500Mi (1%) 22h litmus workflow-controller-856d568f68-wt6dj 125m (1%) 225m (2%) 300Mi (0%) 500Mi (1%) 22h log-stack fluentd-ccwzk 300m (3%) 300m (3%) 1Gi (3%) 1Gi (3%) 47h log-stack opensearch-cluster-coordinate-0 1 (12%) 1 (12%) 2Gi (6%) 2Gi (6%) 47h log-stack opensearch-cluster-master-0 1 (12%) 1 (12%) 2Gi (6%) 2Gi (6%) 47h scheduling-test my-nginx-5b5f4bdd49-qwk7x 500m (6%) 1 (12%) 0 (0%) 0 (0%) 90s Allocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) Resource Requests Limits -------- -------- ------ cpu 4275m (54%) 5150m (65%) memory 6336Mi (20%) 9052Mi (29%) ephemeral-storage 1000Mi (5%) 2Gi (11%) hugepages-1Gi 0 (0%) 0 (0%) hugepages-2Mi 0 (0%) 0 (0%) attachable-volumes-aws-ebs 0 0 Deployment를 수정해 리소스 제한 요청을 늘려보자  기존 리소스 삭제  $ kubectl delete -f nodeselectortest.yaml deployment.apps \u0026#34;my-nginx\u0026#34; deleted service \u0026#34;my-nginx\u0026#34; deleted ingress.extensions \u0026#34;scheduling-ingress\u0026#34; deleted resources: requests: cpu: \u0026#34;5000m\u0026#34; limits: cpu: \u0026#34;6000m\u0026#34;  현재 상태 확인  $ kubectl describe nodes ip-10-83-80-162.ap-northeast-2.compute.internal ..... Non-terminated Pods: (19 in total) Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits Age --------- ---- ------------ ---------- --------------- ------------- --- code-server code-server-84f85bfbb7-q9rs5 0 (0%) 0 (0%) 0 (0%) 0 (0%) 43h gatekeeper-system gatekeeper-controller-manager-77768dcc76-fmggf 100m (1%) 1 (12%) 256Mi (0%) 512Mi (1%) 20h kube-system alb-controller-aws-load-balancer-controller-579798fdbf-5w8zb 0 (0%) 0 (0%) 0 (0%) 0 (0%) 6d15h kube-system aws-cluster-autoscaler-84bd9c55fb-k28ps 0 (0%) 0 (0%) 0 (0%) 0 (0%) 6d15h kube-system aws-node-lsfh6 25m (0%) 0 (0%) 0 (0%) 0 (0%) 6d16h kube-system coredns-6dbb778559-5vn52 100m (1%) 0 (0%) 70Mi (0%) 170Mi (0%) 6d16h kube-system ebs-csi-node-q6df6 0 (0%) 0 (0%) 0 (0%) 0 (0%) 6d15h kube-system efs-csi-controller-664994d876-9wqqx 0 (0%) 0 (0%) 0 (0%) 0 (0%) 18h kube-system efs-csi-node-4x5tj 0 (0%) 0 (0%) 0 (0%) 0 (0%) 18h kube-system kube-proxy-26nsg 100m (1%) 0 (0%) 0 (0%) 0 (0%) 6d16h linkerd-viz tap-766dd477f8-x2m94 200m (2%) 100m (1%) 70Mi (0%) 500Mi (1%) 16h linkerd linkerd-destination-7c8564ff97-g8gf6 300m (3%) 100m (1%) 120Mi (0%) 750Mi (2%) 16h linkerd linkerd-identity-67bcfd69d4-l94zv 200m (2%) 100m (1%) 30Mi (0%) 500Mi (1%) 16h linkerd linkerd-proxy-injector-6f464ddc76-98wj4 200m (2%) 100m (1%) 70Mi (0%) 500Mi (1%) 16h litmus subscriber-cd959f546-4g8kx 125m (1%) 225m (2%) 300Mi (0%) 500Mi (1%) 22h litmus workflow-controller-856d568f68-wt6dj 125m (1%) 225m (2%) 300Mi (0%) 500Mi (1%) 22h log-stack fluentd-ccwzk 300m (3%) 300m (3%) 1Gi (3%) 1Gi (3%) 47h log-stack opensearch-cluster-coordinate-0 1 (12%) 1 (12%) 2Gi (6%) 2Gi (6%) 47h log-stack opensearch-cluster-master-0 1 (12%) 1 (12%) 2Gi (6%) 2Gi (6%) 47h Allocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) Resource Requests Limits -------- -------- ------ cpu 3775m (47%) 4150m (52%) memory 6336Mi (20%) 9052Mi (29%) ephemeral-storage 1000Mi (5%) 2Gi (11%) hugepages-1Gi 0 (0%) 0 (0%) hugepages-2Mi 0 (0%) 0 (0%) attachable-volumes-aws-ebs 0 0 Events: \u0026lt;none\u0026gt;  배포후 pod 상태 확인  $ kubectl apply -f nodeselectortest.yaml deployment.apps/my-nginx created service/my-nginx created ingress.extensions/scheduling-ingress created $ kubectl get pods -n scheduling-test NAME READY STATUS RESTARTS AGE my-nginx-7cbcf9fb8d-lqcpg 0/1 Pending 0 13s  로그 조회  $ kubectl describe pods/my-nginx-7cbcf9fb8d-lqcpg -n scheduling-test Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 5m28s (x2 over 5m29s) default-scheduler 0/3 nodes are available: 1 Insufficient cpu, 2 node(s) didn\u0026#39;t match Pod\u0026#39;s node affinity/selector. Normal TriggeredScaleUp 5m26s cluster-autoscaler pod triggered scale-up: [{eks-black-nodegroup-20220708083127543500000015-58c0ee77-b6ac-cd2a-5d95-c6d8c8c059b3 3-\u0026gt;4 (max: 6)}] Warning FailedScheduling 4m15s (x3 over 4m43s) default-scheduler 0/4 nodes are available: 1 Insufficient cpu, 1 node(s) had taint {node.kubernetes.io/not-ready: }, that the pod didn\u0026#39;t tolerate, 2 node(s) didn\u0026#39;t match Pod\u0026#39;s node affinity/selector. Warning FailedScheduling 3m55s (x2 over 4m5s) default-scheduler 0/4 nodes are available: 1 Insufficient cpu, 3 node(s) didn\u0026#39;t match Pod\u0026#39;s node affinity/selector. Warning FailedScheduling 2m51s (x3 over 3m19s) default-scheduler 0/5 nodes are available: 1 Insufficient cpu, 1 node(s) had taint {node.kubernetes.io/not-ready: }, that the pod didn\u0026#39;t tolerate, 3 node(s) didn\u0026#39;t match Pod\u0026#39;s node affinity/selector. Warning FailedScheduling 2m31s (x2 over 2m41s) default-scheduler 0/5 nodes are available: 1 Insufficient cpu, 4 node(s) didn\u0026#39;t match Pod\u0026#39;s node affinity/selector. Warning FailedScheduling 93s (x3 over 2m1s) default-scheduler 0/6 nodes are available: 1 Insufficient cpu, 1 node(s) had taint {node.kubernetes.io/not-ready: }, that the pod didn\u0026#39;t tolerate, 4 node(s) didn\u0026#39;t match Pod\u0026#39;s node affinity/selector. Warning FailedScheduling 72s (x2 over 82s) default-scheduler 0/6 nodes are available: 1 Insufficient cpu, 5 node(s) didn\u0026#39;t match Pod\u0026#39;s node affinity/selector. Normal NotTriggerScaleUp 24s (x10 over 2m35s) cluster-autoscaler pod didn\u0026#39;t trigger scale-up: 1 max node group size reached 확인 결과 Running중인 pod의 드레인이나 리스케줄링 같은 특이상황은 관찰되지 않았으며, 리소스 부족으로 오토스케일링이 발생하여 max 값까지 노드들이 증가하지만 레이블이 존재하지 않으므로 pod 배포에 실패한다.\npriorityClassName 적용후 동작 확인  priorityClass 적용  nodeSelector: key: mytest-node priorityClassName: system-cluster-critical  배포 성공 및 기존 pod 드레인 확인  Non-terminated Pods: (16 in total) Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits Age --------- ---- ------------ ---------- --------------- ------------- --- code-server code-server-84f85bfbb7-q9rs5 0 (0%) 0 (0%) 0 (0%) 0 (0%) 43h gatekeeper-system gatekeeper-controller-manager-77768dcc76-fmggf 100m (1%) 1 (12%) 256Mi (0%) 512Mi (1%) 20h kube-system alb-controller-aws-load-balancer-controller-579798fdbf-5w8zb 0 (0%) 0 (0%) 0 (0%) 0 (0%) 6d15h kube-system aws-cluster-autoscaler-84bd9c55fb-k28ps 0 (0%) 0 (0%) 0 (0%) 0 (0%) 6d15h kube-system aws-node-lsfh6 25m (0%) 0 (0%) 0 (0%) 0 (0%) 6d16h kube-system coredns-6dbb778559-5vn52 100m (1%) 0 (0%) 70Mi (0%) 170Mi (0%) 6d16h kube-system ebs-csi-node-q6df6 0 (0%) 0 (0%) 0 (0%) 0 (0%) 6d15h kube-system efs-csi-controller-664994d876-9wqqx 0 (0%) 0 (0%) 0 (0%) 0 (0%) 18h kube-system efs-csi-node-4x5tj 0 (0%) 0 (0%) 0 (0%) 0 (0%) 18h kube-system kube-proxy-26nsg 100m (1%) 0 (0%) 0 (0%) 0 (0%) 6d16h litmus subscriber-cd959f546-4g8kx 125m (1%) 225m (2%) 300Mi (0%) 500Mi (1%) 22h litmus workflow-controller-856d568f68-wt6dj 125m (1%) 225m (2%) 300Mi (0%) 500Mi (1%) 22h log-stack fluentd-ccwzk 300m (3%) 300m (3%) 1Gi (3%) 1Gi (3%) 47h log-stack opensearch-cluster-coordinate-0 1 (12%) 1 (12%) 2Gi (6%) 2Gi (6%) 47h log-stack opensearch-cluster-master-0 1 (12%) 1 (12%) 2Gi (6%) 2Gi (6%) 47h scheduling-test my-nginx-7c9849cffb-nr6gn 5 (63%) 6 (75%) 0 (0%) 0 (0%) 99s Allocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) Resource Requests Limits -------- -------- ------ cpu 7875m (99%) 9750m (123%) memory 6046Mi (19%) 6802Mi (22%) ephemeral-storage 1000Mi (5%) 2Gi (11%) hugepages-1Gi 0 (0%) 0 (0%) hugepages-2Mi 0 (0%) 0 (0%) attachable-volumes-aws-ebs 0 0 Events: \u0026lt;none\u0026gt; 결론 우선순위가 낮은 pod들이 이동되는것을 확인함\n"
},
{
	"uri": "https://hnc-hskim.github.io/workshop/ide/vscode/",
	"title": "vscode",
	"tags": [],
	"description": "",
	"content": "\rcode server를 활용한 실습 환경 구성이 가능한지 검증해보자.\n\rReferences  [참고1] https://www.sobyte.net/post/2021-12/deploy-vscode-on-k8s/ [참고2] https://github.com/coder/coder [참고3] https://blog.59s.io/install-code-server [참고4] https://coder.com/docs/code-server/latest/helm [참고5] https://github.com/coder/code-server  Local VS-Code 환경을 Kubernetes에 배포하기  Dockerfile  FROM codercom/code-server COPY .vscode /home/coder/.local/share/code-server RUN curl -Lo shellcheck-v0.7.1.linux.aarch64.tar.xz https://github.com/koalaman/shellcheck/releases/download/v0.7.1/shellcheck-v0.7.1.linux.aarch64.tar.xz \\ \u0026amp;\u0026amp; tar -xvf shellcheck-v0.7.1.linux.aarch64.tar.xz \\ \u0026amp;\u0026amp; chmod +x shellcheck-v0.7.1/shellcheck \u0026amp;\u0026amp; sudo mv shellcheck-v0.7.1/shellcheck /usr/local/bin/ \\ \u0026amp;\u0026amp; rm -rf shellcheck* \u0026amp;\u0026amp; sudo chown -R coder:coder /home/coder/.local/share/code-server \\ \u0026amp;\u0026amp; curl -LO https://storage.googleapis.com/kubernetes-release/release/`curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/linux/amd64/kubectl \\ \u0026amp;\u0026amp; chmod +x kubectl \u0026amp;\u0026amp; sudo mv kubectl /usr/local/bin/  ingress.yaml  --- apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: code-server namespace: code-server annotations: kubernetes.io/ingress.class: nginx cert-manager.io/cluster-issuer: letsencrypt-prod nginx.ingress.kubernetes.io/force-ssl-redirect: \u0026#34;true\u0026#34; kubernetes.io/tls-acme: \u0026#34;true\u0026#34; spec: tls: - secretName: code-server hosts: - code-server.my-domain.com rules: - host: code-server.my-domain.com http: paths: - backend: serviceName: code-server servicePort: 8080 VS code extentions  cloud code kubernetes  아래 명세서를 파일로 저장후 배포한다. (패스워드및 네임스페이스 확인)  code-server.yaml  apiVersion: v1 kind: Namespace metadata: name: code-server --- apiVersion: v1 kind: Service metadata: name: code-server namespace: code-server spec: ports: - port: 80 targetPort: 8080 selector: app: code-server --- apiVersion: apps/v1 kind: Deployment metadata: labels: app: code-server name: code-server namespace: code-server spec: selector: matchLabels: app: code-server template: metadata: labels: app: code-server spec: containers: - image: codercom/code-server imagePullPolicy: IfNotPresent name: code-server ports: - containerPort: 8080 env: - name: PASSWORD value: \u0026#34;your password\u0026#34; --- apiVersion: extensions/v1beta1 kind: Ingress metadata: name: code-server-ingress namespace: code-server annotations: kubernetes.io/ingress.class: nginx spec: rules: - host: \u0026#34;your_host_name.example.com\u0026#34; http: paths: - backend: serviceName: code-server servicePort: 80 ingress 확인(Host에 접속) kubectl get ingress -n code-server NAME CLASS HOSTS ADDRESS PORTS AGE code-server-ingress \u0026lt;none\u0026gt; code.**********.com a**************1-716630360.ap-northeast-2.elb.amazonaws.com 80 16m 접속 화면 web vs code 화면 sample HTML  Extension을 선택하고 Live Preview 검색후 설치한다.  version 확인 $ uname -a Linux code-server-84f85bfbb7-q9rs5 5.4.196-108.356.amzn2.x86_64 #1 SMP Thu May 26 12:49:47 UTC 2022 x86_64 GNU/Linux docker 설치 $ sudo apt-get install -y wget $ sudo wget -qO- http://get.docker.com/ | sh 루트가 아닌 사용자로 docker 관리 [참고] https://docs.docker.com/engine/security/rootless/\n# 시스템 전체의 Docker 데몬이 이미 실행 중인 경우 비활성화하는 것이 좋습니다. $ sudo systemctl disable --now docker.service docker.socket $ dockerd-rootless-setuptool.sh install sudo sh -eux \u0026lt;\u0026lt;EOF # Install newuidmap \u0026amp; newgidmap binaries apt-get install -y uidmap # Load ip_tables module modprobe ip_tables EOF $ docker run hello-world docker: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?. See \u0026#39;docker run --help\u0026#39;. # 도커 설치시 그룹은 생성되어 있을수 있다. $ sudo groupadd docker $ sudo usermod -aG docker $USER # 그룹에 대한 변경 사항을 활성화 $ newgrp docker node.js 설치  터미널을 열고 다음 항목을 붙여 넣는다.  curl -sL https://deb.nodesource.com/setup_14.x | sudo -E bash - sudo apt-get install -y nodejs "
},
{
	"uri": "https://hnc-hskim.github.io/hugo/style/",
	"title": "Style",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://hnc-hskim.github.io/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Contents  kubernetes  CLI   workshop  MSA    A notice disclaimer\n\rAn information disclaimer\n\rA tip disclaimer\n\rA warning disclaimer\n\r"
},
{
	"uri": "https://hnc-hskim.github.io/kubernetes/",
	"title": "Kubernetes",
	"tags": [],
	"description": "",
	"content": "CLI 사용법 "
},
{
	"uri": "https://hnc-hskim.github.io/kubernetes/aws-cli/",
	"title": "Aws Cli",
	"tags": [],
	"description": "",
	"content": "aws 계정 정보 조회 aws sts get-caller-identity --profile \u0026#34;name\u0026#34; kubeconfig 등록 aws eks --profile \u0026#34;profile name\u0026#34; update-kubeconfig --name \u0026#34;cluster name\u0026#34; --region ap-northeast-2 "
},
{
	"uri": "https://hnc-hskim.github.io/kubernetes/cli/",
	"title": "Cli",
	"tags": [],
	"description": "",
	"content": "컨텍스트 조회 # 조회 kubectl config get-contexts # 사용 kubectl config use-context \u0026#34;context name\u0026#34; 클러스터명 조회 kubectl config view --minify -o jsonpath=\u0026#39;{.clusters[].name}\u0026#39; 노드에 레이블 추가 # 레이블 추가 kubectl label nodes \u0026lt;your-node-name\u0026gt; disktype=ssd # 레이블 확인 kubectl get nodes --show-labels Evicted pod 제거 kubectl delete pods --field-selector=status.phase=Failed -A node describe kubectl describe nodes \u0026#34;nodes-name\u0026#34; "
},
{
	"uri": "https://hnc-hskim.github.io/workshop/msa/",
	"title": "Msa",
	"tags": [],
	"description": "",
	"content": "[netflex microservice] https://netflixtechblog.com/tagged/microservices\n마이크로서비스는 소프트웨어가 잘 정의된 API를 통해 통신하는 소규모의 독립적인 서비스로 구성되어 있는 경우의 소프트웨어 개발을 위한 아키텍처 및 조직적 접근 방식입니다.\n이러한 서비스는 독립적인 소규모 팀에서 보유합니다. 마이크로서비스 아키텍처는 애플리케이션의 확장을 용이하게 하고 개발 속도를 앞당겨 혁신을 실현하고 새로운 기능의 출시 시간을 단축할 수 있게 해 줍니다.\n\rMain featrues 1. MSA 소개 2. Software Architecture 3. 10 Common Software Architectural Patterns Goals  소프트웨어 아키텍처가 필요한 이유 모놀리식 아키텍처 계층화된 아키텍처 마이크로 서비스 아키텍처  마이크로서비스 아키텍처의 경우, 애플리케이션이 독립적인 구성 요소로 구축되어 각 애플리케이션 프로세스가 서비스로 실행됩니다. 이러한 서비스는 경량 API를 사용하여 잘 정의된 인터페이스를 통해 통신합니다. 서비스는 비즈니스 기능을 위해 구축되며 서비스마다 한 가지 기능을 수행합니다. 서비스가 독립적으로 실행되기 때문에 애플리케이션의 특정 기능에 대한 수요를 충족하도록 각각의 서비스를 업데이트, 배포 및 확장할 수 있습니다.\n[출처] https://aws.amazon.com/ko/microservices/\nbluewhale-users\n"
},
{
	"uri": "https://hnc-hskim.github.io/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://hnc-hskim.github.io/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]