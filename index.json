[
{
	"uri": "https://hnc-hskim.github.io/workshop/msa/definition/",
	"title": "Definition",
	"tags": [],
	"description": "",
	"content": "[출처] https://ko.wikipedia.org/wiki/%EB%A7%88%EC%9D%B4%ED%81%AC%EB%A1%9C%EC%84%9C%EB%B9%84%EC%8A%A4\n정의 마이크로서비스(microservice)는 애플리케이션을 느슨하게 결합된 서비스의 모임으로 구조화하는 서비스 지향 아키텍처(SOA) 스타일의 일종인 소프트웨어 개발 기법이다. 마이크로서비스 아키텍처에서 서비스들은 섬세(fine-grained)하고 프로토콜은 가벼운 편이다. 애플리케이션을 더 조그마한 여러 서비스로 분해할 때의 장점은 모듈성을 개선하고 애플리케이션의 이해, 개발, 테스트를 더 쉽게 해주고 애플리케이션 침식에 더 탄력적으로 만들어 준다. 규모가 작은 자율적인 팀들이 팀별 서비스를 독립적으로 개발, 전개, 규모 확장을 할 수 있게 함으로써 병렬로 개발할 수 있게 한다. 또, 지속적인 리팩터링을 통해 개개의 서비스 아키텍처가 하나로 병합될 수 있게 허용한다. 마이크로서비스 기반 아키텍처는 지속적 배포를 가능케 한다.\n개요 마이크로서비스의 속성과 관련하여 아직 산업적인 합의는 없으며 공식적인 정의도 없다.\n\r특징들 가운데 일부는 다음을 포함한다:\n 마이크로서비스 아키텍처(microservice architecture, MSA)의 서비스들은 HTTP와 같은 기술 불가지론적인 프로토콜을 사용하여 목표를 달성하기 위해 네트워크를 통해 통신하는 프로세스들인 경우도 있다. 그러나, 서비스들은 공유 메모리와 같은 다른 종류의 프로세스 간 통신 메커니즘을 사용할 수도 있다. 서비스들은 이를테면 OSGI 번들에서처럼 동일한 프로세스 내에서 실행할 수 있다. 마이크로서비스 아키텍처의 서비스들은 독립적인 전개(deploy)가 가능하다. 서비스의 교체가 쉽다. 서비스는 기능별로 분류된다. (예: 사용자 인터페이스 프론트엔드, 추천, 로지스틱스, 청구서 발부 등) 서비스는 최적의 조건에 부합하는 바에 따라 각기 다른 프로그래밍 언어, 데이터베이스, 하드웨어, 소프트웨어 환경을 사용하여 구현할 수 있다. 서비스들은 규모가 작고, 메시지 전달이 가능하며 컨텍스트별로 묶이며 자율적으로 개발되며 독립적으로 전개할 수 있으며 분산적이며 빌드가 되며 자동화된 프로세스들로 출시된다.  마이크로서비스 기반 아키텍처는:\n 모듈성이 있는 구조를 자연스럽게 강제한다. 자기 자신을 지속적 배포 소프트웨어 개발 프로세스에 위치시킨다. 애플리케이션의 사소한 부분의 변경은 하나 이상의 적은 수의 서비스의 다시 빌드, 재전개만을 필요로 한다. 섬세(fine-grained)한 인터페이스(독립적으로 서비스를 전개할 수 있음), 비즈니스 주도의 개발(예: 도메인 드리븐 디자인), 클라우드 애플리케이션 아키텍처, 폴리곳 프로그래밍, 퍼시스턴스, 가벼운 컨테이너 전개, 탈중심화된 지속적 배포, 전체론적인 서비스 모니터링을 갖춘 데브옵스와 같은 원칙들을 고수한다. 확장성에 이득이 되는 특징들을 제공한다.  "
},
{
	"uri": "https://hnc-hskim.github.io/workshop/msa/concept/",
	"title": "Concept",
	"tags": [],
	"description": "",
	"content": "\r아래에 사용된 예제는 NodeJS, ExpressJS, MongoDB를 이용하여 구성되었습니다.\n\rReferenes  https://www.atlassian.com/ko/microservices/microservices-architecture/microservices-vs-monolith Diagram Editor mermaid flowchart syntax  모놀리식 아키텍처는 소프트웨어 프로그램의 전통적인 모델로, 자체 포함 방식이며 다른 애플리케이션과 독립적인 통합된 유닛으로 만들어집니다. “모놀리스\u0026quot;라는 단어는 거대하고 빙하 같은 것을 의미하는 경우가 많은데, 소프트웨어 설계의 모놀리식 아키텍처도 크게 다르지 않습니다. 모놀리식 아키텍처는 모든 비즈니스 관련 사항을 함께 결합하는 하나의 코드 베이스를 갖춘 대규모의 단일 컴퓨팅 네트워크입니다.\n\r1. 모놀리식 아키텍처 VS 마이크로서비스 아키텍처 Monolithic Architecture 전통적인 모놀로식 아키텍처가 비효율적으로 보일 수 있지만 간단한 아키텍처의 경우 모놀리식 아키텍처도 충분히 활용 가능한 솔루션이라고 할 수 있습니다. 시작부터 마이크로 서비스를 고려하는것이 나쁜것은 아니지만 개발중인 애플리케이션이 충분히 복잡하지 않다면 마이크로서비스 아키텍처의 이점을 확인하기는 쉽지 않습니다. 모놀로딕 아키텍처로 시작하여 서비스 확장에 따라 마이크로 서비스 아키텍처로 리팩토링하는것이 효율적인 개발 방법일수도 있습니다.\n장점  손쉬운 배포 : 단일 실행파일 또는 디렉토리로 작성되어 배포가 쉽다. 개발 : 단일 코드베이스로 구성되어 애플리케이션 개발이 쉽다. 성능 : 단일 API를 사용하여 마이크로서비스의 여러 API가 수행하는 결과와 동일한 기능을 수행한다. 테스트 간소화 : 중앙집중식 구성으로 분산된 환경보다 End-TO-End 테스트를 더 빠르게 수행할 수 있다. 디버깅 : 모든 코드가 한곳에 있으므로 요청을 트랙킹해 문제를 찾기 더 쉽다.  애플리케이션 구조 아래의 코드를 통해 monolithic 구조를 살펴보자. (이 예는 Monolithic 아키텍처를 설명하기 위한것으로 구조화되지 않은것이 Monolithic 아키텍처의 특징으로 오해하면 안된다. )\n[monolithic architecture example github] https://github.com/zachgoll/monolithic-architecture-example-app\n이 코드에서 확인 가능한것은 애플리케이션간의 구분이 없다는것이다. app.js에서 데이터베이스, 서버 및 API 엔드포인트에 대한 연결을 확인할 수 있다.\nconst express = require(\u0026#34;express\u0026#34;); const app = express(); const mongoose = require(\u0026#34;mongoose\u0026#34;); const cors = require(\u0026#34;cors\u0026#34;); const bodyParser = require(\u0026#34;body-parser\u0026#34;);  // This will allow our presentation layer to retrieve data from this API without // running into cross-origin issues (CORS) app.use(cors()); app.use(bodyParser.json());  // ============================================ // ========== DATABASE CONNECTION =========== // ============================================ // Connect to running database mongoose.connect(  `mongodb://${process.env.DB_USER}:${process.env.DB_PW}@127.0.0.1:27017/monolithic_app_db`,  { useNewUrlParser: true } );  // User schema for mongodb const UserSchema = mongoose.Schema(  {  name: { type: String },  email: { type: String },  },  { collection: \u0026#34;users\u0026#34; } );  // Define the mongoose model for use below in method const User = mongoose.model(\u0026#34;User\u0026#34;, UserSchema);  function getUserByEmail(email, callback) {  try {  User.findOne({ email: email }, callback);  } catch (err) {  callback(err);  } }  // set the view engine to ejs app.set(\u0026#34;view engine\u0026#34;, \u0026#34;ejs\u0026#34;);  // index page app.get(\u0026#34;/\u0026#34;, function (req, res) {  res.render(\u0026#34;home\u0026#34;); });  // ============================================ // ============ API ENDPOINT ================ // ============================================ app.post(\u0026#34;/register\u0026#34;, function (req, res) {  const newUser = new User({  name: req.body.name,  email: req.body.email,  });   newUser.save((err, user) =\u0026gt; {  res.status(200).json(user);  }); });  // ============================================ // ============== SERVER ===================== // ============================================ app.listen(8080); console.log(\u0026#34;Visit app at http://localhost:8080\u0026#34;); 이 모놀리딕 애플리케이션이 확장하기 시작할 경우 빠르게 코드는 엉망이 될 것이다. 이 단계에서 대부분 마이크로서비스 아키텍처로의 전환을 선택하지만, 리팩토링을 통하여 계층화된 아키텍처로 바꾸는것을 다른 하나의 옵션으로 고민해 볼 수 있다.\nMonolithic Architecture (with better \u0026ldquo;layered\u0026rdquo; or \u0026ldquo;n-tier\u0026rdquo; design) 계층화된 아키텍처는 애플리케이션을 일반적으로 다음과 같은 레이어들로 분할할 수 있다.\n 프리젠테이션 계층(Presentation Layer) 비지니스 계층(Business Layer) 데이터 액세스 계층(Data Access Layer)  다른 형태로 다음과 같은 레이어로 분류할 수도 있다.\n Presentation Layer Application Layer Domain Layer Persistence Layer  Layered Architecture Diagram flowchart TD subgraph Presentation-Layer\rdirection LR\rAngular --- A{{Closed}}\rend\rsubgraph Business-Layer\rdirection LR\rExpress --- B{{Closed}}\rend\rsubgraph Shared-Utilities-Layer\rdirection LR\rString-Utilities --- Object-Transformation-Utilities --- C{{Open}}\rend\rsubgraph Data-Layer\rdirection LR\rMongo --- D{{Closed}}\rend Presentation-Layer -- Business-Layer\rBusiness-Layer -- Shared-Utilities-Layer\rShared-Utilities-Layer -- Data-Layer click Angular \"https://www.github.com\" _blank\r\r중요한 점은 각 레이어 구조에서 바로 아래 레이어만 사용할 수 있게 하도록 구조를 분리하는것입니다. 하지만 Utility 레이어의 경우처럼 때로는 공유하여 쓸수 있는 레이어가 필요할 수도 있습니다. 다이어그램에서 모든 레이어에서 사용할 수 있도록 열린 레이어로 생성한것을 확인할 수 있습니다.\nApplication Structure 위에서 언급한대로 계층화된 아키텍처에서는 각 계층이 바로 아래 계층만 사용할수 있다는 규칙이 있습니다. 그럼 이 중요한 규칙을 기반으로 monolothic archicture를 변경해 보겠습니다.\n 프레젠테이션 계층은 HTML 사용자 양식에서 호출합니다. 프레젠테이션 계층 자바스크립트는 양식을 처리하고 비즈니스 계층에 대한 호출을 실행합니다. 비즈니스 계층은 양식 정보를 처리하고 데이터 액세스 계층을 호출합니다. 데이터 액세스 계층은 정보를 처리하고 사용자를 위해 데이터베이스에 쿼리합니다. 데이터 액세스 계층은 비즈니스 계층에 정보를 반환합니다. 비즈니스 계층은 HTTP를 통해 프레젠테이션 계층에 정보를 반환합니다. 프레젠테이션 레이어는 새로운 정보로 뷰를 렌더링합니다.  1. 프레젠테이션 계층은 HTML 사용자 양식에서 호출합니다. \u0026lt;!-- File: home.ejs --\u0026gt;  \u0026lt;!-- On form submit, home.ejs executes the getDataFromBusinessLayer() function --\u0026gt;  \u0026lt;form id=\u0026#34;emailform\u0026#34; onsubmit=\u0026#34;getDataFromBusinessLayer()\u0026#34;\u0026gt;  \u0026lt;input name=\u0026#34;email\u0026#34; id=\u0026#34;email\u0026#34; placeholder=\u0026#34;Enter email...\u0026#34; /\u0026gt;  \u0026lt;button type=\u0026#34;submit\u0026#34;\u0026gt;Load Profile\u0026lt;/button\u0026gt; \u0026lt;/form\u0026gt; 2. 프리젠테이션 계층 자바스크립트는 양식을 처리하고 비즈니스 계층에 대한 호출을 실행합니다. // File: presentation-layer-user.js  function getDataFromBusinessLayer() {  event.preventDefault();  const email = $(\u0026#34;#email\u0026#34;).val();   // Perform the GET request to the business layer  // ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  $.ajax({  url: `http://localhost:8081/get-user/${email}`,  type: \u0026#34;GET\u0026#34;,  success: function (user) {  // Render the user object on the page  // Ommitted for brevity  },  error: function (jqXHR, textStatus, ex) {  console.log(textStatus + \u0026#34;,\u0026#34; + ex + \u0026#34;,\u0026#34; + jqXHR.responseText);  },  }); } 비즈니스 계층은 양식 정보를 처리하고 데이터 액세스 계층을 호출합니다.  // File: business-layer-user.js  app.get(\u0026#34;/get-user/:useremail\u0026#34;, function (req, res) {  // Makes a call to the data access layer  // ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  const user = User.getUserByEmail(req.params.useremail, (error, user) =\u0026gt; {  res.status(200).json({  name: user.name,  email: user.email,  profileUrl: user.profileUrl,  });  }); }); 데이터 접근 계층은 정보를 처리하고 사용자를 위해 데이터베이스에 쿼리합니다.  // File: data-layer-user.js  module.exports.getUserByEmail = (email, callback) =\u0026gt; {  try {  // Makes a call to the database  // ^^^^^^^^^^^^^^^^^^^^^^^^^^^^  User.findOne({ email: email }, callback);  } catch (err) {  callback(err);  } };  데이터 액세스 계층은 비즈니스 계층에 정보를 반환합니다.\n  비즈니스 계층은 HTTP를 통해 프레젠테이션 계층에 정보를 반환합니다.\n  프레젠테이션 레이어는 새로운 정보로 뷰를 렌더링합니다.\n  각 단계를 통해 계층이 담당하는 구체적인 의무를\n[monolithic layered architecture example github] https://github.com/zachgoll/layered-architecture-example-app\n"
},
{
	"uri": "https://hnc-hskim.github.io/workshop/msa/software-architecture-pattern/",
	"title": "Software Architecture Pattern",
	"tags": [],
	"description": "",
	"content": "Referenes  10 Common Software Architectural Patterns in a nutshell Architectural patterns  10가지 소프트웨어 아키텍처 패턴  계층화 패턴 (Layered pattern) 클라이언트-서버 패턴 (Client-server pattern) 마스터-슬레이브 패턴 (Master-slave pattern) 파이프-필터 패턴 (Pipe-filter pattern) 브로커 패턴 (Broker pattern) 피어 투 피어 패턴 (Peer-to-peer pattern) 이벤트-버스 패턴 (Event-bus pattern) 모델-뷰-컨트롤러 패턴 (Model-view-controller pattern) 블랙보드 패턴 (Blackboard pattern) 인터프리터 패턴 (Interpreter pattern)  1. 계층화 패턴 (Layered pattern) 계층화 패턴에서 흔히 볼 수 있는 4개의 계층은 다음과 같다.\n 프레젠테이션 계층 ( UI 계층 이라고도 함 ) 애플리케이션 계층 ( 서비스 계층 이라고도 함 ) 비즈니스 논리 계층 ( 도메인 계층 이라고도 함 ) 데이터 액세스 계층 ( 지속성 계층 이라고도 함 )  (1) 일반 데스크탑 애플리케이션\n(2) 전자 상거래 웹 애플리케이션\n\r2. 클라이언트-서버 패턴 (Client-server pattern) 흔히 사용되는 패턴입니다. 서버와 여러 클라이언트가 존재할 수 있습니다. 서버는 여러 클라이언트에 서비스를 제공합니다.\n 클라이언트는 서버에 서비스 요청 서버는 클라이언트에 서비스 제공 서버는 클라이언트 요청을 계속 수신  3. 마스터-슬레이브 패턴 (Master-slave pattern) 서비스 주체가 Master와 Slave로 구성됩니다. 마스터 컴포넌트는 슬래이브 컴포넌트간에 작업을 분배하고 슬래이가 반환한 결과로부터 최종 결과를 계산합니다.\n(1) 데이터베이스 복제\n\r4. 파이프-필터 패턴 (Pipe-filter pattern) 데이터스트림을 생성하고 가공하는 시스템을 구성하는데 사용할 수 있습니다. 각 단계는 필터 구성 요소로 구분되고 처리 데이터는 파이프를 통해 전달됩니다.\n(1) 컴파일러\n\r5. 브로커 패턴 (Broker pattern) 분리된 구성 요소가 있는 분산 시스템을 구성하는데 사용됩니다. 원격 서비스 호출을 통해 서로 상호작용할 수 있습니다. 브로커 구성 요소는 구성 요소간의 통신을 담당합니다.\n(1) Apache ActiveMQ\n(2) Apache Kafka\n(3) RabbitMQ\n\r6. 피어 투 피어 패턴 (Peer-to-peer pattern) 개별 구성요소는 피어로 표현하며, 각 피어는 다른 피어에게 서비스를 요청하는 클라이언트와 다른 피어에게 서비스를 제공하는 서버역할을 모두 할 수 있습니다.\n(1) 파일 공유 네트워크(Gnutella)\n(2) 블록체인 기반 상품\n\r7. 이벤트-버스 패턴 (Event-bus pattern) 이벤트 소스, 이벤트 리스터, 채널 및 이벤트 버스 4가지 구성요소로 구성됩니다. 소스는 이벤트 버스의 특정 채널에 메시지를 게시합니다. 청취자는 특정 채널을 구독합니다. 리스너트 이전에 구독한 채널에 게시된 메시지에 대한 알림을 받습니다.\n(1) 안드로이드 개발\n(2) 알림서비스\n\r8. 모델-뷰-컨트롤러 패턴 (Model-view-controller pattern) MVC 패턴으로 불리며 대화형 애플리케이션을 구성하는데 사용됩니다.\n Model : 핵심 기능 및 데이터 View : 사용자에게 정보를 표시 Controller : 사용자의 입력을 처리  (1) 웹 애플리케이션을 위한 아키텍처로 사용\n\r9. 블랙보드 패턴 (Blackboard pattern) 이 패턴은 솔루션이 결정되지 않은 문제에 유용합니다.\n blackboard : 솔루션 공간의 객체를 포함하는 구조화된 전역 메모리 지식 소스 : 고유한 표현이 있는 특수 모듈 제어 구성 요소 : 모듈을 선택하고 구성 및 실행합니다.  (1) 음성 인식\n(2) 차량 식별 및 추적\n\r10. 인터프리터 패턴 (Interpreter pattern) 이 패턴은 전용 언어로 작성된 프로그램을 해석하는 구성 요소를 설계하는데 사용됩니다. 기본 아이디어는 언어의 각 기호에 대한 클래스를 구성하는것입니다.\n(1) SQL과 같은 데이터베이스 쿼리 언어\n(2) 통신 프로토콜에 사용\n\r"
},
{
	"uri": "https://hnc-hskim.github.io/workshop/msa/software-architecture/",
	"title": "Software Architecture",
	"tags": [],
	"description": "",
	"content": "References  [참고1] https://dev.to/zachgoll/introduction-to-software-architecture-monolithic-vs-layered-vs-microservices-452 [참고2] https://github.com/mermaid-js/mermaid Diagram Editor  \u0026ldquo;Any intelligent fool can make things bigger, more complex, and more violent. It takes a touch of genius—and a lot of courage to move in the opposite direction\u0026rdquo;\n[From E.F. Schumacher\u0026rsquo;s book Small is Beautiful]\n\r1. 소프트웨어 아키텍처 소프트웨어 구조 또는 소프트웨어 아키텍처(software architecture)는 소프트웨어의 구성요소들 사이에서 유기적 관계를 표현하고 소프트웨어의 설계와 업그레이드를 통제하는 지침과 원칙이다.\n1.1 소프트웨어 아키텍처 설계시 고려사항  성능: 회전하는 \u0026ldquo;로드 중\u0026rdquo; 아이콘이 사라지기 전에 얼마나 기다려야 합니까? 가용성: 시스템이 실행되는 시간의 백분율은 무엇입니까? 사용성: 사용자가 시스템의 인터페이스를 쉽게 파악할 수 있습니까? 수정 가능성: 개발자가 시스템에 기능을 추가하려는 경우 수행하기 쉽습니까? 상호 운용성: 시스템이 다른 시스템과 원활하게 작동합니까? 보안: 시스템 주변에 보안 포트리스가 있습니까? 이식성: 시스템이 다양한 플랫폼(예: Windows, Mac, Linux)에서 실행될 수 있습니까? 확장성: 사용자 기반을 빠르게 성장시키면 시스템이 새로운 트래픽을 충족하도록 쉽게 확장할 수 있습니까? 배포 가능성: 프로덕션 환경에 새로운 기능을 추가하는 것이 쉽습니까? 안전: 소프트웨어가 물리적 사물을 제어하는 ​​경우 실제 사람에게 위험합니까?  2. 소프트웨어 아키텍처가 프로젝트의 성공에 중요한 13가지 이유 원문\r번역\r\r1. An architecture will inhibit or enable a system’s driving quality attributes. 2. The decisions made in an architecture allow you to reason about and manage change as the system evolves. 3. The analysis of an architecture enables early prediction of a system’s qualities. 4. A documented architecture enhances communication among stakeholders. 5. The architecture is a carrier of the earliest and hence most fundamental, hardest-to-change design decisions. 6. An architecture defines a set of constraints on subsequent implementation. 7. The architecture dictates the structure of an organization, or vice versa. 8. An architecture can provide the basis for evolutionary prototyping. 9. An architecture is the key artifact that allows the architect and project manager to reason about cost and schedule. 10. An architecture can be created as a transferable, reusable model that forms the heart of a product line. 11. Architecture-based development focuses attention on the assembly of components, rather than simply on their creation. 12. By restricting design alternatives, architecture channels the creativity of developers, reducing design and system complexity. 13. An architecture can be the foundation for training a new team member \r\r1. 아키텍처는 시스템의 구동 품질 속성을 억제하거나 활성화합니다. 2. 아키텍처에서 내린 결정을 통해 시스템이 발전함에 따라 변경 사항을 추론하고 관리할 수 있습니다. 3. 아키텍처 분석을 통해 시스템 품질을 조기에 예측할 수 있습니다. 4. 문서화된 아키텍처는 이해 관계자 간의 의사 소통을 향상시킵니다. 5. 아키텍처는 가장 초기에 가장 기본적이고 가장 변경하기 어려운 설계 결정의 전달자입니다. 6. 아키텍처는 후속 구현에 대한 일련의 제약 조건을 정의합니다. 7. 아키텍처는 조직의 구조를 결정하거나 그 반대의 경우도 마찬가지입니다. 8. 아키텍처는 진화적 프로토타이핑의 기초를 제공할 수 있습니다. 9. 아키텍처는 건축가와 프로젝트 관리자가 비용과 일정에 대해 추론할 수 있도록 하는 핵심 아티팩트입니다. 10. 아키텍처는 제품 라인의 핵심을 형성하는 양도 가능하고 재사용 가능한 모델로 생성될 수 있습니다. 11. 아키텍처 기반 개발은 단순히 구성 요소를 만드는 것보다 구성 요소의 조립에 주의를 집중합니다. 12. 설계 대안을 제한함으로써 아키텍처는 개발자의 창의성을 전달하여 설계 및 시스템 복잡성을 줄입니다. 13. 아키텍처는 새로운 팀원을 교육하기 위한 기반이 될 수 있습니다. \r\r\r\r\r"
},
{
	"uri": "https://hnc-hskim.github.io/workshop/",
	"title": "Workshops",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://hnc-hskim.github.io/cloud/",
	"title": "Clouds",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://hnc-hskim.github.io/cloud/rancher/monitoring/",
	"title": "Monitoring",
	"tags": [],
	"description": "",
	"content": "Rancher에 모니터링 도구 설치 및 GPU 모니터링 References  [참고1] https://nvidia.github.io/gpu-monitoring-tools/ [참고2] https://passwd.tistory.com/entry/NVIDIAgpu-monitoring-tools-dcgm-exporter-CrashLoopBackOff  rancher에서 monitoring 도구 설치   Apps -\u0026gt; Charts 이동후 monitoring 검색   Monitoring 설치 설치를 진행하면 모니터링 앱은 Rancher 의 cattle-monitoring-system namespace 에 배포됨\n(설치후 랜처 로그아웃후 다시 로그인)\n  네비게이션 영역을 보면 Monitoring 메뉴가 추가되어 있음   대쉬보드 확인   grafana 확인   그라파나에 로그인합니다. Grafana 인스턴스의 기본 관리자 사용자 이름과 비밀번호는 입니다 admin/prom-operator. (비밀번호가 있는 사람에 관계없이 Rancher의 클러스터 관리자 권한은 여전히 ​​Grafana 인스턴스에 액세스해야 합니다.) 차트를 배포하거나 업그레이드할 때 대체 자격 증명을 제공할 수도 있습니다.\n\rGPU 노드 모니터링을 위한 dcgm-exporter 설치  Helm v3 설치  curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 \u0026amp;\u0026amp; \\\rchmod 700 get_helm.sh \u0026amp;\u0026amp; \\\r./get_helm.sh  Helm 저장소 설정  helm repo add gpu-helm-charts \\\rhttps://nvidia.github.io/gpu-monitoring-tools/helm-charts  레파지터리 업데이트  helm repo update  DCGM-Exporter 공식 차트 설치  #helm install --generate-name gpu-helm-charts/dcgm-exporter\r# orca mlops node를 모니터링하기 위해 tolerations, nodeSelector 수정한 values파일로 설치한다. helm install -f dcgm-values.yaml --generate-name gpu-helm-charts/dcgm-exporter  차트 확인  $ helm search repo gpu-helm-charts\rNAME CHART VERSION APP VERSION DESCRIPTION\rgpu-helm-charts/dcgm-exporter 2.4.0 2.4.0 A Helm chart for DCGM exporter\rgpu-helm-charts/kube-prometheus 0.0.43 Manifests, dashboards, and alerting rules for e...\rgpu-helm-charts/prometheus-operator 0.0.15 Provides easy monitoring definitions for Kubern...\r$ helm inspect chart gpu-helm-charts/dcgm-exporter apiVersion: v2\rappVersion: 2.4.0\rdescription: A Helm chart for DCGM exporter\rhome: https://github.com/nvidia/gpu-monitoring-tools/\ricon: https://assets.nvidiagrid.net/ngc/logos/DCGM.png\rkeywords:\r- gpu\r- cuda\r- compute\r- monitoring\r- telemetry\r- tesla\rkubeVersion: \u0026#39;\u0026gt;= 1.13.0-0\u0026#39;\rname: dcgm-exporter\rsources:\r- https://gitlab.com/nvidia/container-toolkit/gpu-monitoring-tools\rversion: 2.4.0  dcgm-values.yaml로 저장한다.  $ helm inspect values gpu-helm-charts/dcgm-exporter # Copyright (c) 2020, NVIDIA CORPORATION. All rights reserved.\r#\r# Licensed under the Apache License, Version 2.0 (the \u0026#34;License\u0026#34;);\r# you may not use this file except in compliance with the License.\r# You may obtain a copy of the License at\r#\r# http://www.apache.org/licenses/LICENSE-2.0\r#\r# Unless required by applicable law or agreed to in writing, software\r# distributed under the License is distributed on an \u0026#34;AS IS\u0026#34; BASIS,\r# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r# See the License for the specific language governing permissions and\r# limitations under the License.\rimage:\rrepository: nvcr.io/nvidia/k8s/dcgm-exporter\rpullPolicy: IfNotPresent\r# Image tag defaults to AppVersion, but you can use the tag key\r# for the image tag, e.g:\rtag: 2.2.9-2.4.0-ubuntu18.04\r# Comment the following line to stop profiling metrics from DCGM\rarguments: [\u0026#34;-f\u0026#34;, \u0026#34;/etc/dcgm-exporter/dcp-metrics-included.csv\u0026#34;]\r# NOTE: in general, add any command line arguments to arguments above\r# and they will be passed through.\r# Use \u0026#34;-r\u0026#34;, \u0026#34;\u0026lt;HOST\u0026gt;:\u0026lt;PORT\u0026gt;\u0026#34; to connect to an already running hostengine\r# Example arguments: [\u0026#34;-r\u0026#34;, \u0026#34;host123:5555\u0026#34;]\r# Use \u0026#34;-n\u0026#34; to remove the hostname tag from the output.\r# Example arguments: [\u0026#34;-n\u0026#34;]\r# Use \u0026#34;-d\u0026#34; to specify the devices to monitor. -d must be followed by a string\r# in the following format: [f] or [g[:numeric_range][+]][i[:numeric_range]]\r# Where a numeric range is something like 0-4 or 0,2,4, etc.\r# Example arguments: [\u0026#34;-d\u0026#34;, \u0026#34;g+i\u0026#34;] to monitor all GPUs and GPU instances or\r# [\u0026#34;-d\u0026#34;, \u0026#34;g:0-3\u0026#34;] to monitor GPUs 0-3.\rimagePullSecrets: []\rnameOverride: \u0026#34;\u0026#34;\rfullnameOverride: \u0026#34;\u0026#34;\rserviceAccount:\r# Specifies whether a service account should be created\rcreate: true\r# Annotations to add to the service account\rannotations: {}\r# The name of the service account to use.\r# If not set and create is true, a name is generated using the fullname template\rname:\rpodSecurityContext: {}\r# fsGroup: 2000\rsecurityContext:\rrunAsNonRoot: false\rrunAsUser: 0\rcapabilities:\radd: [\u0026#34;SYS_ADMIN\u0026#34;]\r# readOnlyRootFilesystem: true\rservice:\rtype: ClusterIP\rport: 9400\raddress: \u0026#34;:9400\u0026#34;\r# Annotations to add to the service\rannotations: {}\rresources: {}\r# limits:\r# cpu: 100m\r# memory: 128Mi\r# requests:\r# cpu: 100m\r# memory: 128Mi\rserviceMonitor:\renabled: true\rinterval: 15s\radditionalLabels: {}\r#monitoring: prometheus\rmapPodsMetrics: false\rnodeSelector: {}\r#node: gpu\rtolerations: []\r#- operator: Exists\raffinity: {}\r#nodeAffinity:\r# requiredDuringSchedulingIgnoredDuringExecution:\r# nodeSelectorTerms:\r# - matchExpressions:\r# - key: nvidia-gpu\r# operator: Exists\rextraHostVolumes: []\r#- name: host-binaries\r# hostPath: /opt/bin\rextraVolumeMounts: []\r#- name: host-binaries\r# mountPath: /opt/bin\r# readOnly: true\rextraEnv: []\r#- name: EXTRA_VAR\r# value: \u0026#34;TheStringValue\u0026#34;  helm 삭제  $ helm ls\rNAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION dcgm-exporter-1658202339 default 1 2022-07-19 12:45:40.1694572 +0900 KST deployed dcgm-exporter-2.4.0 2.4.0\r$ helm delete dcgm-exporter-1658202339 exporter 추가후 동작 확인 $ kubectl get pods -A | grep exporter cattle-monitoring-system rancher-monitoring-prometheus-node-exporter-6q7pp 1/1 Running 0 56m\rcattle-monitoring-system rancher-monitoring-prometheus-node-exporter-8bmpz 1/1 Running 0 56m\rcattle-monitoring-system rancher-monitoring-prometheus-node-exporter-8xrk6 1/1 Running 0 56m\rcattle-monitoring-system rancher-monitoring-prometheus-node-exporter-blvhr 1/1 Running 0 56m\rcattle-monitoring-system rancher-monitoring-prometheus-node-exporter-kc4ql 1/1 Running 0 56m\rcattle-monitoring-system rancher-monitoring-prometheus-node-exporter-l56nm 1/1 Running 0 56m\rcattle-monitoring-system rancher-monitoring-prometheus-node-exporter-qkk82 1/1 Running 0 56m\rcattle-monitoring-system rancher-monitoring-prometheus-node-exporter-s654d 1/1 Running 0 56m\rcattle-monitoring-system rancher-monitoring-prometheus-node-exporter-x6hjz 1/1 Running 0 56m\rdefault dcgm-exporter-1658202339-6wwlg 0/1 CrashLoopBackOff 6 5m22s\rdefault dcgm-exporter-1658202339-ffch6 0/1 CrashLoopBackOff 6 5m22s\rdefault dcgm-exporter-1658202339-kgldt 0/1 CrashLoopBackOff 7 5m22s\rdefault dcgm-exporter-1658202339-nprlc 0/1 CrashLoopBackOff 6 5m22s CrashLoopBackoff 상태 확인  로그는 정상  $ kubectl logs dcgm-exporter-1658205662-qwhs9\rtime=\u0026#34;2022-07-19T03:50:16Z\u0026#34; level=info msg=\u0026#34;Starting dcgm-exporter\u0026#34;\rtime=\u0026#34;2022-07-19T03:50:16Z\u0026#34; level=info msg=\u0026#34;DCGM successfully initialized!\u0026#34;\rtime=\u0026#34;2022-07-19T03:50:16Z\u0026#34; level=info msg=\u0026#34;Collecting DCP Metrics\u0026#34;\rtime=\u0026#34;2022-07-19T03:50:16Z\u0026#34; level=info msg=\u0026#34;Kubernetes metrics collection enabled!\u0026#34;\rtime=\u0026#34;2022-07-19T03:50:16Z\u0026#34; level=info msg=\u0026#34;Pipeline starting\u0026#34;\rtime=\u0026#34;2022-07-19T03:50:16Z\u0026#34; level=info msg=\u0026#34;Starting webserver\u0026#34;  상태 확인  $ kubectl describe pod dcgm-exporter-1658205662-qwhs9\r......\rEvents:\rType Reason Age From Message\r---- ------ ---- ---- -------\rNormal Scheduled 29m default-scheduler Successfully assigned default/dcgm-exporter-1658205662-qwhs9 to hcidc-sv-paz-orca-worker-09\rNormal Pulled 29m kubelet Container image \u0026#34;nvcr.io/nvidia/k8s/dcgm-exporter:2.2.9-2.4.0-ubuntu18.04\u0026#34; already present on machine\rNormal Created 29m kubelet Created container exporter\rNormal Started 29m kubelet Started container exporter\rWarning Unhealthy 28m (x3 over 29m) kubelet Readiness probe failed: HTTP probe failed with statuscode: 503 CrashLoopBackOff 해결 $ kubectl edit daemonset.apps/dcgm-exporter-1658205662\r# initialDelaySeconds를 60으로 변경\r.......\rlivenessProbe:\rfailureThreshold: 3\rhttpGet:\rpath: /health\rport: 9400\rscheme: HTTP\rinitialDelaySeconds: 60\rperiodSeconds: 5\rsuccessThreshold: 1\rtimeoutSeconds: 1 grafana로 이동후 gpu dashboard 설치 DCGM Exporter Dashboard 설치\rhttps://grafana.com/grafana/dashboards/12239   Import 선택   dashboard id(12239) 입력   Prometheus 선택   Dashboard 확인   "
},
{
	"uri": "https://hnc-hskim.github.io/terraform/introduction/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "테라폼(Terraform)은 하시코프(Hashicorp)에서 오픈소스로 개발중인 클라우드 인프라스트럭처 자동화를 지향하는 코드로서의 인프라스트럭처(Infrastructure as Code), IaC1 도구입니다.\nAWS 클라우드 포메이션AWS CloudFormation의 경우 AWS만 지원하는 것과 달리 테라폼의 경우 아마존 웹 서비스, 구글 클라우드 플랫폼(Google Cloud Platform), 마이크로소프트 애저(Microsoft Azure)와 같은 주요 클라우드 서비스를 비롯한 다양한 클라우드 서비스들을 프로바이더 방식으로 제공하고 있습니다. 이를 통해 테라폼만으로 멀티 클라우드의 리소스들을 선언하고 코드로 관리하는 것도 가능합니다.\n테라폼은 고(Go) 프로그래밍 언어로 개발하고 있습니다.\n테라폼 공식 홈\n  IaC는 코드로 인프라스트럭처를 관리한다는 개념으로 테라폼에서는 하시코프 설정 언어(HCL, Hashicorp Configuration Language)을 사용해 클라우드 리소스를 선언합니다.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   "
},
{
	"uri": "https://hnc-hskim.github.io/terraform/",
	"title": "Terraforms",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://hnc-hskim.github.io/references/kubernetes/",
	"title": "Kubernetes",
	"tags": [],
	"description": "",
	"content": "참고  Advanced Scheduling  "
},
{
	"uri": "https://hnc-hskim.github.io/references/",
	"title": "References",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://hnc-hskim.github.io/references/terraform/",
	"title": "Terraform",
	"tags": [],
	"description": "",
	"content": "참고  helm values파일내 환경변수 전달 nginx controller acm 적용  "
},
{
	"uri": "https://hnc-hskim.github.io/hugo/comments/",
	"title": "Comments",
	"tags": [],
	"description": "",
	"content": "[참고] https://velog.io/@mellonggo/Github-%ED%8E%98%EC%9D%B4%EC%A7%80-%EB%B8%94%EB%A1%9C%EA%B7%B8-%EB%A7%8C%EB%93%A4%EA%B8%B0-with-Hugo\n댓글 기능 추가  리파지터리 생성  blog-comments로 리파지터리 생성  layouts/partials/custom-footer.html 파일 생성 및 스크립트 추가  테마별로 지정해야할 위치가 다를수 있다. learn 테마의 경우 post 레이아웃을 찾을수 없어 custom-footer.html에 추가한다.\n\r 아래 코드를 복하하여 custom-footer.html에 붙여넣는다.  \u0026lt;script src=\u0026#34;https://utteranc.es/client.js\u0026#34;\rrepo=\u0026#34;user id/blog-comments\u0026#34;\rissue-term=\u0026#34;title\u0026#34;\rtheme=\u0026#34;github-light\u0026#34;\rcrossorigin=\u0026#34;anonymous\u0026#34;\rasync\u0026gt;\r\u0026lt;/script\u0026gt; \r첫 로딩시 댓글 기능은 비활성화되어 있고 github 로그인 인증을 통해 해당 리파지터리에서 utterances app 사용을 승인하면 이후 댓글 기능을 사용할 수 있다. 댓글의 경우 생성한 리파지터리의 Issues 생성 기능을 통해 동작한다.\n\r댓글 삭제 해당 기능은 따로 제공하지 않는것 같다. 직접 리파지터리에서 이슈로 이동후 본인이 등록한 이슈를 삭제하자\n댓글 기능 상단의 Comment를 누르면 댓글 리파지터리의 이슈탭으로 이동한다. 이곳에서 삭제하자\n"
},
{
	"uri": "https://hnc-hskim.github.io/hugo/",
	"title": "Hugoes",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://hnc-hskim.github.io/cloud/operation/scheduling/",
	"title": "Scheduling",
	"tags": [],
	"description": "",
	"content": "노드에 파드 할당하기 특정한 노드(들) 집합에서만 동작하도록 파드를 제한할 수 있다. 이를 수행하는 방법에는 여러 가지가 있으며 권장되는 접근 방식은 모두 레이블 셀렉터를 사용하여 선택을 용이하게 한다. 보통은 스케줄러가 자동으로 합리적인 배치(예: 자원이 부족한 노드에 파드를 배치하지 않도록 노드 간에 파드를 분배)를 수행하기에 이러한 제약 조건은 필요하지 않다. 그러나, 예를 들어 SSD가 장착된 머신에 파드가 배포되도록 하거나 또는 많은 통신을 하는 두 개의 서로 다른 서비스의 파드를 동일한 가용성 영역(availability zone)에 배치하는 경우와 같이, 파드가 어느 노드에 배포될지를 제어해야 하는 경우도 있다.\n방법  노드 레이블에 매칭되는 nodeSelector 필드 어피니티 / 안티 어피니티 nodeName 필드  1. 노드 레이블 다른 쿠버네티스 오브젝트와 마찬가지로, 노드도 레이블을 가진다. 레이블을 수동으로 추가할 수 있다. 또한 쿠버네티스도 클러스터의 모든 노드에 표준화된 레이블 집합을 적용한다. 잘 알려진 레이블, 어노테이션, 테인트에서 널리 사용되는 노드 레이블의 목록을 확인한다.\n 노드 조회  $ kubectl get nodes\rNAME STATUS ROLES AGE VERSION\rip-10-83-80-162.ap-northeast-2.compute.internal Ready \u0026lt;none\u0026gt; 6d15h v1.21.12-eks-5308cf7\rip-10-83-82-103.ap-northeast-2.compute.internal Ready \u0026lt;none\u0026gt; 6d15h v1.21.12-eks-5308cf7\rip-10-83-84-128.ap-northeast-2.compute.internal Ready \u0026lt;none\u0026gt; 6d15h v1.21.12-eks-5308cf7  노드 레이블 조회  $ kubectl get nodes --show-labels\rNAME STATUS ROLES AGE VERSION LABELS\rip-10-83-80-162.ap-northeast-2.compute.internal Ready \u0026lt;none\u0026gt; 6d15h v1.21.12-eks-5308cf7 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=m5.2xlarge,beta.kubernetes.io/os=linux,eks.amazonaws.com/capacityType=ON_DEMAND,eks.amazonaws.com/nodegroup-image=ami-0918f823d29c638d9,eks.amazonaws.com/nodegroup=black-nodegroup-20220708083127543500000015,eks.amazonaws.com/sourceLaunchTemplateId=lt-01141f4c6a453c7f0,eks.amazonaws.com/sourceLaunchTemplateVersion=1,failure-domain.beta.kubernetes.io/region=ap-northeast-2,failure-domain.beta.kubernetes.io/zone=ap-northeast-2a,kubernetes.io/arch=amd64,kubernetes.io/hostname=ip-10-83-80-162.ap-northeast-2.compute.internal,kubernetes.io/os=linux,node.kubernetes.io/instance-type=m5.2xlarge,topology.ebs.csi.aws.com/zone=ap-northeast-2a,topology.kubernetes.io/region=ap-northeast-2,topology.kubernetes.io/zone=ap-northeast-2a\rip-10-83-82-103.ap-northeast-2.compute.internal Ready \u0026lt;none\u0026gt; 6d15h v1.21.12-eks-5308cf7 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=m5.2xlarge,beta.kubernetes.io/os=linux,eks.amazonaws.com/capacityType=ON_DEMAND,eks.amazonaws.com/nodegroup-image=ami-0918f823d29c638d9,eks.amazonaws.com/nodegroup=black-nodegroup-20220708083127543500000015,eks.amazonaws.com/sourceLaunchTemplateId=lt-01141f4c6a453c7f0,eks.amazonaws.com/sourceLaunchTemplateVersion=1,failure-domain.beta.kubernetes.io/region=ap-northeast-2,failure-domain.beta.kubernetes.io/zone=ap-northeast-2b,kubernetes.io/arch=amd64,kubernetes.io/hostname=ip-10-83-82-103.ap-northeast-2.compute.internal,kubernetes.io/os=linux,node.kubernetes.io/instance-type=m5.2xlarge,topology.ebs.csi.aws.com/zone=ap-northeast-2b,topology.kubernetes.io/region=ap-northeast-2,topology.kubernetes.io/zone=ap-northeast-2b\rip-10-83-84-128.ap-northeast-2.compute.internal Ready \u0026lt;none\u0026gt; 6d15h v1.21.12-eks-5308cf7 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=m5.2xlarge,beta.kubernetes.io/os=linux,eks.amazonaws.com/capacityType=ON_DEMAND,eks.amazonaws.com/nodegroup-image=ami-0918f823d29c638d9,eks.amazonaws.com/nodegroup=black-nodegroup-20220708083127543500000015,eks.amazonaws.com/sourceLaunchTemplateId=lt-01141f4c6a453c7f0,eks.amazonaws.com/sourceLaunchTemplateVersion=1,failure-domain.beta.kubernetes.io/region=ap-northeast-2,failure-domain.beta.kubernetes.io/zone=ap-northeast-2c,kubernetes.io/arch=amd64,kubernetes.io/hostname=ip-10-83-84-128.ap-northeast-2.compute.internal,kubernetes.io/os=linux,node.kubernetes.io/instance-type=m5.2xlarge,topology.ebs.csi.aws.com/zone=ap-northeast-2c,topology.kubernetes.io/region=ap-northeast-2,topology.kubernetes.io/zone=ap-northeast-2c  호스트 이름으로 조회  $ kubectl describe nodes ip-10-83-80-162.ap-northeast-2.compute.internal\rName: ip-10-83-80-162.ap-northeast-2.compute.internal\rRoles: \u0026lt;none\u0026gt;\rLabels: beta.kubernetes.io/arch=amd64\rbeta.kubernetes.io/instance-type=m5.2xlarge\rbeta.kubernetes.io/os=linux\reks.amazonaws.com/capacityType=ON_DEMAND\reks.amazonaws.com/nodegroup=black-nodegroup-20220708083127543500000015\reks.amazonaws.com/nodegroup-image=ami-0918f823d29c638d9\reks.amazonaws.com/sourceLaunchTemplateId=lt-01141f4c6a453c7f0\reks.amazonaws.com/sourceLaunchTemplateVersion=1\rfailure-domain.beta.kubernetes.io/region=ap-northeast-2\rfailure-domain.beta.kubernetes.io/zone=ap-northeast-2a\rkubernetes.io/arch=amd64\rkubernetes.io/hostname=ip-10-83-80-162.ap-northeast-2.compute.internal\rkubernetes.io/os=linux\rnode.kubernetes.io/instance-type=m5.2xlarge\rtopology.ebs.csi.aws.com/zone=ap-northeast-2a\rtopology.kubernetes.io/region=ap-northeast-2\rtopology.kubernetes.io/zone=ap-northeast-2a\rAnnotations: csi.volume.kubernetes.io/nodeid: {\u0026#34;ebs.csi.aws.com\u0026#34;:\u0026#34;i-0f255f242c1b4616e\u0026#34;,\u0026#34;efs.csi.aws.com\u0026#34;:\u0026#34;i-0f255f242c1b4616e\u0026#34;}\rnode.alpha.kubernetes.io/ttl: 0\rvolumes.kubernetes.io/controller-managed-attach-detach: true\rCreationTimestamp: Fri, 08 Jul 2022 17:32:26 +0900\rTaints: \u0026lt;none\u0026gt;\rUnschedulable: false\rLease:\rHolderIdentity: ip-10-83-80-162.ap-northeast-2.compute.internal\rAcquireTime: \u0026lt;unset\u0026gt;\rRenewTime: Fri, 15 Jul 2022 08:58:03 +0900\rConditions:\rType Status LastHeartbeatTime LastTransitionTime Reason Message\r---- ------ ----------------- ------------------ ------ -------\rMemoryPressure False Fri, 15 Jul 2022 08:55:51 +0900 Fri, 08 Jul 2022 17:32:26 +0900 KubeletHasSufficientMemory kubelet has sufficient memory available\rDiskPressure False Fri, 15 Jul 2022 08:55:51 +0900 Fri, 08 Jul 2022 17:32:26 +0900 KubeletHasNoDiskPressure kubelet has no disk pressure\rPIDPressure False Fri, 15 Jul 2022 08:55:51 +0900 Fri, 08 Jul 2022 17:32:26 +0900 KubeletHasSufficientPID kubelet has sufficient PID available\rReady True Fri, 15 Jul 2022 08:55:51 +0900 Fri, 08 Jul 2022 17:32:47 +0900 KubeletReady kubelet is posting ready status\rAddresses:\rInternalIP: 10.83.80.162\rHostname: ip-10-83-80-162.ap-northeast-2.compute.internal\rInternalDNS: ip-10-83-80-162.ap-northeast-2.compute.internal\rCapacity:\rattachable-volumes-aws-ebs: 25\rcpu: 8\rephemeral-storage: 20959212Ki\rhugepages-1Gi: 0\rhugepages-2Mi: 0\rmemory: 32408676Ki\rpods: 58\rAllocatable:\rattachable-volumes-aws-ebs: 25\rcpu: 7910m\rephemeral-storage: 18242267924\rhugepages-1Gi: 0\rhugepages-2Mi: 0\rmemory: 31391844Ki\rpods: 58\rSystem Info:\rMachine ID: ec2cf9fd6e8ff9955c3f7269a4a9d3da\rSystem UUID: ec2cf9fd-6e8f-f995-5c3f-7269a4a9d3da\rBoot ID: c48572a6-8e6d-427b-b5b4-f9f21e8e0838\rKernel Version: 5.4.196-108.356.amzn2.x86_64\rOS Image: Amazon Linux 2\rOperating System: linux\rArchitecture: amd64\rContainer Runtime Version: docker://20.10.13\rKubelet Version: v1.21.12-eks-5308cf7\rKube-Proxy Version: v1.21.12-eks-5308cf7\rProviderID: aws:///ap-northeast-2a/i-0f255f242c1b4616e\rNon-terminated Pods: (19 in total)\rNamespace Name CPU Requests CPU Limits Memory Requests Memory Limits Age\r--------- ---- ------------ ---------- --------------- ------------- ---\rcode-server code-server-84f85bfbb7-q9rs5 0 (0%) 0 (0%) 0 (0%) 0 (0%) 42h\rgatekeeper-system gatekeeper-controller-manager-77768dcc76-fmggf 100m (1%) 1 (12%) 256Mi (0%) 512Mi (1%) 19h\rkube-system alb-controller-aws-load-balancer-controller-579798fdbf-5w8zb 0 (0%) 0 (0%) 0 (0%) 0 (0%) 6d14h\rkube-system aws-cluster-autoscaler-84bd9c55fb-k28ps 0 (0%) 0 (0%) 0 (0%) 0 (0%) 6d14h\rkube-system aws-node-lsfh6 25m (0%) 0 (0%) 0 (0%) 0 (0%) 6d15h\rkube-system coredns-6dbb778559-5vn52 100m (1%) 0 (0%) 70Mi (0%) 170Mi (0%) 6d15h\rkube-system ebs-csi-node-q6df6 0 (0%) 0 (0%) 0 (0%) 0 (0%) 6d14h\rkube-system efs-csi-controller-664994d876-9wqqx 0 (0%) 0 (0%) 0 (0%) 0 (0%) 17h\rkube-system efs-csi-node-4x5tj 0 (0%) 0 (0%) 0 (0%) 0 (0%) 17h\rkube-system kube-proxy-26nsg 100m (1%) 0 (0%) 0 (0%) 0 (0%) 6d15h\rlinkerd-viz tap-766dd477f8-x2m94 200m (2%) 100m (1%) 70Mi (0%) 500Mi (1%) 16h\rlinkerd linkerd-destination-7c8564ff97-g8gf6 300m (3%) 100m (1%) 120Mi (0%) 750Mi (2%) 16h\rlinkerd linkerd-identity-67bcfd69d4-l94zv 200m (2%) 100m (1%) 30Mi (0%) 500Mi (1%) 16h\rlinkerd linkerd-proxy-injector-6f464ddc76-98wj4 200m (2%) 100m (1%) 70Mi (0%) 500Mi (1%) 16h\rlitmus subscriber-cd959f546-4g8kx 125m (1%) 225m (2%) 300Mi (0%) 500Mi (1%) 21h\rlitmus workflow-controller-856d568f68-wt6dj 125m (1%) 225m (2%) 300Mi (0%) 500Mi (1%) 21h\rlog-stack fluentd-ccwzk 300m (3%) 300m (3%) 1Gi (3%) 1Gi (3%) 46h\rlog-stack opensearch-cluster-coordinate-0 1 (12%) 1 (12%) 2Gi (6%) 2Gi (6%) 46h\rlog-stack opensearch-cluster-master-0 1 (12%) 1 (12%) 2Gi (6%) 2Gi (6%) 46h\rAllocated resources:\r(Total limits may be over 100 percent, i.e., overcommitted.)\rResource Requests Limits\r-------- -------- ------\rcpu 3775m (47%) 4150m (52%)\rmemory 6336Mi (20%) 9052Mi (29%)\rephemeral-storage 1000Mi (5%) 2Gi (11%)\rhugepages-1Gi 0 (0%) 0 (0%)\rhugepages-2Mi 0 (0%) 0 (0%)\rattachable-volumes-aws-ebs 0 0\rEvents: \u0026lt;none\u0026gt;  적용된 레이블 확인  Labels: beta.kubernetes.io/arch=amd64\rbeta.kubernetes.io/instance-type=m5.2xlarge\rbeta.kubernetes.io/os=linux\reks.amazonaws.com/capacityType=ON_DEMAND\reks.amazonaws.com/nodegroup=black-nodegroup-20220708083127543500000015\reks.amazonaws.com/nodegroup-image=ami-0918f823d29c638d9\reks.amazonaws.com/sourceLaunchTemplateId=lt-01141f4c6a453c7f0\reks.amazonaws.com/sourceLaunchTemplateVersion=1\rfailure-domain.beta.kubernetes.io/region=ap-northeast-2\rfailure-domain.beta.kubernetes.io/zone=ap-northeast-2a\rkubernetes.io/arch=amd64\rkubernetes.io/hostname=ip-10-83-80-162.ap-northeast-2.compute.internal\rkubernetes.io/os=linux\rnode.kubernetes.io/instance-type=m5.2xlarge\rtopology.ebs.csi.aws.com/zone=ap-northeast-2a\rtopology.kubernetes.io/region=ap-northeast-2\rtopology.kubernetes.io/zone=ap-northeast-2a  현재 스케줄링중인 pod 상태  Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits Age\r--------- ---- ------------ ---------- --------------- ------------- ---\rcode-server code-server-84f85bfbb7-q9rs5 0 (0%) 0 (0%) 0 (0%) 0 (0%) 42h\rgatekeeper-system gatekeeper-controller-manager-77768dcc76-fmggf 100m (1%) 1 (12%) 256Mi (0%) 512Mi (1%) 19h\rkube-system alb-controller-aws-load-balancer-controller-579798fdbf-5w8zb 0 (0%) 0 (0%) 0 (0%) 0 (0%) 6d14h\rkube-system aws-cluster-autoscaler-84bd9c55fb-k28ps 0 (0%) 0 (0%) 0 (0%) 0 (0%) 6d14h\rkube-system aws-node-lsfh6 25m (0%) 0 (0%) 0 (0%) 0 (0%) 6d15h\rkube-system coredns-6dbb778559-5vn52 100m (1%) 0 (0%) 70Mi (0%) 170Mi (0%) 6d15h\rkube-system ebs-csi-node-q6df6 0 (0%) 0 (0%) 0 (0%) 0 (0%) 6d14h\rkube-system efs-csi-controller-664994d876-9wqqx 0 (0%) 0 (0%) 0 (0%) 0 (0%) 17h\rkube-system efs-csi-node-4x5tj 0 (0%) 0 (0%) 0 (0%) 0 (0%) 17h\rkube-system kube-proxy-26nsg 100m (1%) 0 (0%) 0 (0%) 0 (0%) 6d15h\rlinkerd-viz tap-766dd477f8-x2m94 200m (2%) 100m (1%) 70Mi (0%) 500Mi (1%) 16h\rlinkerd linkerd-destination-7c8564ff97-g8gf6 300m (3%) 100m (1%) 120Mi (0%) 750Mi (2%) 16h\rlinkerd linkerd-identity-67bcfd69d4-l94zv 200m (2%) 100m (1%) 30Mi (0%) 500Mi (1%) 16h\rlinkerd linkerd-proxy-injector-6f464ddc76-98wj4 200m (2%) 100m (1%) 70Mi (0%) 500Mi (1%) 16h\rlitmus subscriber-cd959f546-4g8kx 125m (1%) 225m (2%) 300Mi (0%) 500Mi (1%) 21h\rlitmus workflow-controller-856d568f68-wt6dj 125m (1%) 225m (2%) 300Mi (0%) 500Mi (1%) 21h\rlog-stack fluentd-ccwzk 300m (3%) 300m (3%) 1Gi (3%) 1Gi (3%) 46h\rlog-stack opensearch-cluster-coordinate-0 1 (12%) 1 (12%) 2Gi (6%) 2Gi (6%) 46h\rlog-stack opensearch-cluster-master-0 1 (12%) 1 (12%) 2Gi (6%) 2Gi (6%) 46h  리소스 사용 현황을 보자 cpu 점유율이 47%이다.  Allocated resources:\r(Total limits may be over 100 percent, i.e., overcommitted.)\rResource Requests Limits\r-------- -------- ------\rcpu 3775m (47%) 4150m (52%)\rmemory 6336Mi (20%) 9052Mi (29%)\rephemeral-storage 1000Mi (5%) 2Gi (11%)\rhugepages-1Gi 0 (0%) 0 (0%)\rhugepages-2Mi 0 (0%) 0 (0%)\rattachable-volumes-aws-ebs 0 0 label을 이용한 특정 Node에 Pod 배포 테스트를 위해 nginx app을 1개 배포해 보자.\n 네임스페이스를 생성한다.(scheduling-test)  $ kubectl create namespace scheduling-test\rnamespace/scheduling-test created  label 추가  kubectl label nodes [node_name] [key]=[value]\r$ kubectl label nodes ip-10-83-80-162.ap-northeast-2.compute.internal key=mytest-node\rnode/ip-10-83-80-162.ap-northeast-2.compute.internal labeled\r# 레이블 삭제\rkubectl label nodes mytest-node key-  레이블 확인(key : mytest-node)  $ kubectl describe nodes ip-10-83-80-162.ap-northeast-2.compute.internal\rName: ip-10-83-80-162.ap-northeast-2.compute.internal\rRoles: \u0026lt;none\u0026gt;\rLabels: beta.kubernetes.io/arch=amd64\rbeta.kubernetes.io/instance-type=m5.2xlarge\rbeta.kubernetes.io/os=linux\reks.amazonaws.com/capacityType=ON_DEMAND\reks.amazonaws.com/nodegroup=black-nodegroup-20220708083127543500000015\reks.amazonaws.com/nodegroup-image=ami-0918f823d29c638d9\reks.amazonaws.com/sourceLaunchTemplateId=lt-01141f4c6a453c7f0\reks.amazonaws.com/sourceLaunchTemplateVersion=1\rfailure-domain.beta.kubernetes.io/region=ap-northeast-2\rfailure-domain.beta.kubernetes.io/zone=ap-northeast-2a\rkey=mytest-node\rkubernetes.io/arch=amd64\rkubernetes.io/hostname=ip-10-83-80-162.ap-northeast-2.compute.internal\rkubernetes.io/os=linux\rnode.kubernetes.io/instance-type=m5.2xlarge\rtopology.ebs.csi.aws.com/zone=ap-northeast-2a\rtopology.kubernetes.io/region=ap-northeast-2\rtopology.kubernetes.io/zone=ap-northeast-2a  scheduling.yaml  apiVersion: apps/v1\rkind: Deployment\rmetadata:\rname: my-nginx\rnamespace: scheduling-test\rlabels:\rapp: my-nginx\rspec:\rreplicas: 3\rselector:\rmatchLabels:\rapp: my-nginx\rtemplate:\rmetadata:\rlabels:\rapp: my-nginx\rspec:\rcontainers:\r- name: my-nginx\rimage: nginx:1.14.2\rports:\r- containerPort: 80\rresources:\rrequests: cpu: \u0026#34;500m\u0026#34;\rlimits: cpu: \u0026#34;1000m\u0026#34;\rnodeSelector:\rkey: mytest-node\r---\rapiVersion: v1\rkind: Service\rmetadata:\rname: my-nginx\rnamespace: scheduling-test\rlabels:\rrun: my-nginx\rspec:\rports:\r- port: 80\rtargetPort: 80\rprotocol: TCP\rselector:\rapp: my-nginx\r---\rapiVersion: extensions/v1beta1\rkind: Ingress\rmetadata:\rname: scheduling-ingress\rnamespace: scheduling-test\rannotations:\rkubernetes.io/ingress.class: nginx\rspec:\rrules:\r- host: nginxtest.black.cloud.hancom.com\rhttp:\rpaths:\r- backend:\rserviceName: my-nginx\rservicePort: 80  배포  $ kubectl apply -f .\\nodeselectortest.yaml\rdeployment.apps/my-nginx created\rservice/my-nginx created ingress.extensions/scheduling-ingress created  pod 상태 조회  $ kubectl get pods -n scheduling-test\rNAME READY STATUS RESTARTS AGE\rmy-nginx-5b5f4bdd49-qwk7x 1/1 Running 0 9s  node 상태 조회  ......\rNon-terminated Pods: (20 in total)\rNamespace Name CPU Requests CPU Limits Memory Requests Memory Limits Age\r--------- ---- ------------ ---------- --------------- ------------- ---\rcode-server code-server-84f85bfbb7-q9rs5 0 (0%) 0 (0%) 0 (0%) 0 (0%) 43h\rgatekeeper-system gatekeeper-controller-manager-77768dcc76-fmggf 100m (1%) 1 (12%) 256Mi (0%) 512Mi (1%) 20h\rkube-system alb-controller-aws-load-balancer-controller-579798fdbf-5w8zb 0 (0%) 0 (0%) 0 (0%) 0 (0%) 6d15h\rkube-system aws-cluster-autoscaler-84bd9c55fb-k28ps 0 (0%) 0 (0%) 0 (0%) 0 (0%) 6d15h\rkube-system aws-node-lsfh6 25m (0%) 0 (0%) 0 (0%) 0 (0%) 6d15h\rkube-system coredns-6dbb778559-5vn52 100m (1%) 0 (0%) 70Mi (0%) 170Mi (0%) 6d16h\rkube-system ebs-csi-node-q6df6 0 (0%) 0 (0%) 0 (0%) 0 (0%) 6d15h\rkube-system efs-csi-controller-664994d876-9wqqx 0 (0%) 0 (0%) 0 (0%) 0 (0%) 17h\rkube-system efs-csi-node-4x5tj 0 (0%) 0 (0%) 0 (0%) 0 (0%) 17h\rkube-system kube-proxy-26nsg 100m (1%) 0 (0%) 0 (0%) 0 (0%) 6d15h\rlinkerd-viz tap-766dd477f8-x2m94 200m (2%) 100m (1%) 70Mi (0%) 500Mi (1%) 16h\rlinkerd linkerd-destination-7c8564ff97-g8gf6 300m (3%) 100m (1%) 120Mi (0%) 750Mi (2%) 16h\rlinkerd linkerd-identity-67bcfd69d4-l94zv 200m (2%) 100m (1%) 30Mi (0%) 500Mi (1%) 16h\rlinkerd linkerd-proxy-injector-6f464ddc76-98wj4 200m (2%) 100m (1%) 70Mi (0%) 500Mi (1%) 16h\rlitmus subscriber-cd959f546-4g8kx 125m (1%) 225m (2%) 300Mi (0%) 500Mi (1%) 22h\rlitmus workflow-controller-856d568f68-wt6dj 125m (1%) 225m (2%) 300Mi (0%) 500Mi (1%) 22h\rlog-stack fluentd-ccwzk 300m (3%) 300m (3%) 1Gi (3%) 1Gi (3%) 47h\rlog-stack opensearch-cluster-coordinate-0 1 (12%) 1 (12%) 2Gi (6%) 2Gi (6%) 47h\rlog-stack opensearch-cluster-master-0 1 (12%) 1 (12%) 2Gi (6%) 2Gi (6%) 47h\rscheduling-test my-nginx-5b5f4bdd49-qwk7x 500m (6%) 1 (12%) 0 (0%) 0 (0%) 90s\rAllocated resources:\r(Total limits may be over 100 percent, i.e., overcommitted.)\rResource Requests Limits\r-------- -------- ------\rcpu 4275m (54%) 5150m (65%)\rmemory 6336Mi (20%) 9052Mi (29%)\rephemeral-storage 1000Mi (5%) 2Gi (11%)\rhugepages-1Gi 0 (0%) 0 (0%)\rhugepages-2Mi 0 (0%) 0 (0%)\rattachable-volumes-aws-ebs 0 0 Deployment를 수정해 리소스 제한 요청을 늘려보자  기존 리소스 삭제  $ kubectl delete -f nodeselectortest.yaml deployment.apps \u0026#34;my-nginx\u0026#34; deleted\rservice \u0026#34;my-nginx\u0026#34; deleted ingress.extensions \u0026#34;scheduling-ingress\u0026#34; deleted resources:\rrequests: cpu: \u0026#34;5000m\u0026#34;\rlimits: cpu: \u0026#34;6000m\u0026#34;  현재 상태 확인  $ kubectl describe nodes ip-10-83-80-162.ap-northeast-2.compute.internal\r.....\rNon-terminated Pods: (19 in total)\rNamespace Name CPU Requests CPU Limits Memory Requests Memory Limits Age\r--------- ---- ------------ ---------- --------------- ------------- ---\rcode-server code-server-84f85bfbb7-q9rs5 0 (0%) 0 (0%) 0 (0%) 0 (0%) 43h\rgatekeeper-system gatekeeper-controller-manager-77768dcc76-fmggf 100m (1%) 1 (12%) 256Mi (0%) 512Mi (1%) 20h\rkube-system alb-controller-aws-load-balancer-controller-579798fdbf-5w8zb 0 (0%) 0 (0%) 0 (0%) 0 (0%) 6d15h\rkube-system aws-cluster-autoscaler-84bd9c55fb-k28ps 0 (0%) 0 (0%) 0 (0%) 0 (0%) 6d15h\rkube-system aws-node-lsfh6 25m (0%) 0 (0%) 0 (0%) 0 (0%) 6d16h\rkube-system coredns-6dbb778559-5vn52 100m (1%) 0 (0%) 70Mi (0%) 170Mi (0%) 6d16h\rkube-system ebs-csi-node-q6df6 0 (0%) 0 (0%) 0 (0%) 0 (0%) 6d15h\rkube-system efs-csi-controller-664994d876-9wqqx 0 (0%) 0 (0%) 0 (0%) 0 (0%) 18h\rkube-system efs-csi-node-4x5tj 0 (0%) 0 (0%) 0 (0%) 0 (0%) 18h\rkube-system kube-proxy-26nsg 100m (1%) 0 (0%) 0 (0%) 0 (0%) 6d16h\rlinkerd-viz tap-766dd477f8-x2m94 200m (2%) 100m (1%) 70Mi (0%) 500Mi (1%) 16h\rlinkerd linkerd-destination-7c8564ff97-g8gf6 300m (3%) 100m (1%) 120Mi (0%) 750Mi (2%) 16h\rlinkerd linkerd-identity-67bcfd69d4-l94zv 200m (2%) 100m (1%) 30Mi (0%) 500Mi (1%) 16h\rlinkerd linkerd-proxy-injector-6f464ddc76-98wj4 200m (2%) 100m (1%) 70Mi (0%) 500Mi (1%) 16h\rlitmus subscriber-cd959f546-4g8kx 125m (1%) 225m (2%) 300Mi (0%) 500Mi (1%) 22h\rlitmus workflow-controller-856d568f68-wt6dj 125m (1%) 225m (2%) 300Mi (0%) 500Mi (1%) 22h\rlog-stack fluentd-ccwzk 300m (3%) 300m (3%) 1Gi (3%) 1Gi (3%) 47h\rlog-stack opensearch-cluster-coordinate-0 1 (12%) 1 (12%) 2Gi (6%) 2Gi (6%) 47h\rlog-stack opensearch-cluster-master-0 1 (12%) 1 (12%) 2Gi (6%) 2Gi (6%) 47h\rAllocated resources:\r(Total limits may be over 100 percent, i.e., overcommitted.)\rResource Requests Limits\r-------- -------- ------\rcpu 3775m (47%) 4150m (52%)\rmemory 6336Mi (20%) 9052Mi (29%)\rephemeral-storage 1000Mi (5%) 2Gi (11%)\rhugepages-1Gi 0 (0%) 0 (0%)\rhugepages-2Mi 0 (0%) 0 (0%)\rattachable-volumes-aws-ebs 0 0\rEvents: \u0026lt;none\u0026gt;  배포후 pod 상태 확인  $ kubectl apply -f nodeselectortest.yaml\rdeployment.apps/my-nginx created\rservice/my-nginx created ingress.extensions/scheduling-ingress created\r$ kubectl get pods -n scheduling-test\rNAME READY STATUS RESTARTS AGE\rmy-nginx-7cbcf9fb8d-lqcpg 0/1 Pending 0 13s  로그 조회  $ kubectl describe pods/my-nginx-7cbcf9fb8d-lqcpg -n scheduling-test\rEvents:\rType Reason Age From Message\r---- ------ ---- ---- -------\rWarning FailedScheduling 5m28s (x2 over 5m29s) default-scheduler 0/3 nodes are available: 1 Insufficient cpu, 2 node(s) didn\u0026#39;t match Pod\u0026#39;s node affinity/selector.\rNormal TriggeredScaleUp 5m26s cluster-autoscaler pod triggered scale-up: [{eks-black-nodegroup-20220708083127543500000015-58c0ee77-b6ac-cd2a-5d95-c6d8c8c059b3 3-\u0026gt;4 (max: 6)}]\rWarning FailedScheduling 4m15s (x3 over 4m43s) default-scheduler 0/4 nodes are available: 1 Insufficient cpu, 1 node(s) had taint {node.kubernetes.io/not-ready: }, that the pod didn\u0026#39;t tolerate, 2 node(s) didn\u0026#39;t match Pod\u0026#39;s node affinity/selector.\rWarning FailedScheduling 3m55s (x2 over 4m5s) default-scheduler 0/4 nodes are available: 1 Insufficient cpu, 3 node(s) didn\u0026#39;t match Pod\u0026#39;s node affinity/selector.\rWarning FailedScheduling 2m51s (x3 over 3m19s) default-scheduler 0/5 nodes are available: 1 Insufficient cpu, 1 node(s) had taint {node.kubernetes.io/not-ready: }, that the pod didn\u0026#39;t tolerate, 3 node(s) didn\u0026#39;t match Pod\u0026#39;s node affinity/selector.\rWarning FailedScheduling 2m31s (x2 over 2m41s) default-scheduler 0/5 nodes are available: 1 Insufficient cpu, 4 node(s) didn\u0026#39;t match Pod\u0026#39;s node affinity/selector.\rWarning FailedScheduling 93s (x3 over 2m1s) default-scheduler 0/6 nodes are available: 1 Insufficient cpu, 1 node(s) had taint {node.kubernetes.io/not-ready: }, that the pod didn\u0026#39;t tolerate, 4 node(s) didn\u0026#39;t match Pod\u0026#39;s node affinity/selector.\rWarning FailedScheduling 72s (x2 over 82s) default-scheduler 0/6 nodes are available: 1 Insufficient cpu, 5 node(s) didn\u0026#39;t match Pod\u0026#39;s node affinity/selector.\rNormal NotTriggerScaleUp 24s (x10 over 2m35s) cluster-autoscaler pod didn\u0026#39;t trigger scale-up: 1 max node group size reached 확인 결과 Running중인 pod의 드레인이나 리스케줄링 같은 특이상황은 관찰되지 않았으며, 리소스 부족으로 오토스케일링이 발생하여 max 값까지 노드들이 증가하지만 레이블이 존재하지 않으므로 pod 배포에 실패한다.\npriorityClassName 적용후 동작 확인  priorityClass 적용  nodeSelector:\rkey: mytest-node\rpriorityClassName: system-cluster-critical  배포 성공 및 기존 pod 드레인 확인  Non-terminated Pods: (16 in total)\rNamespace Name CPU Requests CPU Limits Memory Requests Memory Limits Age\r--------- ---- ------------ ---------- --------------- ------------- ---\rcode-server code-server-84f85bfbb7-q9rs5 0 (0%) 0 (0%) 0 (0%) 0 (0%) 43h\rgatekeeper-system gatekeeper-controller-manager-77768dcc76-fmggf 100m (1%) 1 (12%) 256Mi (0%) 512Mi (1%) 20h\rkube-system alb-controller-aws-load-balancer-controller-579798fdbf-5w8zb 0 (0%) 0 (0%) 0 (0%) 0 (0%) 6d15h\rkube-system aws-cluster-autoscaler-84bd9c55fb-k28ps 0 (0%) 0 (0%) 0 (0%) 0 (0%) 6d15h\rkube-system aws-node-lsfh6 25m (0%) 0 (0%) 0 (0%) 0 (0%) 6d16h\rkube-system coredns-6dbb778559-5vn52 100m (1%) 0 (0%) 70Mi (0%) 170Mi (0%) 6d16h\rkube-system ebs-csi-node-q6df6 0 (0%) 0 (0%) 0 (0%) 0 (0%) 6d15h\rkube-system efs-csi-controller-664994d876-9wqqx 0 (0%) 0 (0%) 0 (0%) 0 (0%) 18h\rkube-system efs-csi-node-4x5tj 0 (0%) 0 (0%) 0 (0%) 0 (0%) 18h\rkube-system kube-proxy-26nsg 100m (1%) 0 (0%) 0 (0%) 0 (0%) 6d16h\rlitmus subscriber-cd959f546-4g8kx 125m (1%) 225m (2%) 300Mi (0%) 500Mi (1%) 22h\rlitmus workflow-controller-856d568f68-wt6dj 125m (1%) 225m (2%) 300Mi (0%) 500Mi (1%) 22h\rlog-stack fluentd-ccwzk 300m (3%) 300m (3%) 1Gi (3%) 1Gi (3%) 47h\rlog-stack opensearch-cluster-coordinate-0 1 (12%) 1 (12%) 2Gi (6%) 2Gi (6%) 47h\rlog-stack opensearch-cluster-master-0 1 (12%) 1 (12%) 2Gi (6%) 2Gi (6%) 47h\rscheduling-test my-nginx-7c9849cffb-nr6gn 5 (63%) 6 (75%) 0 (0%) 0 (0%) 99s\rAllocated resources:\r(Total limits may be over 100 percent, i.e., overcommitted.)\rResource Requests Limits\r-------- -------- ------\rcpu 7875m (99%) 9750m (123%)\rmemory 6046Mi (19%) 6802Mi (22%)\rephemeral-storage 1000Mi (5%) 2Gi (11%)\rhugepages-1Gi 0 (0%) 0 (0%)\rhugepages-2Mi 0 (0%) 0 (0%)\rattachable-volumes-aws-ebs 0 0\rEvents: \u0026lt;none\u0026gt; 결론 우선순위가 낮은 pod들이 이동되는것을 확인함\n"
},
{
	"uri": "https://hnc-hskim.github.io/hugo/style/",
	"title": "Style",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://hnc-hskim.github.io/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Contents  kubernetes  CLI   workshop  MSA    A notice disclaimer\n\rAn information disclaimer\n\rA tip disclaimer\n\rA warning disclaimer\n\r"
},
{
	"uri": "https://hnc-hskim.github.io/kubernetes/",
	"title": "Kubernetes",
	"tags": [],
	"description": "",
	"content": "CLI 사용법 "
},
{
	"uri": "https://hnc-hskim.github.io/kubernetes/aws-cli/",
	"title": "Aws Cli",
	"tags": [],
	"description": "",
	"content": "aws 계정 정보 조회 aws sts get-caller-identity --profile \u0026#34;name\u0026#34; kubeconfig 등록 aws eks --profile \u0026#34;profile name\u0026#34; update-kubeconfig --name \u0026#34;cluster name\u0026#34; --region ap-northeast-2 "
},
{
	"uri": "https://hnc-hskim.github.io/kubernetes/cli/",
	"title": "Cli",
	"tags": [],
	"description": "",
	"content": "컨텍스트 조회 # 조회\rkubectl config get-contexts\r# 사용\rkubectl config use-context \u0026#34;context name\u0026#34; 클러스터명 조회 kubectl config view --minify -o jsonpath=\u0026#39;{.clusters[].name}\u0026#39; 노드에 레이블 추가 # 레이블 추가\rkubectl label nodes \u0026lt;your-node-name\u0026gt; disktype=ssd\r# 레이블 확인\rkubectl get nodes --show-labels Evicted pod 제거 kubectl delete pods --field-selector=status.phase=Failed -A node describe kubectl describe nodes \u0026#34;nodes-name\u0026#34; "
},
{
	"uri": "https://hnc-hskim.github.io/workshop/msa/",
	"title": "Msa",
	"tags": [],
	"description": "",
	"content": "[netflex microservice] https://netflixtechblog.com/tagged/microservices\n마이크로서비스는 소프트웨어가 잘 정의된 API를 통해 통신하는 소규모의 독립적인 서비스로 구성되어 있는 경우의 소프트웨어 개발을 위한 아키텍처 및 조직적 접근 방식입니다.\n이러한 서비스는 독립적인 소규모 팀에서 보유합니다. 마이크로서비스 아키텍처는 애플리케이션의 확장을 용이하게 하고 개발 속도를 앞당겨 혁신을 실현하고 새로운 기능의 출시 시간을 단축할 수 있게 해 줍니다.\n\rMain featrues 1. MSA 소개 2. Software Architecture 3. 10 Common Software Architectural Patterns Goals  소프트웨어 아키텍처가 필요한 이유 모놀리식 아키텍처 계층화된 아키텍처 마이크로 서비스 아키텍처  마이크로서비스 아키텍처의 경우, 애플리케이션이 독립적인 구성 요소로 구축되어 각 애플리케이션 프로세스가 서비스로 실행됩니다. 이러한 서비스는 경량 API를 사용하여 잘 정의된 인터페이스를 통해 통신합니다. 서비스는 비즈니스 기능을 위해 구축되며 서비스마다 한 가지 기능을 수행합니다. 서비스가 독립적으로 실행되기 때문에 애플리케이션의 특정 기능에 대한 수요를 충족하도록 각각의 서비스를 업데이트, 배포 및 확장할 수 있습니다.\n[출처] https://aws.amazon.com/ko/microservices/\nbluewhale-users\n"
},
{
	"uri": "https://hnc-hskim.github.io/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://hnc-hskim.github.io/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]